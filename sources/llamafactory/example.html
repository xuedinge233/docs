

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>全流程昇腾实践 &mdash; 昇腾开源  文档</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=ec38875e" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../_static/statistics.js?v=ed8c2343"></script>
      <script src="../../_static/translations.js?v=beaddf03"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="FAQ" href="faq.html" />
    <link rel="prev" title="单机多卡微调" href="multi_npu.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            昇腾开源
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="搜索文档" aria-label="搜索文档" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="导航菜单">
              <p class="caption" role="heading"><span class="caption-text">开始使用</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../ascend/quick_install.html">快速安装昇腾环境</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">原生支持的AI项目</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../pytorch/index.html">PyTorch</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">LLaMA-Factory</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="install.html">安装指南</a></li>
<li class="toctree-l2"><a class="reference internal" href="quick_start.html">快速开始</a></li>
<li class="toctree-l2"><a class="reference internal" href="multi_npu.html">单机多卡微调</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">全流程昇腾实践</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id4">前置准备</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id5">安装准备</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">配置文件准备</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id7">原始模型直接推理</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id8">自定义数据集构建</a></li>
<li class="toctree-l3"><a class="reference internal" href="#lora-sft">基于 LoRA 的 sft 指令微调</a></li>
<li class="toctree-l3"><a class="reference internal" href="#lora">动态合并 LoRA 的推理</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id10">批量预测和训练效果评估</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id11">LoRA 模型合并导出</a></li>
<li class="toctree-l3"><a class="reference internal" href="#webui-board">一站式 webui board 的使用</a></li>
<li class="toctree-l3"><a class="reference internal" href="#api-server">API Server的启动与调用</a></li>
<li class="toctree-l3"><a class="reference internal" href="#benchmark">进阶-大模型主流评测 benchmark</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="faq.html">FAQ</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../transformers/index.html">Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../whisper_cpp/index.html">Whisper.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torchchat/index.html">Torchchat</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torchtitan/index.html">TorchTitan</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="移动版导航菜单" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">昇腾开源</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="页面导航">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">LLaMA-Factory</a></li>
      <li class="breadcrumb-item active">全流程昇腾实践</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/sources/llamafactory/example.rst.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<h1>全流程昇腾实践<a class="headerlink" href="#id1" title="Link to this heading"></a></h1>
<p>开始本篇之前，请阅读 <a class="reference external" href="https://zhuanlan.zhihu.com/p/695287607">LLaMA-Factory QuickStart</a> 了解 LLaMA-Factory 及其主要功能的用法，
并参考 <a class="reference internal" href="install.html"><span class="doc">安装指南</span></a> 及  <a class="reference internal" href="quick_start.html"><span class="doc">快速开始</span></a> 完成基本的环境准备、LLaMA-Factory 安装及简单的微调和推理功能。
本篇在此基础上，以 Qwen1.5-7B 模型为例，帮助开发者在昇腾 NPU 上使用 LLaMA-Factory 更多实用特性。</p>
<p><a class="reference external" href="https://zhuanlan.zhihu.com/p/695287607">LLaMA-Factory QuickStart</a> 中详解了下列 9 种功能，本教程为在 NPU 上全流程实践示例，
有关功能及参数的详细解析请参考 <a class="reference external" href="https://zhuanlan.zhihu.com/p/695287607">LLaMA-Factory QuickStart</a></p>
<ol class="arabic simple">
<li><p>原始模型直接推理</p></li>
<li><p>自定义数据集构建</p></li>
<li><p>基于 LoRA 的 sft 指令微调</p></li>
<li><p>动态合并 LoRA 的推理</p></li>
<li><p>批量预测和训练效果评估</p></li>
<li><p>LoRA模型合并导出</p></li>
<li><p>一站式 webui board 的使用</p></li>
<li><p>API Server的启动与调用</p></li>
<li><p>大模型主流评测 benchmark</p></li>
</ol>
<section id="id4">
<h2>前置准备<a class="headerlink" href="#id4" title="Link to this heading"></a></h2>
<section id="id5">
<h3>安装准备<a class="headerlink" href="#id5" title="Link to this heading"></a></h3>
<p>请确认已按照 <a class="reference internal" href="install.html"><span class="doc">安装指南</span></a> 安装 CANN 和 LLaMA-Factory 并完成安装校验。</p>
</section>
<section id="id6">
<h3>配置文件准备<a class="headerlink" href="#id6" title="Link to this heading"></a></h3>
<p>本示例中用到的参数配置文件与快速开始 <a class="reference internal" href="quick_start.html#qwen-yaml"><span class="std std-ref">qwen1_5_lora_sft_ds.yaml</span></a> 中一致，可参考快速开始。</p>
</section>
</section>
<section id="id7">
<h2>原始模型直接推理<a class="headerlink" href="#id7" title="Link to this heading"></a></h2>
<p>验证 LLaMA-Factory 在昇腾 NPU 上推理功能是否正常：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="nv">ASCEND_RT_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>llamafactory-cli<span class="w"> </span>webchat<span class="w"> </span>--model_name_or_path<span class="w"> </span>qwen/Qwen1.5-7B<span class="w"> </span><span class="se">\</span>
<span class="linenos">2</span><span class="w">            </span>--adapter_name_or_path<span class="w"> </span>saves/Qwen1.5-7B/lora/sft<span class="w"> </span><span class="se">\</span>
<span class="linenos">3</span><span class="w">            </span>--template<span class="w"> </span>qwen<span class="w"> </span><span class="se">\</span>
<span class="linenos">4</span><span class="w">            </span>--finetuning_type<span class="w"> </span>lora
</pre></div>
</div>
<p>如下图所示可正常进行对话，即为可正常推理：</p>
<figure class="align-center">
<img alt="../../_images/webchat.png" src="../../_images/webchat.png" />
</figure>
</section>
<section id="id8">
<h2>自定义数据集构建<a class="headerlink" href="#id8" title="Link to this heading"></a></h2>
<p>本篇用到的数据集为 LLaMA-Factory 自带的 identity 和 alpaca_en_demo，对 identity 数据集进行如下全局替换即可实现定制指令：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">{{name}}</span></code> 替换为 <code class="docutils literal notranslate"><span class="pre">Ascend-helper</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">{{author}}</span></code> 替换为 <code class="docutils literal notranslate"><span class="pre">Ascend</span></code></p></li>
</ul>
<p>更多自定义数据集的构建请参考 <a class="reference external" href="https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README_zh.md">官方数据集构造指引</a> 。</p>
</section>
<section id="lora-sft">
<span id="sft"></span><h2>基于 LoRA 的 sft 指令微调<a class="headerlink" href="#lora-sft" title="Link to this heading"></a></h2>
<p>在 <a class="reference internal" href="quick_start.html"><span class="doc">快速开始</span></a> 中，已经尝试过使用 src/train.py 为入口的微调脚本，本篇中均使用 llamafactory-cli 命令启动微调、推理等程序。</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="nv">ASCEND_RT_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>llamafactory-cli<span class="w"> </span>train<span class="w"> </span>&lt;your_path&gt;/qwen1_5_lora_sft_ds.yaml
</pre></div>
</div>
</section>
<section id="lora">
<h2>动态合并 LoRA 的推理<a class="headerlink" href="#lora" title="Link to this heading"></a></h2>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="nv">ASCEND_RT_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>llamafactory-cli<span class="w"> </span>chat<span class="w"> </span>--model_name_or_path<span class="w"> </span>qwen/Qwen1.5-7B<span class="w"> </span><span class="se">\</span>
<span class="linenos">2</span><span class="w">            </span>--adapter_name_or_path<span class="w"> </span>saves/Qwen1.5-7B/lora/sft<span class="w"> </span><span class="se">\</span>
<span class="linenos">3</span><span class="w">            </span>--template<span class="w"> </span>qwen<span class="w"> </span><span class="se">\</span>
<span class="linenos">4</span><span class="w">            </span>--finetuning_type<span class="w"> </span>lora
</pre></div>
</div>
<p>通过询问大模型是谁检验 sft 指令微调的成果，如下图，大模型回答自己是 Ascend-helper 说明 sft 成功，如失败，可返回 <a class="reference internal" href="#sft"><span class="std std-ref">基于 LoRA 的 sft 指令微调</span></a> 增加训练轮数重新训练。</p>
<figure class="align-center">
<img alt="../../_images/sft-chat.gif" src="../../_images/sft-chat.gif" />
</figure>
</section>
<section id="id10">
<h2>批量预测和训练效果评估<a class="headerlink" href="#id10" title="Link to this heading"></a></h2>
<p>使用批量预测和评估前，需先安装 jieba、rouge-chinese、nltk 三个库：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span>pip<span class="w"> </span>install<span class="w"> </span>jieba,rouge-chinese,nltk<span class="w"> </span>-i<span class="w"> </span>https://pypi.tuna.tsinghua.edu.cn/simple
</pre></div>
</div>
<p>然后使用以下指令对微调后的模型在 alpaca_gpt4_zh 和 identity 数据集上进行批量预测和效果评估：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="nv">ASCEND_RT_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>llamafactory-cli<span class="w"> </span>train<span class="w"> </span><span class="se">\</span>
<span class="linenos"> 2</span><span class="w">            </span>--stage<span class="w"> </span>sft<span class="w"> </span><span class="se">\</span>
<span class="linenos"> 3</span><span class="w">            </span>--do_predict<span class="w"> </span><span class="se">\</span>
<span class="linenos"> 4</span><span class="w">            </span>--model_name_or_path<span class="w"> </span>qwen/Qwen1.5-7B<span class="w"> </span><span class="se">\</span>
<span class="linenos"> 5</span><span class="w">            </span>--adapter_name_or_path<span class="w"> </span>./saves/Qwen1.5-7B/lora/sft<span class="w">  </span><span class="se">\</span>
<span class="linenos"> 6</span><span class="w">            </span>--dataset<span class="w"> </span>alpaca_gpt4_zh,identity<span class="w"> </span><span class="se">\</span>
<span class="linenos"> 7</span><span class="w">            </span>--dataset_dir<span class="w"> </span>./data<span class="w"> </span><span class="se">\</span>
<span class="linenos"> 8</span><span class="w">            </span>--template<span class="w"> </span>qwen<span class="w"> </span><span class="se">\</span>
<span class="linenos"> 9</span><span class="w">            </span>--finetuning_type<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
<span class="linenos">10</span><span class="w">            </span>--output_dir<span class="w"> </span>./saves/Qwen1.5-7B/lora/predict<span class="w"> </span><span class="se">\</span>
<span class="linenos">11</span><span class="w">            </span>--overwrite_cache<span class="w"> </span><span class="se">\</span>
<span class="linenos">12</span><span class="w">            </span>--overwrite_output_dir<span class="w"> </span><span class="se">\</span>
<span class="linenos">13</span><span class="w">            </span>--cutoff_len<span class="w"> </span><span class="m">1024</span><span class="w"> </span><span class="se">\</span>
<span class="linenos">14</span><span class="w">            </span>--preprocessing_num_workers<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="linenos">15</span><span class="w">            </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="linenos">16</span><span class="w">            </span>--max_samples<span class="w"> </span><span class="m">20</span><span class="w"> </span><span class="se">\</span>
<span class="linenos">17</span><span class="w">            </span>--predict_with_generate
</pre></div>
</div>
<p>完成批量预测与评估后，在指定的输出路径下会看到以下文件：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>all_results.json
generated_predictions.jsonl
predict_results.json
trainer_log.jsonl
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">generated_predictions.json</span></code> 中为所有测试样本的输入提示词 prompt、标签 label 和模型输出预测结果 predict，下面是其中一个示例：</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="nt">&quot;prompt&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;system\nYou are a helpful assistant.\nuser\n保持健康的三个提示。\nassistant\n&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;label&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;以下是保持健康的三个提示：\n\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\n\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\n\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;predict&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;保持健康的三个提示包括：1. 均衡饮食：饮食应包括各种食物，如蔬菜、水果、全麦面包、蛋白质和健康脂肪，以满足身体的营养需求。\n2. 锻炼：每周至少进行150分钟的中等强度有氧运动，如快走、跑步、游泳或骑自行车，以及至少两次力量训练，以帮助维持身体健康。\n3. 睡眠：保持规律的睡眠习惯，每晚至少睡7-8小时，以帮助身体恢复和充电。&quot;</span><span class="p">}</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">predict_results.json</span></code> 中即为训练效果评估所得结果：</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="p p-Indicator">{</span>
<span class="w">    </span><span class="s">&quot;predict_bleu-4&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">50.941235</span><span class="p p-Indicator">,</span>
<span class="w">    </span><span class="s">&quot;predict_rouge-1&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">65.7085975</span><span class="p p-Indicator">,</span>
<span class="w">    </span><span class="s">&quot;predict_rouge-2&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">52.576409999999996</span><span class="p p-Indicator">,</span>
<span class="w">    </span><span class="s">&quot;predict_rouge-l&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">60.487535</span><span class="p p-Indicator">,</span>
<span class="w">    </span><span class="s">&quot;predict_runtime&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">196.1634</span><span class="p p-Indicator">,</span>
<span class="w">    </span><span class="s">&quot;predict_samples_per_second&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">0.204</span><span class="p p-Indicator">,</span>
<span class="w">    </span><span class="s">&quot;predict_steps_per_second&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="nv">0.204</span>
<span class="p p-Indicator">}</span>
</pre></div>
</div>
</section>
<section id="id11">
<h2>LoRA 模型合并导出<a class="headerlink" href="#id11" title="Link to this heading"></a></h2>
<p>LoRA 模型合并和导出时，可通过指定 <code class="docutils literal notranslate"><span class="pre">export_device</span></code> 参数为 <code class="docutils literal notranslate"><span class="pre">auto</span></code> 来自动检测当前加速卡环境，
启用 NPU 作为导出设备：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="nv">ASCEND_RT_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>llamafactory-cli<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="se">\</span>
<span class="linenos">2</span><span class="w">            </span>--model_name_or_path<span class="w"> </span>qwen/Qwen1.5-7B<span class="w"> </span><span class="se">\</span>
<span class="linenos">3</span><span class="w">            </span>--adapter_name_or_path<span class="w"> </span>./saves/Qwen1.5-7B/lora/sft<span class="w">  </span><span class="se">\</span>
<span class="linenos">4</span><span class="w">            </span>--template<span class="w"> </span>qwen<span class="w"> </span><span class="se">\</span>
<span class="linenos">5</span><span class="w">            </span>--finetuning_type<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
<span class="linenos">6</span><span class="w">            </span>--export_dir<span class="w"> </span>./saves/Qwen1.5-7B/lora/megred-model-path<span class="w"> </span><span class="se">\</span>
<span class="linenos">7</span><span class="w">            </span>--export_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="linenos">8</span><span class="w">            </span>--export_device<span class="w"> </span>auto<span class="w"> </span><span class="se">\</span>
<span class="linenos">9</span><span class="w">            </span>--export_legacy_format<span class="w"> </span>False
</pre></div>
</div>
</section>
<section id="webui-board">
<h2>一站式 webui board 的使用<a class="headerlink" href="#webui-board" title="Link to this heading"></a></h2>
<p>使用 webui 可零代码实现以上功能，启动命令如下：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="nv">ASCEND_RT_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">GRADIO_SHARE</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">GRADIO_SERVER_PORT</span><span class="o">=</span><span class="m">7007</span><span class="w"> </span><span class="nv">GRADIO_SERVER_NAME</span><span class="o">=</span><span class="s2">&quot;0.0.0.0&quot;</span><span class="w"> </span>llamafactory-cli<span class="w"> </span>webui
</pre></div>
</div>
<p>在 webui 实现 Qwen1.5-7B 模型的 LoRA 模型微调、动态推理和模型导出的操作示例：</p>
<iframe width="696" height="422" src="//player.bilibili.com/player.html?bvid=BV1BM3NeAEx1&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe></section>
<section id="api-server">
<h2>API Server的启动与调用<a class="headerlink" href="#api-server" title="Link to this heading"></a></h2>
<p><code class="docutils literal notranslate"><span class="pre">API_PORT</span></code> 为 API 服务的端口号，可替换为自定义端口。通过以下命令启动 API 服务：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="nv">ASCEND_RT_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">API_PORT</span><span class="o">=</span><span class="m">7007</span><span class="w"> </span>llamafactory-cli<span class="w"> </span>api<span class="w"> </span><span class="se">\</span>
<span class="linenos">2</span><span class="w">            </span>--model_name_or_path<span class="w"> </span>qwen/Qwen1.5-7B<span class="w"> </span><span class="se">\</span>
<span class="linenos">3</span><span class="w">            </span>--adapter_name_or_path<span class="w"> </span>./saves/Qwen1.5-7B/lora/sft<span class="w"> </span><span class="se">\</span>
<span class="linenos">4</span><span class="w">            </span>--template<span class="w"> </span>qwen<span class="w"> </span><span class="se">\</span>
<span class="linenos">5</span><span class="w">            </span>--finetuning_type<span class="w"> </span>lora
</pre></div>
</div>
<p>终端输出如下关键信息时，即可在下游任务重通过 API 调用 Qwen1.5-7B</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span>Visit<span class="w"> </span>http://localhost:7007/docs<span class="w"> </span><span class="k">for</span><span class="w"> </span>API<span class="w"> </span>document.
<span class="linenos">2</span>INFO:<span class="w">     </span>Started<span class="w"> </span>server<span class="w"> </span>process<span class="w"> </span><span class="o">[</span><span class="m">2261535</span><span class="o">]</span>
<span class="linenos">3</span>INFO:<span class="w">     </span>Waiting<span class="w"> </span><span class="k">for</span><span class="w"> </span>application<span class="w"> </span>startup.
<span class="linenos">4</span>INFO:<span class="w">     </span>Application<span class="w"> </span>startup<span class="w"> </span>complete.
<span class="linenos">5</span>INFO:<span class="w">     </span>Uvicorn<span class="w"> </span>running<span class="w"> </span>on<span class="w"> </span>http://0.0.0.0:7007<span class="w"> </span><span class="o">(</span>Press<span class="w"> </span>CTRL+C<span class="w"> </span>to<span class="w"> </span>quit<span class="o">)</span>
</pre></div>
</div>
<p>使用 API 调用 Qwen1.5-7B 实现问答聊天的示例代码，通过 <code class="docutils literal notranslate"><span class="pre">message</span></code> 传入您的问题：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="linenos"> 2</span><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="linenos"> 3</span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers.utils.versions</span><span class="w"> </span><span class="kn">import</span> <span class="n">require_version</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span><span class="n">require_version</span><span class="p">(</span><span class="s2">&quot;openai&gt;=1.5.0&quot;</span><span class="p">,</span> <span class="s2">&quot;To fix: pip install openai&gt;=1.5.0&quot;</span><span class="p">)</span>
<span class="linenos"> 6</span>
<span class="linenos"> 7</span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
<span class="linenos"> 8</span>    <span class="c1"># change to your custom port</span>
<span class="linenos"> 9</span>    <span class="n">port</span> <span class="o">=</span> <span class="mi">7007</span>
<span class="linenos">10</span>    <span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span>
<span class="linenos">11</span>        <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;0&quot;</span><span class="p">,</span>
<span class="linenos">12</span>        <span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://localhost:</span><span class="si">{}</span><span class="s2">/v1&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;API_PORT&quot;</span><span class="p">,</span> <span class="mi">7007</span><span class="p">)),</span>
<span class="linenos">13</span>    <span class="p">)</span>
<span class="linenos">14</span>    <span class="n">messages</span> <span class="o">=</span> <span class="p">[]</span>
<span class="linenos">15</span>    <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;hello, what is Ascend NPU&quot;</span><span class="p">})</span>
<span class="linenos">16</span>    <span class="n">result</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>
<span class="linenos">17</span>    <span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="p">)</span>
</pre></div>
</div>
<p>执行成功后可在终端看到如下输出，Qwen1.5-7B 正确介绍了 Ascend NPU：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>ChatCompletionMessage<span class="o">(</span><span class="nv">content</span><span class="o">=</span><span class="s1">&#39;The Ascend NPU, or Neural Processing Unit, is an AI chip developed by Huawei that is designed to accelerate the performance of deep learning and artificial intelligence workloads. It is specifically designed to be energy-efficient, and is intended to be used in a wide range of devices, from smartphones to data centers. The Ascend NPU is designed to support a variety of AI workloads, including object detection, natural language processing, and speech recognition.&#39;</span>,<span class="w"> </span><span class="nv">role</span><span class="o">=</span><span class="s1">&#39;assistant&#39;</span>,<span class="w"> </span><span class="nv">function_call</span><span class="o">=</span>None,<span class="w"> </span><span class="nv">tool_calls</span><span class="o">=</span>None<span class="o">)</span>
</pre></div>
</div>
</section>
<section id="benchmark">
<h2>进阶-大模型主流评测 benchmark<a class="headerlink" href="#benchmark" title="Link to this heading"></a></h2>
<p>通过以下指令启动对 Qwen1.5-7B 模型在 mmlu 数据集的评测：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span>llamafactory-cli<span class="w"> </span><span class="nb">eval</span><span class="w"> </span><span class="se">\</span>
<span class="linenos">2</span><span class="w">    </span>--model_name_or_path<span class="w"> </span>qwen/Qwen1.5-7B<span class="w"> </span><span class="se">\</span>
<span class="linenos">3</span><span class="w">    </span>--template<span class="w"> </span>fewshot<span class="w"> </span><span class="se">\</span>
<span class="linenos">4</span><span class="w">    </span>--task<span class="w"> </span>mmlu<span class="w"> </span><span class="se">\</span>
<span class="linenos">5</span><span class="w">    </span>--split<span class="w"> </span>validation<span class="w"> </span><span class="se">\</span>
<span class="linenos">6</span><span class="w">    </span>--lang<span class="w"> </span>en<span class="w"> </span><span class="se">\</span>
<span class="linenos">7</span><span class="w">    </span>--n_shot<span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="se">\</span>
<span class="linenos">8</span><span class="w">    </span>--batch_size<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
<p>评测完成后，终端输出的评测结果如下，与 Qwen1.5-7B 官方报告对齐：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w">        </span>Average:<span class="w"> </span><span class="m">61</span>.79
<span class="w">           </span>STEM:<span class="w"> </span><span class="m">54</span>.83
Social<span class="w"> </span>Sciences:<span class="w"> </span><span class="m">73</span>.00
<span class="w">     </span>Humanities:<span class="w"> </span><span class="m">55</span>.02
<span class="w">          </span>Other:<span class="w"> </span><span class="m">67</span>.32
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="页脚">
        <a href="multi_npu.html" class="btn btn-neutral float-left" title="单机多卡微调" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="faq.html" class="btn btn-neutral float-right" title="FAQ" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2024, Ascend。</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用的 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a> 开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>
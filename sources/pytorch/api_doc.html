

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>API 文档 &mdash; 昇腾开源  文档</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=ec38875e" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=7d86a446"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../_static/package_info.js?v=2b3ed588"></script>
      <script src="../../_static/statistics.js?v=ed8c2343"></script>
      <script src="../../_static/translations.js?v=beaddf03"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="FAQ" href="faq.html" />
    <link rel="prev" title="功能样例" href="examples.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            昇腾开源
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="搜索文档" aria-label="搜索文档" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="导航菜单">
              <p class="caption" role="heading"><span class="caption-text">开始使用</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../ascend/quick_install.html">快速安装昇腾环境</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">原生支持的AI项目</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">PyTorch</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="install.html">安装指南</a></li>
<li class="toctree-l2"><a class="reference internal" href="quick_start.html">快速开始</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples.html">功能样例</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">API 文档</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu._npu_dropout"><code class="docutils literal notranslate"><span class="pre">torch_npu._npu_dropout()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.copy_memory_"><code class="docutils literal notranslate"><span class="pre">torch_npu.copy_memory_()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.empty_with_format"><code class="docutils literal notranslate"><span class="pre">torch_npu.empty_with_format()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.fast_gelu"><code class="docutils literal notranslate"><span class="pre">torch_npu.fast_gelu()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_alloc_float_status"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_alloc_float_status()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_anchor_response_flags"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_anchor_response_flags()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_apply_adam"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_apply_adam()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_batch_nms"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_batch_nms()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_bert_apply_adam"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_bert_apply_adam()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_bmmV2"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_bmmV2()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_bounding_box_decode"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_bounding_box_decode()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_bounding_box_encode"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_bounding_box_encode()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_broadcast"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_broadcast()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_ciou"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_ciou()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_clear_float_status"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_clear_float_status()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_confusion_transpose"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_confusion_transpose()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_conv2d"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_conv2d()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_conv3d"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_conv3d()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_conv_transpose2d"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_conv_transpose2d()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_convolution"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_convolution()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_convolution_transpose"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_convolution_transpose()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_deformable_conv2d"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_deformable_conv2d()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_diou"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_diou()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_dropout_with_add_softmax"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_dropout_with_add_softmax()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_dtype_cast"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_dtype_cast()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_format_cast"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_format_cast()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_format_cast_"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_format_cast_()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_fused_attention_score"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_fused_attention_score()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_fusion_attention"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_fusion_attention()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_geglu"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_geglu()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_get_float_status"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_get_float_status()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_giou"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_giou()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_grid_assign_positive"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_grid_assign_positive()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_gru"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_gru()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_ifmr"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_ifmr()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_indexing"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_indexing()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_iou"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_iou()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_layer_norm_eval"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_layer_norm_eval()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_linear"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_linear()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_lstm"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_lstm()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_masked_fill_range"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_masked_fill_range()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_max"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_max()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_min"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_min()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_mish"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_mish()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_multi_head_attention"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_multi_head_attention()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_nms_rotated"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_nms_rotated()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_nms_v4"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_nms_v4()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_nms_with_mask"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_nms_with_mask()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_normalize_batch"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_normalize_batch()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_one_hot"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_one_hot()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_pad"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_pad()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_ps_roi_pooling"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_ps_roi_pooling()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_ptiou"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_ptiou()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_random_choice_with_mask"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_random_choice_with_mask()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_reshape"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_reshape()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_rms_norm"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_rms_norm()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_roi_align"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_roi_align()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_rotary_mul"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_rotary_mul()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_rotated_box_decode"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_rotated_box_decode()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_rotated_box_encode"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_rotated_box_encode()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_rotated_iou"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_rotated_iou()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_rotated_overlaps"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_rotated_overlaps()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_scaled_masked_softmax"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_scaled_masked_softmax()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_scatter"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_scatter()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_sign_bits_pack"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_sign_bits_pack()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_sign_bits_unpack"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_sign_bits_unpack()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_silu"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_silu()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_slice"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_slice()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_softmax_cross_entropy_with_logits"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_softmax_cross_entropy_with_logits()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_sort_v2"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_sort_v2()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_stride_add"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_stride_add()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_transpose"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_transpose()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_yolo_boxes_encode"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_yolo_boxes_encode()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.one_"><code class="docutils literal notranslate"><span class="pre">torch_npu.one_()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_swiglu"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_swiglu()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_trans_quant_param"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_trans_quant_param()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_quant_matmul"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_quant_matmul()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_weight_quant_batchmatmul"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_weight_quant_batchmatmul()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_convert_weight_to_int4pack"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_convert_weight_to_int4pack()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_grouped_matmul"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_grouped_matmul()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_quant_scatter"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_quant_scatter()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_quant_scatter_"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_quant_scatter_()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_scatter_nd_update"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_scatter_nd_update()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_scatter_nd_update_"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_scatter_nd_update_()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_anti_quant"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_anti_quant()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_mm_all_reduce_base"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_mm_all_reduce_base()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_ffn"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_ffn()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_incre_flash_attention"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_incre_flash_attention()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_prompt_flash_attention"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_prompt_flash_attention()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch_npu.npu_fused_infer_attention_score"><code class="docutils literal notranslate"><span class="pre">torch_npu.npu_fused_infer_attention_score()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="faq.html">FAQ</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../llamafactory/index.html">LLaMA-Factory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerate/index.html">Accelerate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../transformers/index.html">Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deepspeed/index.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnxruntime/index.html">ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../open_clip/index.html">open_clip</a></li>
<li class="toctree-l1"><a class="reference internal" href="../timm/index.html">timm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Diffusers/index.html">Diffusers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../opencv/index.html">OpenCV</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sd_webui/index.html">Stable-Diffusion-WebUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lm_evaluation/index.html">LM-Evalution-Harness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../wenet/index.html">WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../whisper_cpp/index.html">Whisper.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llama_cpp/index.html">Llama.cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sentence_transformers/index.html">Sentence Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../trl/index.html">Transformer Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../opencompass/index.html">OpenCompass</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lm_deploy/index.html">LMDeploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torchchat/index.html">Torchchat</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torchtitan/index.html">TorchTitan</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="移动版导航菜单" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">昇腾开源</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="页面导航">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">PyTorch</a></li>
      <li class="breadcrumb-item active">API 文档</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/sources/pytorch/api_doc.rst.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="api">
<span id="pytorch-api"></span><h1>API 文档<a class="headerlink" href="#api" title="Link to this heading"></a></h1>
<p>PyTorch-NPU 除了提供了 PyTorch 官方算子实现之外，也提供了大量高性能的自定义算子，详细的算子信息以及描述如下所示：</p>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>在运行下述示例之前，需要导入torch_npu扩展包 <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">torch_npu</span></code></p>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu._npu_dropout">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">_npu_dropout</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu._npu_dropout" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu._npu_dropout(self, p) -&gt; (Tensor, Tensor)</p>
<p><strong>功能描述</strong></p>
<p>不使用种子(seed)进行dropout结果计数。与torch.dropout相似，优化NPU设备实现。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 输入张量。</p>
<p>p (Float) - 丢弃概率。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">,</span><span class="mf">4.</span><span class="p">])</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">prob</span> <span class="o">=</span> <span class="mf">0.3</span><span class="o">&gt;&gt;&gt;</span> <span class="n">output</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">_npu_dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">prob</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">2.8571</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span> <span class="mi">98</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">188</span><span class="p">,</span> <span class="mi">186</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="mi">157</span><span class="p">,</span> <span class="mi">175</span><span class="p">,</span> <span class="mi">159</span><span class="p">,</span>  <span class="mi">77</span><span class="p">,</span> <span class="mi">223</span><span class="p">,</span> <span class="mi">127</span><span class="p">,</span>  <span class="mi">79</span><span class="p">,</span> <span class="mi">247</span><span class="p">,</span> <span class="mi">151</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">253</span><span class="p">,</span> <span class="mi">255</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.copy_memory_">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">copy_memory_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.copy_memory_" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.copy_memory_(dst, src, non_blocking=False) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>从src拷贝元素到self张量，并返回self。</p>
<p><strong>参数说明</strong></p>
<p>dst (Tensor) - 拷贝源张量。</p>
<p>src (Tensor) - 返回张量所需数据类型。</p>
<p>non_blocking (Bool,默认值为False) - 如果设置为True且此拷贝位于CPU和NPU之间，则拷贝可能相对于主机异步发生。在其他情况下，此参数没有效果。</p>
<p><strong>约束说明</strong></p>
<p>copy_memory_仅支持NPU张量。copy_memory_的输入张量应具有相同的dtype和设备index。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">IntTensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">IntTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">copy_memory_</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.empty_with_format">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">empty_with_format</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.empty_with_format" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.empty_with_format(size, dtype, layout, device, pin_memory, acl_format)</p>
<p><strong>功能描述</strong></p>
<p>返回一个填充未初始化数据的张量。</p>
<p><strong>参数说明</strong></p>
<p>size (ListInt) - 定义输出张量shape的整数序列。可以是参数数量(可变值)，也可以是列表或元组等集合。</p>
<p>dtype (torch.dtype, 可选，默认值为None) - 返回张量所需数据类型。如果值为None，请使用全局默认值(请参见torch.set_default_tensor_type()).</p>
<p>layout (torch.layout, 可选，默认值为torch.strided) - 返回张量所需布局。</p>
<p>device (torch.device, 可选，默认值为None) - 返回张量的所需设备。</p>
<p>pin_memory (Bool, 可选，默认值为False) - 如果设置此参数，返回张量将分配在固定内存中。</p>
<p>acl_format (Int，默认值为2) - 返回张量所需内存格式。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch_npu</span><span class="o">.</span><span class="n">empty_with_format</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.fast_gelu">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">fast_gelu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.fast_gelu" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.fast_gelu(self) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>gelu的npu实现。支持FakeTensor模式。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 数据类型：float16、float32。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; 示例一：
&gt;&gt;&gt;
&gt;&gt;&gt; x = torch.rand(2).npu()
&gt;&gt;&gt; x
&gt;&gt;&gt; tensor([0.5991, 0.4094], device=&#39;npu:0&#39;)
&gt;&gt;&gt; torch_npu.fast_gelu(x)
&gt;&gt;&gt; tensor([0.4403, 0.2733], device=&#39;npu:0&#39;)
&gt;&gt;&gt; 示例二：
&gt;&gt;&gt;
&gt;&gt;&gt; //FakeTensor模式
&gt;&gt;&gt; from torch._subclasses.fake_tensor import FakeTensorMode
&gt;&gt;&gt; with FakeTensorMode():
&gt;&gt;&gt; ...     x = torch.rand(2).npu()
&gt;&gt;&gt; ...     torch_npu.fast_gelu(x)
&gt;&gt;&gt; FakeTensor(..., device=&#39;npu:0&#39;, size=(2,))
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_alloc_float_status">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_alloc_float_status</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_alloc_float_status" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_alloc_float_status(self) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>生成一个包含8个0的一维张量。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 任何张量。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>    <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_alloc_float_status</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span> <span class="mf">2.2324</span><span class="p">,</span>  <span class="mf">0.2478</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1056</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.1273</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2573</span><span class="p">,</span>  <span class="mf">1.0558</span><span class="p">]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_anchor_response_flags">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_anchor_response_flags</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_anchor_response_flags" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_anchor_response_flags(self, featmap_size, stride, num_base_anchors) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>在单个特征图中生成锚点的责任标志。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 真值框，shape为[batch, 4]的2D张量。</p>
<p>featmap_size (ListInt of length 2) - 特征图大小。</p>
<p>strides (ListInt of length 2) - 当前水平的步长。</p>
<p>num_base_anchors (Int) - base anchors的数量。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_anchor_response_flags</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">60</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">32400</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_apply_adam">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_apply_adam</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_apply_adam" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_apply_adam(beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, use_locking, use_nesterov, out = (var, m, v))</p>
<p><strong>功能描述</strong></p>
<p>adam结果计数。</p>
<p><strong>参数说明</strong></p>
<p>beta1_power (Scalar) - beta1的幂。</p>
<p>beta2_power (Scalar) - beta2的幂。</p>
<p>lr (Scalar) - 学习率。</p>
<p>beta1 (Scalar) - 一阶矩估计值的指数衰减率。</p>
<p>beta2 (Scalar) - 二阶矩估计值的指数衰减率。</p>
<p>epsilon (Scalar) - 添加到分母中以提高数值稳定性的项数。</p>
<p>grad (Tensor) - 梯度。</p>
<p>use_locking (Bool，可选) - 设置为True时使用lock进行更新操作。</p>
<p>use_nesterov (Bool，可选) - 设置为True时采用nesterov更新。</p>
<p>var (Tensor) - 待优化变量。</p>
<p>m (Tensor) - 变量平均值。</p>
<p>v (Tensor) - 变量方差。</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_batch_nms">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_batch_nms</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_batch_nms" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_batch_nms(self, scores, score_threshold, iou_threshold, max_size_per_class, max_total_size, change_coordinate_frame=False, transpose_box=False) -&gt; (Tensor, Tensor, Tensor, Tensor)</p>
<p><strong>功能描述</strong></p>
<p>根据batch分类计算输入框评分，通过评分排序，删除评分高于阈值(iou_threshold)的框，支持多批多类处理。通过NonMaxSuppression(nms)操作可有效删除冗余的输入框，提高检测精度。NonMaxSuppression：抑制不是极大值的元素，搜索局部的极大值，常用于计算机视觉任务中的检测类模型。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 必填值，输入框的tensor，包含batch大小，数据类型Float16，输入示例：[batch_size, num_anchors, q, 4]，其中q=1或q=num_classes。</p>
<p>scores (Tensor) - 必填值，输入tensor，数据类型Float16，输入示例：[batch_size, num_anchors, num_classes]。</p>
<p>score_threshold (Float32) - 必填值，指定评分过滤器的iou_threshold，用于筛选框，去除得分较低的框，数据类型Float32。</p>
<p>iou_threshold (Float32) - 必填值，指定nms的iou_threshold，用于设定阈值，去除高于阈值的的框，数据类型Float32。</p>
<p>max_size_per_class (Int) - 必填值，指定每个类别的最大可选的框数，数据类型Int。</p>
<p>max_total_size (Int) - 必填值，指定每个batch最大可选的框数，数据类型Int。</p>
<p>change_coordinate_frame (Bool，默认值为False) -可选值， 是否正则化输出框坐标矩阵，数据类型Bool。</p>
<p>transpose_box (Bool，默认值为False) - 可选值，确定是否在此op之前插入转置，数据类型Bool。True表示boxes使用4,N排布。 False表示boxes使用过N,4排布。</p>
<p><strong>输出说明</strong></p>
<p>nmsed_boxes (Tensor) - shape为(batch, max_total_size, 4)的3D张量，指定每批次输出的nms框，数据类型Float16。</p>
<p>nmsed_scores (Tensor) - shape为(batch, max_total_size)的2D张量，指定每批次输出的nms分数，数据类型Float16。</p>
<p>nmsed_classes (Tensor) - shape为(batch, max_total_size)的2D张量，指定每批次输出的nms类，数据类型Float16。</p>
<p>nmsed_num (Tensor) - shape为(batch)的1D张量，指定nmsed_boxes的有效数量，数据类型Int32。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">boxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nmsed_boxes</span><span class="p">,</span> <span class="n">nmsed_scores</span><span class="p">,</span> <span class="n">nmsed_classes</span><span class="p">,</span> <span class="n">nmsed_num</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_batch_nms</span><span class="p">(</span><span class="n">boxes</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nmsed_boxes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nmsed_scores</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nmsed_classes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nmsed_num</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_bert_apply_adam">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_bert_apply_adam</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_bert_apply_adam" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_bert_apply_adam(lr, beta1, beta2, epsilon, grad, max_grad_norm, global_grad_norm, weight_decay, step_size=None, adam_mode=0, <a href="#id1"><span class="problematic" id="id2">*</span></a>, out=(var,m,v))</p>
<p><strong>功能描述</strong></p>
<p>adam结果计数。</p>
<p><strong>参数说明</strong></p>
<p>参数:</p>
<p>var (Tensor) - float16或float32类型张量。</p>
<p>m (Tensor) - 数据类型和shape与exp_avg相同。</p>
<p>v (Tensor) - 数据类型和shape与exp_avg相同。</p>
<p>lr (Scalar) - 数据类型与exp_avg相同。</p>
<p>beta1 (Scalar) - 数据类型与exp_avg相同。</p>
<p>beta2 (Scalar) - 数据类型与exp_avg相同。</p>
<p>epsilon (Scalar) - 数据类型与exp_avg相同。</p>
<p>grad (Tensor) - 数据类型和shape与exp_avg相同。</p>
<p>max_grad_norm (Scalar) - 数据类型与exp_avg相同。</p>
<p>global_grad_norm (Scalar) - 数据类型与exp_avg相同。</p>
<p>weight_decay (Scalar) - 数据类型与exp_avg相同。</p>
<p>step_size (Tensor，可选，默认值为None) - shape为(1, )，数据类型与exp_avg一致。</p>
<p>adam_mode (Int，默认值为0) - 选择adam模式。0表示“adam”，1表示“mbert_adam”。</p>
<p>关键字参数:</p>
<p>out (Tensor，可选) - 输出张量。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">var_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">321538</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mf">32.</span><span class="p">,</span> <span class="mf">21.</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">321538</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">321538</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">321538</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">max_grad_norm</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta1</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta2</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-06</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">global_grad_norm</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">var_out</span><span class="p">,</span> <span class="n">m_out</span><span class="p">,</span> <span class="n">v_out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_bert_apply_adam</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">max_grad_norm</span><span class="p">,</span> <span class="n">global_grad_norm</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="p">(</span><span class="n">var_in</span><span class="p">,</span> <span class="n">m_in</span><span class="p">,</span> <span class="n">v_in</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">var_out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span> <span class="mf">14.7733</span><span class="p">,</span> <span class="o">-</span><span class="mf">30.1218</span><span class="p">,</span>  <span class="o">-</span><span class="mf">1.3647</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">16.6840</span><span class="p">,</span>   <span class="mf">7.1518</span><span class="p">,</span>   <span class="mf">8.4872</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_bmmV2">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_bmmV2</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_bmmV2" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_bmmV2(self, mat2, output_sizes) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>将矩阵“a”乘以矩阵“b”，生成“a*b”。支持FakeTensor模式。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 2D或更高维度矩阵张量。数据类型：float16、float32、int32。格式：[ND, NHWC, FRACTAL_NZ]。</p>
<p>mat2 (Tensor) - 2D或更高维度矩阵张量。数据类型：float16、float32、int32。格式：[ND, NHWC, FRACTAL_NZ]。</p>
<p>output_sizes (ListInt，默认值为[]) - 输出的shape，用于matmul的反向传播。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; 示例一：
&gt;&gt;&gt;
&gt;&gt;&gt; mat1 = torch.randn(10, 3, 4).npu()
&gt;&gt;&gt; mat2 = torch.randn(10, 4, 5).npu()
&gt;&gt;&gt; res = torch_npu.npu_bmmV2(mat1, mat2, [])
&gt;&gt;&gt; res.shape
&gt;&gt;&gt; torch.Size([10, 3, 5])
&gt;&gt;&gt; 示例二：
&gt;&gt;&gt;
&gt;&gt;&gt; //FakeTensor模式
&gt;&gt;&gt; from torch._subclasses.fake_tensor import FakeTensorMode
&gt;&gt;&gt; with FakeTensorMode():
&gt;&gt;&gt; ...     mat1 = torch.randn(10, 3, 4).npu()
&gt;&gt;&gt; ...     mat2 = torch.randn(10, 4, 5).npu()
&gt;&gt;&gt; ...     result = torch_npu.npu_bmmV2(mat1, mat2, [])
&gt;&gt;&gt; ...
&gt;&gt;&gt; result
&gt;&gt;&gt; FakeTensor(..., device=&#39;npu:0&#39;, size=(10, 3, 5))
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_bounding_box_decode">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_bounding_box_decode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_bounding_box_decode" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_bounding_box_decode(rois, deltas, means0, means1, means2, means3, stds0, stds1, stds2, stds3, max_shape, wh_ratio_clip) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>根据rois和deltas生成标注框。自定义FasterRcnn算子。</p>
<p><strong>参数说明</strong></p>
<p>rois (Tensor) - 区域候选网络(RPN)生成的region of interests(ROI)。shape为(N,4)数据类型为float32或float16的2D张量。“N”表示ROI的数量， “4”表示“x0”、“x1”、“y0”和“y1”。</p>
<p>deltas (Tensor) - RPN生成的ROI和真值框之间的绝对变化。shape为(N,4)数据类型为float32或float16的2D张量。“N”表示错误数，“4”表示“dx”、“dy”、“dw”和“dh”。</p>
<p>means0 (Float) - index。</p>
<p>means1 (Float) - index。</p>
<p>means2 (Float) - index。</p>
<p>means3 (Float，默认值为[0,0,0,0]) - index。&quot;deltas&quot; = &quot;deltas&quot; x &quot;stds&quot; + &quot;means&quot;</p>
<p>stds0 (Float) - index。</p>
<p>stds1 (Float) - index。</p>
<p>stds2 (Float) - index。</p>
<p>stds3 (Float, 默认值：[1.0,1.0,1.0,1.0]) - index。&quot;deltas&quot; = &quot;deltas&quot; x &quot;stds&quot; + &quot;means&quot;</p>
<p>max_shape (ListInt of length 2) - shape[h, w]，指定传输到网络的图像大小。用于确保转换后的bbox shape不超过“max_shape”。</p>
<p>wh_ratio_clip (Float) -“dw”和“dh”的值在(-wh_ratio_clip, wh_ratio_clip)范围内。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rois</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">]],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">deltas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">],</span> <span class="p">[</span><span class="mf">7.</span><span class="p">,</span><span class="mf">8.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">]],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_bounding_box_decode</span><span class="p">(</span><span class="n">rois</span><span class="p">,</span> <span class="n">deltas</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">2.5000</span><span class="p">,</span> <span class="mf">6.5000</span><span class="p">,</span> <span class="mf">9.0000</span><span class="p">,</span> <span class="mf">9.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">9.0000</span><span class="p">,</span> <span class="mf">9.0000</span><span class="p">,</span> <span class="mf">9.0000</span><span class="p">,</span> <span class="mf">9.0000</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_bounding_box_encode">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_bounding_box_encode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_bounding_box_encode" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_bounding_box_encode(anchor_box, ground_truth_box, means0, means1, means2, means3, stds0, stds1, stds2, stds3) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>计算标注框和ground truth真值框之间的坐标变化。自定义FasterRcnn算子。</p>
<p><strong>参数说明</strong></p>
<p>anchor_box (Tensor) - 输入张量。锚点框。shape为(N,4)数据类型为float32的2D张量。“N”表示标注框的数量，“4”表示“x0”、“x1”、“y0”和“y1”。</p>
<p>ground_truth_box (Tensor) - 输入张量。真值框。shape为(N,4)数据类型为float32的2D张量。“N”表示标注框的数量，“4”表示“x0”、“x1”、“y0”和“y1”。</p>
<p>means0 (Float) - index。</p>
<p>means1 (Float) - index。</p>
<p>means2 (Float) - index。</p>
<p>means3 (Float, 默认值为[0,0,0,0]) - index。 &quot;deltas&quot; = &quot;deltas&quot; x &quot;stds&quot; + &quot;means&quot;</p>
<p>stds0 (Float) - index。</p>
<p>stds1 (Float) - index。</p>
<p>stds2 (Float) - index。</p>
<p>stds3 (Float, 默认值：[1.0,1.0,1.0,1.0]) -index。 &quot;deltas&quot; = &quot;deltas&quot; x &quot;stds&quot; + &quot;means&quot;</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">anchor_box</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">]],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ground_truth_box</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">],</span> <span class="p">[</span><span class="mf">7.</span><span class="p">,</span><span class="mf">8.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">]],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_bounding_box_encode</span><span class="p">(</span><span class="n">anchor_box</span><span class="p">,</span> <span class="n">ground_truth_box</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputtensor</span><span class="p">([[</span><span class="mf">13.3281</span><span class="p">,</span> <span class="mf">13.3281</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">13.3281</span><span class="p">,</span>  <span class="mf">6.6641</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.4922</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_broadcast">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_broadcast</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_broadcast" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_broadcast(self, size) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>返回self张量的新视图，其单维度扩展，结果连续。</p>
<p>张量也可以扩展更多维度，新的维度添加在最前面。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 输入张量。</p>
<p>size (ListInt) - 对应扩展尺寸。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">npu_broadcast</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_ciou">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_ciou</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_ciou" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_ciou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=True, int mode=0, bool atan_sub_flag=False) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>应用基于NPU的CIoU操作。在DIoU的基础上增加了penalty item，并propose CIoU。</p>
<p><strong>参数说明</strong></p>
<p>boxes1 (Tensor)：格式为xywh、shape为(4, n)的预测检测框。</p>
<p>boxes2 (Tensor)：相应的gt检测框，shape为(4, n)。</p>
<p>trans (Bool，默认值为False)：是否有偏移。</p>
<p>is_cross (Bool，默认值为True)：box1和box2之间是否有交叉操作。</p>
<p>mode (Int，默认值为0)：选择CIoU的计算方式。0表示IoU，1表示IoF。</p>
<p>atan_sub_flag (Bool，默认值为False)：是否将正向的第二个值传递给反向。</p>
<p><strong>输出说明</strong></p>
<p>torch.Tensor：mask操作的结果。</p>
<p><strong>约束说明</strong></p>
<p>到目前为止，CIoU向后只支持当前版本中的trans==True、is_cross==False、mode==0('iou')。如果需要反向传播，确保参数正确。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">box1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box1</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box2</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">diou</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">function</span><span class="o">.</span><span class="n">npu_ciou</span><span class="p">(</span><span class="n">box1</span><span class="p">,</span> <span class="n">box2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">ciou</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_clear_float_status">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_clear_float_status</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_clear_float_status" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_clear_float_status(self) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>在每个核中设置地址0x40000的值为0。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 数据类型为float32的张量。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_clear_float_status</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_confusion_transpose">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_confusion_transpose</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_confusion_transpose" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_confusion_transpose(self, perm, shape, transpose_first) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>混淆reshape和transpose运算。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 数据类型：float16、float32、int8、int16、int32、int64、uint8、uint16、uint32、uint64。</p>
<p>perm (ListInt) - self张量的维度排列。</p>
<p>shape (ListInt) - 输入shape。</p>
<p>transpose_first (Bool) - 如果值为True，首先执行transpose，否则先执行reshape。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_confusion_transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">18</span><span class="p">),</span> <span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">18</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y2</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_confusion_transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y2</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_conv2d">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_conv2d</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_conv2d" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_conv2d(input, weight, bias, stride, padding, dilation, groups) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>在由多个输入平面组成的输入图像上应用一个2D卷积。</p>
<p><strong>参数说明</strong></p>
<p>input (Tensor) - shape的输入张量，值为 (minibatch, in_channels, iH, iW)。</p>
<p>weight (Tensor) - shape过滤器，值为 (out_channels, in_channels/groups, kH, kW)。</p>
<p>bias (Tensor, 可选) - shape偏差 (out_channels)。</p>
<p>stride (ListInt) - 卷积核步长。</p>
<p>padding (ListInt) - 输入两侧的隐式填充。</p>
<p>dilation (ListInt) - 内核元素间距。</p>
<p>groups (Int) - 对输入进行分组。In_channels可被组数整除。</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_conv3d">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_conv3d</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_conv3d" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_conv3d(input, weight, bias, stride, padding, dilation, groups) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>在由多个输入平面组成的输入图像上应用一个3D卷积。</p>
<p><strong>参数说明</strong></p>
<p>input (Tensor) - shape的输入张量，值为 (minibatch, in_channels, iT, iH, iW)。</p>
<p>weight (Tensor) - shape过滤器，值为 (out_channels, in_channels/groups, kT, kH, kW)。</p>
<p>bias (Tensor, 可选) - shape偏差 (out_channels)。</p>
<p>stride (ListInt) - 卷积核步长。</p>
<p>padding (ListInt) - 输入两侧的隐式填充。</p>
<p>dilation (ListInt) - 内核元素间距。</p>
<p>groups (Int) - 对输入进行分组。In_channels可被组数整除。</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_conv_transpose2d">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_conv_transpose2d</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_conv_transpose2d" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_conv_transpose2d(input, weight, bias, padding, output_padding, stride, dilation, groups) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>在由多个输入平面组成的输入图像上应用一个2D转置卷积算子，有时这个过程也被称为“反卷积”。</p>
<p><strong>参数说明</strong></p>
<p>input (Tensor) - shape的输入张量，值为 (minibatch, in_channels, iH, iW)。</p>
<p>weight (Tensor) - shape过滤器，值为 (in_channels, out_channels/groups, kH, kW)。</p>
<p>bias (Tensor, 可选) - shape偏差 (out_channels)。</p>
<p>padding (ListInt) - (dilation * (kernel_size - 1) - padding) 用零来填充输入每个维度的两侧。</p>
<p>output_padding (ListInt) - 添加到输出shape每个维度一侧的附加尺寸。</p>
<p>stride (ListInt) - 卷积核步长。</p>
<p>dilation (ListInt) - 内核元素间距。</p>
<p>groups (Int) - 对输入进行分组。In_channels可被组数整除。</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_convolution">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_convolution</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_convolution" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_convolution(input, weight, bias, stride, padding, dilation, groups) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>在由多个输入平面组成的输入图像上应用一个2D或3D卷积。</p>
<p><strong>参数说明</strong></p>
<p>input (Tensor) - shape的输入张量，值为 (minibatch, in_channels, iH, iW) 或 (minibatch, in_channels, iT, iH, iW)。</p>
<p>weight (Tensor) - shape过滤器，值为 (out_channels, in_channels/groups, kH, kW) 或 (out_channels, in_channels/groups, kT, kH, kW)。</p>
<p>bias (Tensor, 可选) - shape偏差 (out_channels)。</p>
<p>stride (ListInt) - 卷积核步长。</p>
<p>padding (ListInt) - 输入两侧的隐式填充。</p>
<p>dilation (ListInt) - 内核元素间距。</p>
<p>groups (Int) - 对输入进行分组。In_channels可被组数整除。</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_convolution_transpose">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_convolution_transpose</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_convolution_transpose" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_convolution_transpose(input, weight, bias, padding, output_padding, stride, dilation, groups) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>在由多个输入平面组成的输入图像上应用一个2D或3D转置卷积算子，有时这个过程也被称为“反卷积”。</p>
<p><strong>参数说明</strong></p>
<p>input (Tensor) - shape的输入张量，值为 (minibatch, in_channels, iH, iW) 或 (minibatch, in_channels, iT, iH, iW)。</p>
<p>weight (Tensor) - shape过滤器，值为 (in_channels, out_channels/groups, kH, kW) 或 (in_channels, out_channels/groups, kT, kH, kW)。</p>
<p>bias (Tensor, 可选) - shape偏差 (out_channels)。</p>
<p>padding (ListInt) - (dilation * (kernel_size - 1) - padding) 用零来填充输入每个维度的两侧。</p>
<p>output_padding (ListInt) - 添加到输出shape每个维度一侧的附加尺寸。</p>
<p>stride (ListInt) - 卷积核步长。</p>
<p>dilation (ListInt) - 内核元素间距。</p>
<p>groups (Int) - 对输入进行分组。In_channels可被组数整除。</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_deformable_conv2d">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_deformable_conv2d</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_deformable_conv2d" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_deformable_conv2d(self, weight, offset, bias, kernel_size, stride, padding, dilation=[1,1,1,1], groups=1, deformable_groups=1, modulated=True) -&gt; (Tensor, Tensor)</p>
<p><strong>功能描述</strong></p>
<p>使用预期输入计算变形卷积输出(deformed convolution output)。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 输入图像的4D张量。格式为“NHWC”，数据按以下顺序存储：[batch, in_height, in_width, in_channels]。</p>
<p>weight (Tensor) - 可学习过滤器的4D张量。数据类型需与self相同。格式为“HWCN”，数据按以下顺序存储：[filter_height, filter_width, in_channels / groups, out_channels]。</p>
<p>offset (Tensor) - x-y坐标偏移和掩码的4D张量。格式为“NHWC”，数据按以下顺序存储：[batch, out_height, out_width, deformable_groups * filter_height * filter_width * 3]。</p>
<p>bias (Tensor，可选) - 过滤器输出附加偏置(additive bias)的1D张量，数据按[out_channels]的顺序存储。</p>
<p>kernel_size (ListInt of length 2) - 内核大小，2个整数的元组/列表。</p>
<p>stride (ListInt) - 4个整数的列表，表示每个输入维度的滑动窗口步长。维度顺序根据self的数据格式解释。N维和C维必须设置为1。</p>
<p>padding (ListInt) - 4个整数的列表，表示要添加到输入每侧(顶部、底部、左侧、右侧)的像素数。</p>
<p>dilations (ListInt，默认值为[1, 1, 1, 1]) - 4个整数的列表，表示输入每个维度的膨胀系数(dilation factor)。维度顺序根据self的数据格式解释。N维和C维必须设置为1。</p>
<p>groups (Int，默认值为1) - int32类型单整数，表示从输入通道到输出通道的阻塞连接数。In_channels和out_channels需都可被“groups”数整除。</p>
<p>deformable_groups (Int，默认值为1) - int32类型单整数，表示可变形组分区的数量。In_channels需可被“deformable_groups”数整除。</p>
<p>modulated (Bool，可选，默认值为True) - 指定DeformableConv2D版本。True表示v2版本, False表示v1版本，目前仅支持v2。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">75</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_deformable_conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">stride</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">padding</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_diou">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_diou</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_diou" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_diou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>应用基于NPU的DIoU操作。考虑到目标之间距离，以及距离和范围的重叠率，不同目标或边界需趋于稳定。</p>
<p><strong>参数说明</strong></p>
<p>boxes1 (Tensor) - 格式为xywh、shape为(4, n)的预测检测框。</p>
<p>boxes2 (Tensor) - 相应的gt检测框，shape为(4, n)。</p>
<p>trans (Bool，默认值为False) - 是否有偏移。</p>
<p>is_cross (Bool，默认值为False) - box1和box2之间是否有交叉操作。</p>
<p>mode (Int，默认值为0) - 选择DIoU的计算方式。0表示IoU，1表示IoF。</p>
<p><strong>输出说明</strong></p>
<p>torch.Tensor (Tensor) - mask操作的结果。</p>
<p><strong>约束说明</strong></p>
<p>到目前为止，DIoU向后只支持当前版本中的trans==True、is_cross==False、mode==0('iou')。如果需要反向传播，确保参数正确。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">box1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box1</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box2</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">diou</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">function</span><span class="o">.</span><span class="n">npu_diou</span><span class="p">(</span><span class="n">box1</span><span class="p">,</span> <span class="n">box2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">diou</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_dropout_with_add_softmax">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_dropout_with_add_softmax</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_dropout_with_add_softmax" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_dropout_with_add_softmax(Tensor self, Tensor x1, Scalar alpha, float prob, int dim) -&gt; (Tensor, Tensor, Tensor)</p>
<p><strong>功能描述</strong></p>
<p>实现axpy_v2、softmax_v2、drop_out_domask_v3功能。即：</p>
<p>y=x1+ self <a href="#id3"><span class="problematic" id="id4">*</span></a>alpha</p>
<p>Softmax(xi)= exp(xi)/∑jexp(xj)</p>
<p>output = 根据mask舍弃x中的元素，留下来的元素乘(1/prob)</p>
<p><strong>参数说明</strong></p>
<p>Tensor self：4维张量，shape为(N, C, H, W)。</p>
<p>Tensor x1：4维张量，shape为(N, C, H, W)。</p>
<p><strong>约束说明</strong></p>
<p>self和x1的shape相同；</p>
<p>H和W是[128, 256, 384, 512]其中之一；</p>
<p>(N * C)%32结果为0；</p>
<p>dim为-1。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[[</span><span class="mf">7.2556e-02</span><span class="p">,</span> <span class="mf">3.0909e-01</span><span class="p">,</span> <span class="mf">7.9734e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">6.1179e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.2624e-03</span><span class="p">,</span> <span class="mf">8.5186e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">8.9196e-02</span><span class="p">,</span> <span class="mf">3.3319e-01</span><span class="p">,</span> <span class="mf">4.0780e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">1.9144e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.2701e-01</span><span class="p">,</span> <span class="mf">6.4018e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">4.7275e-01</span><span class="p">,</span> <span class="mf">7.4895e-01</span><span class="p">,</span> <span class="mf">4.6215e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">9.3753e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.6048e-02</span><span class="p">,</span> <span class="mf">8.1877e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">7.9366e-01</span><span class="p">,</span> <span class="mf">5.1516e-01</span><span class="p">,</span> <span class="mf">5.6594e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">1.6457e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.0640e-01</span><span class="p">,</span> <span class="mf">3.4322e-03</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.5743e-02</span><span class="p">,</span> <span class="mf">1.2893e-01</span><span class="p">,</span> <span class="mf">5.8990e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">4.1721e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.7816e-02</span><span class="p">,</span> <span class="mf">6.8886e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">4.2980e-01</span><span class="p">,</span> <span class="mf">5.5447e-01</span><span class="p">,</span> <span class="mf">3.1894e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">9.2638e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.9324e-01</span><span class="p">,</span> <span class="mf">4.6225e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">6.2426e-01</span><span class="p">,</span> <span class="mf">4.5948e-01</span><span class="p">,</span> <span class="mf">1.0837e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">8.9386e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.6932e-01</span><span class="p">,</span> <span class="mf">1.2406e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">9.1823e-01</span><span class="p">,</span> <span class="mf">6.2311e-01</span><span class="p">,</span> <span class="mf">5.1474e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">2.1042e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.5943e-01</span><span class="p">,</span> <span class="mf">3.1797e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">5.2891e-01</span><span class="p">,</span> <span class="mf">2.0183e-01</span><span class="p">,</span> <span class="mf">2.1452e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">9.1638e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.4109e-01</span><span class="p">,</span> <span class="mf">9.4484e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">3.7783e-02</span><span class="p">,</span> <span class="mf">1.3218e-01</span><span class="p">,</span> <span class="mf">3.1192e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">2.4931e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.8809e-01</span><span class="p">,</span> <span class="mf">9.6085e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">3.3197e-01</span><span class="p">,</span> <span class="mf">9.1186e-02</span><span class="p">,</span> <span class="mf">2.4839e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">2.1156e-03</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.4952e-01</span><span class="p">,</span> <span class="mf">8.5996e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.7941e-01</span><span class="p">,</span> <span class="mf">5.1532e-01</span><span class="p">,</span> <span class="mf">7.8133e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">3.5526e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.3576e-01</span><span class="p">,</span> <span class="mf">6.0538e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">2.6743e-01</span><span class="p">,</span> <span class="mf">7.4942e-01</span><span class="p">,</span> <span class="mf">1.9146e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">4.9179e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.3319e-01</span><span class="p">,</span> <span class="mf">9.9269e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.5163e-01</span><span class="p">,</span> <span class="mf">3.7388e-01</span><span class="p">,</span> <span class="mf">8.0604e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">8.1193e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.7922e-01</span><span class="p">,</span> <span class="mf">8.6578e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">8.2558e-01</span><span class="p">,</span> <span class="mf">9.5139e-01</span><span class="p">,</span> <span class="mf">2.1313e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">2.1722e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.8402e-01</span><span class="p">,</span> <span class="mf">8.8888e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.8222e-01</span><span class="p">,</span> <span class="mf">2.7645e-01</span><span class="p">,</span> <span class="mf">6.7305e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">6.8003e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.0917e-01</span><span class="p">,</span> <span class="mf">7.6655e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">3.1234e-01</span><span class="p">,</span> <span class="mf">7.8519e-01</span><span class="p">,</span> <span class="mf">8.8509e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">7.2574e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.6134e-01</span><span class="p">,</span> <span class="mf">2.2267e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">4.9233e-01</span><span class="p">,</span> <span class="mf">8.8407e-01</span><span class="p">,</span> <span class="mf">7.4390e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">5.2253e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.5150e-02</span><span class="p">,</span> <span class="mf">4.4108e-02</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">4.3370e-01</span><span class="p">,</span> <span class="mf">2.1176e-01</span><span class="p">,</span> <span class="mf">4.7512e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">5.7611e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.2619e-01</span><span class="p">,</span> <span class="mf">1.1523e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">6.1469e-01</span><span class="p">,</span> <span class="mf">7.4528e-01</span><span class="p">,</span> <span class="mf">7.9559e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">9.7112e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.8391e-01</span><span class="p">,</span> <span class="mf">8.9883e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">8.6677e-02</span><span class="p">,</span> <span class="mf">3.5051e-02</span><span class="p">,</span> <span class="mf">1.6875e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">3.9833e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.7967e-01</span><span class="p">,</span> <span class="mf">4.7062e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">7.1648e-01</span><span class="p">,</span> <span class="mf">1.8378e-01</span><span class="p">,</span> <span class="mf">5.3054e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">8.4282e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.1972e-01</span><span class="p">,</span> <span class="mf">7.0031e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">5.9876e-01</span><span class="p">,</span> <span class="mf">6.7868e-01</span><span class="p">,</span> <span class="mf">6.4128e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">4.9516e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.2571e-01</span><span class="p">,</span> <span class="mf">5.8792e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">7.6723e-01</span><span class="p">,</span> <span class="mf">6.9527e-01</span><span class="p">,</span> <span class="mf">9.3573e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">6.3490e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.6129e-01</span><span class="p">,</span> <span class="mf">2.4517e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">5.0158e-01</span><span class="p">,</span> <span class="mf">8.2565e-01</span><span class="p">,</span> <span class="mf">7.5532e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">6.9342e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.3244e-01</span><span class="p">,</span> <span class="mf">5.3913e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">2.3347e-01</span><span class="p">,</span> <span class="mf">9.7822e-02</span><span class="p">,</span> <span class="mf">1.5009e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">5.5090e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.1813e-01</span><span class="p">,</span> <span class="mf">7.9857e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">7.2416e-02</span><span class="p">,</span> <span class="mf">5.9086e-01</span><span class="p">,</span> <span class="mf">1.2243e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">7.8511e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.4803e-01</span><span class="p">,</span> <span class="mf">5.3717e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">7.4899e-01</span><span class="p">,</span> <span class="mf">1.5467e-02</span><span class="p">,</span> <span class="mf">4.9711e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">2.2938e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.6099e-01</span><span class="p">,</span> <span class="mf">3.1928e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">3.9111e-01</span><span class="p">,</span> <span class="mf">1.2422e-01</span><span class="p">,</span> <span class="mf">6.1795e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">8.4212e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.1346e-01</span><span class="p">,</span> <span class="mf">1.0957e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">3.6311e-02</span><span class="p">,</span> <span class="mf">8.9652e-01</span><span class="p">,</span> <span class="mf">7.7428e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">9.2212e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.9290e-01</span><span class="p">,</span> <span class="mf">4.5609e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">2.2052e-01</span><span class="p">,</span> <span class="mf">4.4260e-01</span><span class="p">,</span> <span class="mf">8.8627e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">9.2381e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.7046e-01</span><span class="p">,</span> <span class="mf">9.2057e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">5.5775e-01</span><span class="p">,</span> <span class="mf">8.8951e-01</span><span class="p">,</span> <span class="mf">7.9238e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">3.9209e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.6636e-01</span><span class="p">,</span> <span class="mf">8.1876e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">3.4709e-01</span><span class="p">,</span> <span class="mf">7.8678e-01</span><span class="p">,</span> <span class="mf">1.4396e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">7.9073e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.9021e-01</span><span class="p">,</span> <span class="mf">8.5285e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.4238e-01</span><span class="p">,</span> <span class="mf">9.8432e-01</span><span class="p">,</span> <span class="mf">2.7802e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">5.1720e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.6290e-01</span><span class="p">,</span> <span class="mf">8.2036e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">2.0184e-01</span><span class="p">,</span> <span class="mf">1.0635e-01</span><span class="p">,</span> <span class="mf">1.9612e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">9.7101e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.6679e-01</span><span class="p">,</span> <span class="mf">7.0811e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">5.8240e-01</span><span class="p">,</span> <span class="mf">3.1642e-01</span><span class="p">,</span> <span class="mf">9.6549e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">5.1130e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.6725e-01</span><span class="p">,</span> <span class="mf">3.5238e-01</span><span class="p">]]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[[</span><span class="mf">2.4353e-01</span><span class="p">,</span> <span class="mf">8.5665e-01</span><span class="p">,</span> <span class="mf">5.3571e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">5.9101e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.0872e-01</span><span class="p">,</span> <span class="mf">6.3873e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.4489e-01</span><span class="p">,</span> <span class="mf">8.7982e-01</span><span class="p">,</span> <span class="mf">3.3114e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">2.5155e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.4987e-01</span><span class="p">,</span> <span class="mf">8.7096e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">6.5837e-02</span><span class="p">,</span> <span class="mf">2.2677e-02</span><span class="p">,</span> <span class="mf">7.2063e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">2.3542e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.3041e-01</span><span class="p">,</span> <span class="mf">8.9596e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">5.1450e-01</span><span class="p">,</span> <span class="mf">7.9412e-01</span><span class="p">,</span> <span class="mf">8.9288e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">3.3639e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.6086e-01</span><span class="p">,</span> <span class="mf">4.8770e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">4.7557e-01</span><span class="p">,</span> <span class="mf">1.4793e-01</span><span class="p">,</span> <span class="mf">4.9800e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">3.9479e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.6052e-01</span><span class="p">,</span> <span class="mf">9.8271e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">7.4438e-01</span><span class="p">,</span> <span class="mf">7.5646e-01</span><span class="p">,</span> <span class="mf">2.7942e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">3.0381e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.3703e-01</span><span class="p">,</span> <span class="mf">1.4037e-02</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">4.0232e-01</span><span class="p">,</span> <span class="mf">9.4407e-01</span><span class="p">,</span> <span class="mf">6.4969e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">3.4524e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.2647e-01</span><span class="p">,</span> <span class="mf">5.4792e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.1801e-01</span><span class="p">,</span> <span class="mf">1.8281e-01</span><span class="p">,</span> <span class="mf">6.1723e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">1.9393e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.5877e-01</span><span class="p">,</span> <span class="mf">8.9990e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">2.6244e-01</span><span class="p">,</span> <span class="mf">6.9614e-01</span><span class="p">,</span> <span class="mf">3.6008e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">5.0258e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.1919e-01</span><span class="p">,</span> <span class="mf">4.6943e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">7.4710e-01</span><span class="p">,</span> <span class="mf">5.8911e-01</span><span class="p">,</span> <span class="mf">1.5292e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">6.6590e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.0754e-01</span><span class="p">,</span> <span class="mf">3.6944e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">9.0501e-01</span><span class="p">,</span> <span class="mf">2.7943e-01</span><span class="p">,</span> <span class="mf">3.7068e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">1.5053e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.3413e-01</span><span class="p">,</span> <span class="mf">7.9626e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">9.5200e-01</span><span class="p">,</span> <span class="mf">7.8327e-01</span><span class="p">,</span> <span class="mf">3.4033e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">8.0892e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.0480e-01</span><span class="p">,</span> <span class="mf">3.8717e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">7.5938e-01</span><span class="p">,</span> <span class="mf">2.9089e-01</span><span class="p">,</span> <span class="mf">5.9916e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">6.2526e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.9670e-01</span><span class="p">,</span> <span class="mf">3.3548e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">7.0733e-01</span><span class="p">,</span> <span class="mf">8.1400e-01</span><span class="p">,</span> <span class="mf">4.9259e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">1.6607e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.5331e-01</span><span class="p">,</span> <span class="mf">7.3150e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">5.2770e-01</span><span class="p">,</span> <span class="mf">7.8141e-01</span><span class="p">,</span> <span class="mf">4.1904e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">3.8917e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.1405e-01</span><span class="p">,</span> <span class="mf">9.9596e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">4.8669e-01</span><span class="p">,</span> <span class="mf">9.9948e-01</span><span class="p">,</span> <span class="mf">1.2023e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">7.0420e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.8522e-01</span><span class="p">,</span> <span class="mf">6.6192e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">4.9718e-01</span><span class="p">,</span> <span class="mf">7.5792e-01</span><span class="p">,</span> <span class="mf">6.6748e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">9.7302e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.3443e-01</span><span class="p">,</span> <span class="mf">3.6536e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">7.7033e-01</span><span class="p">,</span> <span class="mf">6.0550e-01</span><span class="p">,</span> <span class="mf">8.2024e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">2.9711e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.9410e-01</span><span class="p">,</span> <span class="mf">6.6304e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">1.0284e-01</span><span class="p">,</span> <span class="mf">6.5712e-01</span><span class="p">,</span> <span class="mf">6.0831e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">6.2622e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.0355e-01</span><span class="p">,</span> <span class="mf">9.4250e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">4.9053e-01</span><span class="p">,</span> <span class="mf">2.0148e-01</span><span class="p">,</span> <span class="mf">2.4974e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">9.2521e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.9919e-01</span><span class="p">,</span> <span class="mf">4.4700e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">7.6515e-01</span><span class="p">,</span> <span class="mf">8.7755e-01</span><span class="p">,</span> <span class="mf">1.3500e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">8.2136e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.0848e-01</span><span class="p">,</span> <span class="mf">5.6432e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">3.3618e-01</span><span class="p">,</span> <span class="mf">1.8585e-01</span><span class="p">,</span> <span class="mf">5.3475e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">4.9333e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.1018e-01</span><span class="p">,</span> <span class="mf">9.5052e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">2.1400e-01</span><span class="p">,</span> <span class="mf">1.7407e-01</span><span class="p">,</span> <span class="mf">5.8925e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">7.5722e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.9850e-01</span><span class="p">,</span> <span class="mf">3.9298e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">6.3625e-01</span><span class="p">,</span> <span class="mf">1.7168e-01</span><span class="p">,</span> <span class="mf">2.9183e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">9.9674e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.1718e-01</span><span class="p">,</span> <span class="mf">5.2626e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">1.8651e-01</span><span class="p">,</span> <span class="mf">2.5385e-01</span><span class="p">,</span> <span class="mf">2.0384e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">3.4462e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.4150e-01</span><span class="p">,</span> <span class="mf">4.7431e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">2.4992e-01</span><span class="p">,</span> <span class="mf">1.1788e-01</span><span class="p">,</span> <span class="mf">1.9730e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">4.3722e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.8943e-01</span><span class="p">,</span> <span class="mf">9.9097e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.4493e-02</span><span class="p">,</span> <span class="mf">6.4856e-01</span><span class="p">,</span> <span class="mf">8.3344e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">8.6623e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.5456e-01</span><span class="p">,</span> <span class="mf">7.8423e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">6.1458e-01</span><span class="p">,</span> <span class="mf">4.4260e-01</span><span class="p">,</span> <span class="mf">7.4133e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">2.5126e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.7251e-01</span><span class="p">,</span> <span class="mf">6.9784e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">2.2419e-01</span><span class="p">,</span> <span class="mf">3.4159e-01</span><span class="p">,</span> <span class="mf">2.3232e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">8.2850e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.2644e-02</span><span class="p">,</span> <span class="mf">4.8390e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.0171e-01</span><span class="p">,</span> <span class="mf">8.7662e-01</span><span class="p">,</span> <span class="mf">2.0457e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">7.6868e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.6592e-01</span><span class="p">,</span> <span class="mf">3.1254e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">1.8866e-01</span><span class="p">,</span> <span class="mf">1.5755e-01</span><span class="p">,</span> <span class="mf">3.1025e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">6.5044e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.8293e-01</span><span class="p">,</span> <span class="mf">9.8030e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">3.7703e-01</span><span class="p">,</span> <span class="mf">5.3198e-01</span><span class="p">,</span> <span class="mf">1.8633e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">4.7398e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.3618e-01</span><span class="p">,</span> <span class="mf">8.7283e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">5.7119e-01</span><span class="p">,</span> <span class="mf">4.3620e-01</span><span class="p">,</span> <span class="mf">8.2536e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">2.5390e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.6144e-01</span><span class="p">,</span> <span class="mf">4.4044e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.3243e-01</span><span class="p">,</span> <span class="mf">6.2002e-02</span><span class="p">,</span> <span class="mf">7.5278e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">7.5907e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.2472e-01</span><span class="p">,</span> <span class="mf">1.7624e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">4.7985e-01</span><span class="p">,</span> <span class="mf">7.9769e-01</span><span class="p">,</span> <span class="mf">8.1433e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">7.3780e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.2877e-02</span><span class="p">,</span> <span class="mf">4.8816e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">4.5100e-01</span><span class="p">,</span> <span class="mf">9.9698e-02</span><span class="p">,</span> <span class="mf">7.0776e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">9.8046e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.2372e-01</span><span class="p">,</span> <span class="mf">8.6304e-01</span><span class="p">]]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_dropout_with_add_softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0639</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0632</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0794</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.1571</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.1270</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.1030</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.2134</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0342</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0633</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.1578</span><span class="p">,</span> <span class="mf">0.1334</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.2316</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0237</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.2128</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.1421</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0499</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0218</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.1461</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.1130</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.1976</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">]]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_dtype_cast">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_dtype_cast</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_dtype_cast" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_dtype_cast(input, dtype) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>执行张量数据类型(dtype)转换。支持FakeTensor模式。</p>
<p><strong>参数说明</strong></p>
<p>input (Tensor) - 输入张量。</p>
<p>dtype (torch.dtype) - 返回张量的目标数据类型。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; 示例一：
&gt;&gt;&gt;
&gt;&gt;&gt; torch_npu.npu_dtype_cast(torch.tensor([0, 0.5, -1.]).npu(), dtype=torch.int)
&gt;&gt;&gt; tensor([ 0,  0, -1], device=&#39;npu:0&#39;, dtype=torch.int32)
&gt;&gt;&gt; 示例二：
&gt;&gt;&gt;
&gt;&gt;&gt; //FakeTensor模式
&gt;&gt;&gt; from torch._subclasses.fake_tensor import FakeTensorMode
&gt;&gt;&gt; with FakeTensorMode():
&gt;&gt;&gt; ...     x = torch.rand(2, dtype=torch.float32).npu()
&gt;&gt;&gt; ...     res = torch_npu.npu_dtype_cast(x, torch.float16)
&gt;&gt;&gt; ...
&gt;&gt;&gt; res
&gt;&gt;&gt; FakeTensor(..., device=&#39;npu:0&#39;, size=(2,), dtype=torch.float16)
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_format_cast">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_format_cast</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_format_cast" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_format_cast(self, acl_format) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>修改NPU张量的格式。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 输入张量。</p>
<p>acl_format (Int) - 目标格式。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch_npu</span><span class="o">.</span><span class="n">get_npu_format</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">npu_format_cast</span><span class="p">(</span><span class="mi">29</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch_npu</span><span class="o">.</span><span class="n">get_npu_format</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">29</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_format_cast_">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_format_cast_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_format_cast_" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_format_cast_(self, src) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>原地修改self张量格式，与src格式保持一致。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 输入张量。</p>
<p>src (Tensor，int) - 目标格式。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch_npu</span><span class="o">.</span><span class="n">get_npu_format</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch_npu</span><span class="o">.</span><span class="n">get_npu_format</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">npu_format_cast_</span><span class="p">(</span><span class="mi">29</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">29</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_fused_attention_score">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_fused_attention_score</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_fused_attention_score" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_fused_attention_score(Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor attention_mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool bmm_score_transpose_a=False, bool bmm_score_transpose_b=False, bool value_transpose=False, bool dx_transpose=False) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>实现“Transformer attention score”的融合计算逻辑，主要将matmul、transpose、add、softmax、dropout、batchmatmul、permute等计算进行了融合。</p>
<p><strong>参数说明</strong></p>
<p>query_layer：Tensor类型，仅支持float16。</p>
<p>key_layer：Tensor类型，仅支持float16。</p>
<p>value_layer：Tensor类型，仅支持float16 。</p>
<p>attention_mask：Tensor类型，仅支持float16 。</p>
<p>scale：缩放系数，浮点数标量 。</p>
<p>keep_prob：不做dropout的概率，0-1之间，浮点数。</p>
<p>query_transpose：query是否做转置，bool类型，默认为False 。</p>
<p>key_transpose：key是否做转置，bool类型，默认为False 。</p>
<p>bmm_score_transpose_a：bmm计算中左矩阵是否做转置，bool类型，默认为False。</p>
<p>bmm_score_transpose_b：bmm计算中右矩阵是否做转置，bool类型，默认为False。</p>
<p>value_transpose：value是否做转置，bool类型，默认为False。</p>
<p>dx_transpose：反向计算时dx是否做转置，bool类型，默认为False。</p>
<p><strong>约束说明</strong></p>
<p>输入tensor的格式编号必须均为29，数据类型为FP16。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">query_layer</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_format_cast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span> <span class="p">,</span> <span class="mi">29</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">query_layer</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_format_cast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">(),</span> <span class="mi">29</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">key_layer</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_format_cast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">(),</span> <span class="mi">29</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value_layer</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_format_cast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">(),</span> <span class="mi">29</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_format_cast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">(),</span> <span class="mi">29</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="mf">0.125</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">keep_prob</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context_layer</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_fused_attention_score</span><span class="p">(</span><span class="n">query_layer</span><span class="p">,</span> <span class="n">key_layer</span><span class="p">,</span> <span class="n">value_layer</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">context_layer</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.5063</span><span class="p">,</span> <span class="mf">0.4900</span><span class="p">,</span> <span class="mf">0.4951</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.5493</span><span class="p">,</span> <span class="mf">0.5249</span><span class="p">,</span> <span class="mf">0.5400</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.4844</span><span class="p">,</span> <span class="mf">0.4724</span><span class="p">,</span> <span class="mf">0.4927</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.5176</span><span class="p">,</span> <span class="mf">0.4702</span><span class="p">,</span> <span class="mf">0.4790</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.4683</span><span class="p">,</span> <span class="mf">0.4771</span><span class="p">,</span> <span class="mf">0.5054</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.4917</span><span class="p">,</span> <span class="mf">0.4614</span><span class="p">,</span> <span class="mf">0.4739</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.5137</span><span class="p">,</span> <span class="mf">0.5010</span><span class="p">,</span> <span class="mf">0.5078</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.4656</span><span class="p">,</span> <span class="mf">0.4592</span><span class="p">,</span> <span class="mf">0.5034</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.5425</span><span class="p">,</span> <span class="mf">0.5732</span><span class="p">,</span> <span class="mf">0.5347</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.5054</span><span class="p">,</span> <span class="mf">0.5024</span><span class="p">,</span> <span class="mf">0.4924</span><span class="p">],</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_fusion_attention">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_fusion_attention</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_fusion_attention" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_fusion_attention(Tensor query, Tensor key, Tensor value, int head_num, str input_layout, Tensor? pse=None, Tensor? padding_mask=None, Tensor? atten_mask=None, float scale=1., float keep_prob=1., int pre_tockens=2147483647, int next_tockens=2147483647, int inner_precise=1, int[]? prefix=None, int sparse_mode=0, bool gen_mask_parallel=True, bool sync=False ) -&gt; (Tensor, Tensor, Tensor, Tensor, int, int, int)</p>
<p><strong>参数说明</strong></p>
<p>query：Device侧的Tensor，公式中输入Q，数据类型支持FLOAT、FLOAT16、BFLOAT16，数据格式支持ND。</p>
<p>key：Device侧的Tensor，公式中输入K，数据类型支持FLOAT、FLOAT16、BFLOAT16，数据格式支持ND。</p>
<p>value：Device侧的Tensor，公式中输入V，数据类型支持FLOAT、FLOAT16、BFLOAT16，数据格式支持ND。</p>
<p>pse：Device侧的Tensor，公式中输入pse，可选参数，表示位置编码。数据类型支持FLOAT、FLOAT16、BFLOAT16，数据格式支持ND。四维输入，参数每个batch不相同，BNSS格式；每个batch相同，1NSS格式。alibi位置编码,如果S大于1024且下三角掩码场景,只输入下三角倒数1024行进行内存优化，参数每个batch不相同，输入BNHS ,每个batch相同，输入1NHS(H=1024)。</p>
<p>dropMask：Device侧的Tensor，可选属性，数据类型支持UINT8(标识8个1bit BOOL)，数据格式支持ND。</p>
<p>paddingMask：Device侧的Tensor，暂不支持该传参。</p>
<p>attenMask：Device侧的Tensor，可选属性，代表下三角全为0上三角全为负无穷的倒三角mask矩阵，数据类型支持BOOL(8bit的BOOL)、UINT8，数据格式支持ND。</p>
<p>prefix：Device侧的Tensor，可选属性，代表prefix稀疏计算场景每个Batch的N值。数据类型支持INT64，数据格式支持ND。</p>
<p>scale：Host侧的double，公式中d开根号的倒数，代表缩放系数，作为计算流中Muls的scalar值，数据类型支持DOUBLE。</p>
<p>keepProb：Host侧的double，可选参数，代表dropMask中1的比例，数据类型支持DOUBLE。</p>
<p>preTokens：Host侧的int64_t，用于稀疏计算的参数，可选参数，数据类型支持INT64。</p>
<p>nextTokens：Host侧的int64_t，用于稀疏计算的参数，可选参数，数据类型支持INT64。</p>
<p>headNum：Host侧的int64_t，代表head个数，数据类型支持INT64。</p>
<p>inputLayout：Host侧的string，代表输入query、key、value的数据排布格式，支持BSH、SBH、BSND、BNSD。</p>
<p>innerPrecise：Host侧的int32_t，数据类型支持INT32，保留参数，暂未使用。</p>
<p>sparseMode：Host侧的int，表示sparse的模式。数据类型支持：INT64。</p>
<p>sparseMode为0时，代表defaultMask模式，如果attenmask未传入则不做mask操作，忽略preTokens和nextTokens(内部赋值为INT_MAX)；如果传入，则需要传入完整的attenmask矩阵(S1 * S2)，表示preTokens和nextTokens之间的部分需要计算。</p>
<p>sparseMode为为1时，代表allMask，即传入完整的attenmask矩阵。。</p>
<p>sparseMode为2时，代表leftUpCausal模式的mask，对应以左顶点为划分的下三角场景，需要传入优化后的attenmask矩阵(2048*2048)。</p>
<p>sparseMode为3时，代表rightDownCausal模式的mask，对应以右下顶点为划分的下三角场景，需要传入优化后的attenmask矩阵(2048*2048)。</p>
<p>sparseMode为为4时，代表band场景，即计算preTokens和nextTokens之间的部分。</p>
<p>sparseMode为为5时，代表prefix场景，即在rightDownCasual的基础上，左侧加上一个长为S1，宽为N的矩阵，N的值由新增的输入prefix获取，且每个Batch轴的N值不一样。</p>
<p>sparseMode为为6、7、8时，分别代表global、dilated、block_local，均暂不支持。</p>
<p>gen_mask_parallel：debug参数，DSA生成dropout随机数向量mask的控制开关，默认值为True：同AICORE计算并行，False：同AICORE计算串行</p>
<p>sync：debug参数，DSA生成dropout随机数向量mask的控制开关，默认值为False：dropout mask异步生成，True：dropout mask同步生成</p>
<p><strong>输出说明</strong></p>
<p>共7个输出</p>
<p>(Tensor, Tensor, Tensor, Tensor, int, int, int)</p>
<p>第1个输出为Tensor，计算公式的最终输出y。</p>
<p>第2个输出为Tensor，Softmax 计算的Max中间结果，用于反向计算。</p>
<p>第3个输出为Tensor，Softmax计算的Sum中间结果，用于反向计算。</p>
<p>第4个输出为Tensor，保留参数，暂未使用。</p>
<p>第5个输出为int，DSA生成dropoutmask中，Philox算法的seed。</p>
<p>第6个输出为int，DSA生成dropoutmask中，Philox算法的offset。</p>
<p>第7个输出为int，DSA生成dropoutmask的长度。</p>
<p><strong>约束说明</strong></p>
<p>输入query、key、value的B：batchsize必须相等。</p>
<p>输入query的N和key/value的N 必须成比例关系，即Nq/Nkv必须是非0整数，当Nq/Nkv &gt; 1时，即为GQA，当Nkv=1时，即为MQA。</p>
<p>输入key/value的shape必须一致。</p>
<p>输入query、key、value的S：sequence length，取值范围1~32K，且为16的倍数。</p>
<p>输入query、key、value的D：head dim，取值范围64、80、96、120、128、256。</p>
<p>当pre_tockens&lt;Sq 的时候, 使能band sparse计算，pre_tockens不能小于0。</p>
<p>当next_tockens&lt;Skv的时候，使能bandsparse计算，next_tokens不能小于0。</p>
<p>当pre_tokens &gt;= Sq，同时next_tokens=0时，使能causal计算。</p>
<p>在使能band sparse、causal计算时，必须输入atten_mask。</p>
<p>当所有的attenmask的shape小于2048且相同的时候，建议使用default模式，即sparse_mode配置为0，来减少内存使用量；sparse_mode配置为2或3时，禁止配置preTokens、nextTokens。</p>
<p>支持的PyTorch版本</p>
<p>PyTorch 2.1</p>
<p>PyTorch 2.0</p>
<p>PyTorch 1.11.0</p>
<p>支持的型号</p>
<p>Atlas 训练系列产品</p>
<p>Atlas A2训练系列产品</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">unittest</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch_npu.testing.testcase</span><span class="w"> </span><span class="kn">import</span> <span class="n">TestCase</span><span class="p">,</span> <span class="n">run_tests</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch_npu.testing.common_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_npu_device</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">DEVICE_NAME</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="mi">0</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">TestNPUFlashAttention</span><span class="p">(</span><span class="n">TestCase</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">supported_op_exec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="mf">0.08838</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softmax_res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">qk</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">softmax_res</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">return</span> <span class="n">output</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">custom_op_exec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="mf">0.08838</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">return</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_fusion_attention</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">head_num</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">input_layout</span><span class="o">=</span><span class="s2">&quot;BSH&quot;</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">trans_BNSD2BSH</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_geglu">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_geglu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_geglu" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu. npu_geglu(Tensor self, int dim=-1, int approximate=1) -&gt; (Tensor, Tensor)</p>
<p><strong>功能描述</strong></p>
<p>对输入Tensor完成GeGlu运算。</p>
<p><strong>参数说明</strong></p>
<p>Tensor self：待进行GeGlu计算的入参，npu device侧的aclTensor，数据类型支持FLOAT32、FLOAT16、BFLOAT16(Atlas A2 训练系列产品支持)，支持非连续的Tensor，数据格式支持ND。</p>
<p>int dim：可选入参，设定的slice轴，数据类型支持INT64。</p>
<p>int approximate：可选入参，GeGlu计算使用的激活函数索引，0表示使用none，1表示使用tanh，数据类型支持INT64。</p>
<p>out：GeGlu计算的出参，npu device侧的aclTensor，数据类型必须和self一致，支持非连续的Tensor，数据格式支持ND。</p>
<p>outGelu：GeGlu计算的出参，npu device侧的aclTensor，数据类型必须和self一致，支持非连续的Tensor，数据格式支持ND。</p>
<p><strong>约束说明</strong></p>
<p>out、outGelu在dim维的size等于self在dim维size的1/2。</p>
<p>当self.dim()==0时，dim的取值在[-1, 0]范围内；当self.dim()&gt;0时，取值在[-self.dim(), self.dim()-1]范围内。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span><span class="mi">9216</span><span class="p">,</span><span class="mi">2560</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_npu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_x</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_npu</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span> <span class="mf">0.8750</span><span class="p">,</span>  <span class="mf">0.4766</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3535</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4619</span><span class="p">,</span>  <span class="mf">0.3542</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8389</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.9424</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0291</span><span class="p">,</span>  <span class="mf">0.9482</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.5640</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2959</span><span class="p">,</span>  <span class="mf">1.7666</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.4958</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6787</span><span class="p">,</span>  <span class="mf">0.0179</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.4365</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8311</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7676</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.1611</span><span class="p">,</span>  <span class="mf">1.4766</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1934</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5913</span><span class="p">,</span>  <span class="mf">1.1553</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4626</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.4873</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8105</span><span class="p">,</span>  <span class="mf">0.5723</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.3193</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1558</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6191</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.6816</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2080</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6953</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3096</span><span class="p">,</span>  <span class="mf">0.4158</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2168</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">1.4287</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9863</span><span class="p">,</span>  <span class="mf">1.4053</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7676</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6709</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1582</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.3281</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9043</span><span class="p">,</span>  <span class="mf">0.7725</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5596</span><span class="p">,</span>  <span class="mf">0.1632</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0732</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.0254</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6650</span><span class="p">,</span>  <span class="mf">0.1318</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8159</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7134</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4536</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0327</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6206</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1492</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2559</span><span class="p">,</span>  <span class="mf">0.3777</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2822</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.1904</span><span class="p">,</span>  <span class="mf">1.1260</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3369</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4814</span><span class="p">,</span>  <span class="mf">0.4463</span><span class="p">,</span>  <span class="mf">1.0205</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.1192</span><span class="p">,</span>  <span class="mf">1.7783</span><span class="p">,</span>  <span class="mf">0.1040</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.0010</span><span class="p">,</span>  <span class="mf">1.5342</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5728</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">0.3296</span><span class="p">,</span>  <span class="mf">0.5703</span><span class="p">,</span>  <span class="mf">0.6338</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.2131</span><span class="p">,</span>  <span class="mf">1.1113</span><span class="p">,</span>  <span class="mf">0.9854</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.4336</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7568</span><span class="p">,</span>  <span class="mf">1.8164</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2012</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8721</span><span class="p">,</span>  <span class="mf">0.6904</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.6934</span><span class="p">,</span>  <span class="mf">0.3743</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9448</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9946</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6494</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3564</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.1855</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9663</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8252</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.2285</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5684</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4277</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.1260</span><span class="p">,</span>  <span class="mf">1.2871</span><span class="p">,</span>  <span class="mf">1.2754</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5171</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1064</span><span class="p">,</span>  <span class="mf">0.9624</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.4639</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0661</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7178</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.2656</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9023</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1641</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">1.8350</span><span class="p">,</span>  <span class="mf">1.0625</span><span class="p">,</span>  <span class="mf">1.6172</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.4160</span><span class="p">,</span>  <span class="mf">1.2490</span><span class="p">,</span>  <span class="mf">1.9775</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.5615</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9990</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5996</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9404</span><span class="p">,</span>  <span class="mf">0.5068</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9829</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.0771</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5537</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5654</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.4678</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5215</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7920</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.3389</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3228</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1514</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.8882</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9971</span><span class="p">,</span>  <span class="mf">1.2432</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.5439</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8154</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9238</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.2556</span><span class="p">,</span>  <span class="mf">0.2131</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7471</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.1074</span><span class="p">,</span>  <span class="mf">1.0391</span><span class="p">,</span>  <span class="mf">0.1556</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.1689</span><span class="p">,</span>  <span class="mf">0.6470</span><span class="p">,</span>  <span class="mf">0.2463</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">1.2617</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8911</span><span class="p">,</span>  <span class="mf">1.9160</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3027</span><span class="p">,</span>  <span class="mf">1.7764</span><span class="p">,</span>  <span class="mf">0.3381</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.4160</span><span class="p">,</span>  <span class="mf">1.6201</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5396</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.8271</span><span class="p">,</span>  <span class="mf">1.3086</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8770</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.8252</span><span class="p">,</span>  <span class="mf">1.3779</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3535</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5215</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4727</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0420</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.4600</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7617</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7754</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.4697</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4734</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3838</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.8506</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3945</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0142</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3447</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6587</span><span class="p">,</span>  <span class="mf">0.5728</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.1523</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8027</span><span class="p">,</span>  <span class="mf">0.4731</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.5464</span><span class="p">,</span>  <span class="mf">1.4014</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8594</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">0.1467</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5752</span><span class="p">,</span>  <span class="mf">0.3298</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9902</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8281</span><span class="p">,</span>  <span class="mf">1.8506</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.2473</span><span class="p">,</span>  <span class="mf">1.0693</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8184</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.9277</span><span class="p">,</span>  <span class="mf">1.6543</span><span class="p">,</span>  <span class="mf">1.0088</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0804</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7939</span><span class="p">,</span>  <span class="mf">1.3486</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1543</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4053</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0055</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.3672</span><span class="p">,</span>  <span class="mf">0.3274</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3369</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.4951</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9580</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7847</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.3525</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4780</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1610</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9209</span><span class="p">,</span>  <span class="mf">1.5498</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.4905</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7832</span><span class="p">,</span>  <span class="mf">0.4243</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.9492</span><span class="p">,</span>  <span class="mf">0.3335</span><span class="p">,</span>  <span class="mf">0.9565</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_npu</span><span class="p">,</span> <span class="n">y_gelu_npu</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_geglu</span><span class="p">(</span><span class="n">x_npu</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">approximate</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_npu</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">9.2590e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2054e-01</span><span class="p">,</span>  <span class="mf">1.6980e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.8542e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">2.5254e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.9519e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.2405e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4902e+00</span><span class="p">,</span>  <span class="mf">8.0750e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">3.4570e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.5029e+00</span><span class="p">,</span>  <span class="mf">2.8442e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">9.0271e-02</span><span class="p">,</span>  <span class="mf">4.3335e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7402e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.3574e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">5.5762e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3123e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.0004e-01</span><span class="p">,</span>  <span class="mf">1.5312e+00</span><span class="p">,</span>  <span class="mf">1.4189e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6172e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.6113e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1887e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">5.9845e-02</span><span class="p">,</span>  <span class="mf">2.0911e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.4735e-03</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">5.1422e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.6289e+00</span><span class="p">,</span>  <span class="mf">2.5977e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.3649e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3329e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.9031e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">3.5977e+00</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.2178e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3242e+00</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">3.1816e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6719e+00</span><span class="p">,</span>  <span class="mf">1.4038e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">2.6660e+00</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.7820e-02</span><span class="p">,</span>  <span class="mf">2.3999e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">2.9297e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7754e+00</span><span class="p">,</span>  <span class="mf">2.6703e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3318e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">6.2109e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9072e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.1316e-01</span><span class="p">,</span>  <span class="mf">5.8887e-01</span><span class="p">,</span>  <span class="mf">8.2959e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.1273e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.1481e-01</span><span class="p">,</span>  <span class="mf">4.2419e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">2.6831e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7288e-02</span><span class="p">,</span>  <span class="mf">2.6343e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">9.3750e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">2.2324e+00</span><span class="p">,</span>  <span class="mf">1.2894e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">2.0630e-01</span><span class="p">,</span>  <span class="mf">5.9619e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4210e-03</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2598e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">6.5552e-02</span><span class="p">,</span>  <span class="mf">1.1115e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.6143e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6150e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.9774e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">8.6426e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.1879e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9795e+00</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">4.3152e-02</span><span class="p">,</span>  <span class="mf">1.9250e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.7485e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.8632e-03</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.4551e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1289e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">4.7951e-03</span><span class="p">,</span>  <span class="mf">2.0691e-01</span><span class="p">,</span>  <span class="mf">4.4458e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">4.7485e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">4.8889e-02</span><span class="p">,</span>  <span class="mf">1.5684e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">8.9404e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.0420e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9248e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.6205e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.5449e+00</span><span class="p">,</span>  <span class="mf">8.2397e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.9385e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8838e+00</span><span class="p">,</span>  <span class="mf">6.0010e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.5059e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.1829e-02</span><span class="p">,</span>  <span class="mf">1.0547e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">5.1086e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0760e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.1228e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.2468e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.7900e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5278e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.7078e-01</span><span class="p">,</span>  <span class="mf">1.6846e-01</span><span class="p">,</span>  <span class="mf">2.5528e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.3708e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.4954e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8418e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">6.3574e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0156e+00</span><span class="p">,</span>  <span class="mf">9.3994e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">2.2402e+00</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">6.2218e-03</span><span class="p">,</span>  <span class="mf">8.7402e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.5010e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8518e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0930e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.1511e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">3.8300e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6150e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">2.8442e-01</span><span class="p">,</span>  <span class="mf">4.4373e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0022e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">9.2468e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.2524e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2115e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">3.4760e-02</span><span class="p">,</span>  <span class="mf">1.9812e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.1431e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1650e+00</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.4011e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0919e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.5283e-01</span><span class="p">,</span>  <span class="mf">1.8535e+00</span><span class="p">,</span>  <span class="mf">4.4360e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">6.4844e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">2.8784e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5938e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">9.9915e-02</span><span class="p">,</span>  <span class="mf">4.6436e-01</span><span class="p">,</span>  <span class="mf">6.6528e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2817e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.5686e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.4962e-02</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">2.3279e-01</span><span class="p">,</span>  <span class="mf">4.5630e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.4834e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">5.9013e-03</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">4.7974e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.7617e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.0760e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0371e+00</span><span class="p">,</span>  <span class="mf">3.7915e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">6.4551e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.6953e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0910e-03</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">4.9683e-01</span><span class="p">,</span>  <span class="mf">1.2402e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0429e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">3.4294e-03</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">8.2959e-01</span><span class="p">,</span>  <span class="mf">1.2012e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.6956e-01</span><span class="p">,</span>  <span class="mf">5.3027e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6418e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1094e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">9.8267e-02</span><span class="p">,</span>  <span class="mf">2.3364e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">4.1687e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1365e-01</span><span class="p">,</span>  <span class="mf">1.2598e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.6299e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.5967e+00</span><span class="p">,</span>  <span class="mf">9.3445e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">9.7656e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.5410e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9395e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6565e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">8.2153e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.0068e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">1.6345e-01</span><span class="p">,</span>  <span class="mf">2.5806e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.1951e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.5857e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">6.0303e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9080e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.9666e-01</span><span class="p">,</span>  <span class="mf">1.8262e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1951e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.0138e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">2.0911e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.0638e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">6.9141e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5234e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2734e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.0510e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.6504e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.7070e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">2.5406e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.1342e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.0862e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">9.2041e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.7271e-02</span><span class="p">,</span>  <span class="mf">8.0518e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.5161e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.8848e-02</span><span class="p">,</span>  <span class="mf">7.0801e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">7.0166e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">3.3661e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4319e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">3.0899e-02</span><span class="p">,</span>  <span class="mf">1.4490e-01</span><span class="p">,</span>  <span class="mf">1.9763e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.1116e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.8955e-01</span><span class="p">,</span>  <span class="mf">1.8347e-01</span><span class="p">]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_gelu_npu</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">1.5771e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4331e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0846e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1133e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.3818e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5076e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.8600e-02</span><span class="p">,</span>  <span class="mf">1.6904e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.9336e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">3.6890e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.6768e+00</span><span class="p">,</span>  <span class="mf">2.5146e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">7.5342e-01</span><span class="p">,</span>  <span class="mf">6.0742e-01</span><span class="p">,</span>  <span class="mf">1.0820e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.5063e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.1572e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.4482e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.5796e-01</span><span class="p">,</span>  <span class="mf">8.4082e-01</span><span class="p">,</span>  <span class="mf">9.2627e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6064e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.1096e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6370e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">3.4814e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6418e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.1982e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5186e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.3330e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4111e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">8.4778e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1023e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0669e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.9521e+00</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.5654e-01</span><span class="p">,</span>  <span class="mf">1.5635e+00</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">1.7881e+00</span><span class="p">,</span>  <span class="mf">1.8359e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6663e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.4609e+00</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.6760e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6528e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.9434e+00</span><span class="p">,</span>  <span class="mf">1.7168e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1615e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.8816e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.4043e-01</span><span class="p">,</span>  <span class="mf">1.2344e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.6064e-01</span><span class="p">,</span>  <span class="mf">5.7031e-01</span><span class="p">,</span>  <span class="mf">1.6475e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0809e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.6785e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6345e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.6797e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.6326e-02</span><span class="p">,</span>  <span class="mf">2.6904e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">6.9458e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.3174e+00</span><span class="p">,</span>  <span class="mf">1.3486e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.0645e-01</span><span class="p">,</span>  <span class="mf">3.0249e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.9411e-03</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3928e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.0974e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.1533e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.7012e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0254e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.2825e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.8492e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.1926e-01</span><span class="p">,</span>  <span class="mf">1.7490e+00</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">6.6650e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0370e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3788e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0706e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.6980e-01</span><span class="p">,</span>  <span class="mf">1.4209e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">5.2986e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1133e-01</span><span class="p">,</span>  <span class="mf">2.5439e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.9459e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">6.8909e-02</span><span class="p">,</span>  <span class="mf">1.2119e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">6.1035e-01</span><span class="p">,</span>  <span class="mf">6.8506e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5039e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">5.8136e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.8232e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.7383e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.4434e+00</span><span class="p">,</span>  <span class="mf">1.6787e+00</span><span class="p">,</span>  <span class="mf">1.2422e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">7.5488e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">5.0720e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.8787e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.4600e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2213e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6711e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">3.7280e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.3125e+00</span><span class="p">,</span>  <span class="mf">2.2375e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">3.4985e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2659e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.6722e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4685e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.4856e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6406e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">4.8730e-01</span><span class="p">,</span>  <span class="mf">1.6680e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.7098e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.4189e+00</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.1983e-03</span><span class="p">,</span>  <span class="mf">7.8857e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.1328e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6931e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1163e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6467e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.5309e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5173e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.6858e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.9111e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4709e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.1970e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.4248e-01</span><span class="p">,</span>  <span class="mf">5.0830e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">2.1936e-01</span><span class="p">,</span>  <span class="mf">7.7197e-01</span><span class="p">,</span>  <span class="mf">4.8737e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">8.7842e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.6406e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.1716e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.2720e-01</span><span class="p">,</span>  <span class="mf">1.9404e+00</span><span class="p">,</span>  <span class="mf">1.0391e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">7.3877e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.6199e-01</span><span class="p">,</span>  <span class="mf">1.5781e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.6968e-01</span><span class="p">,</span>  <span class="mf">1.0664e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6431e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.5439e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.5332e-01</span><span class="p">,</span>  <span class="mf">2.1790e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">3.0981e-01</span><span class="p">,</span>  <span class="mf">6.0010e-01</span><span class="p">,</span>  <span class="mf">7.9346e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">4.0169e-03</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.8447e-01</span><span class="p">,</span>  <span class="mf">1.7109e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.6699e-01</span><span class="p">,</span>  <span class="mf">1.7646e+00</span><span class="p">,</span>  <span class="mf">5.9326e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">3.3813e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.5845e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.7699e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">3.7573e-01</span><span class="p">,</span>  <span class="mf">9.4580e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.5276e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">2.4805e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.3350e-01</span><span class="p">,</span>  <span class="mf">1.2573e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.5369e-01</span><span class="p">,</span>  <span class="mf">1.2021e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6626e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1108e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.6084e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4807e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">4.6234e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.4331e-02</span><span class="p">,</span>  <span class="mf">8.9844e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">9.2871e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.9834e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6992e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">6.4941e-02</span><span class="p">,</span>  <span class="mf">1.1465e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5161e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5076e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">8.6487e-02</span><span class="p">,</span>  <span class="mf">1.0137e+00</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">1.1731e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4404e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.9050e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2128e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.0919e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6943e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.5186e-01</span><span class="p">,</span>  <span class="mf">1.1396e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.5735e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.4829e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.6455e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.9355e-02</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">6.4404e-01</span><span class="p">,</span>  <span class="mf">1.5625e+00</span><span class="p">,</span>  <span class="mf">1.7725e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.5176e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.7920e+00</span><span class="p">,</span>  <span class="mf">6.6504e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.9083e-03</span><span class="p">,</span>  <span class="mf">3.8452e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.9011e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5405e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.6003e-01</span><span class="p">,</span>  <span class="mf">1.3975e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.0437e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.6182e-02</span><span class="p">,</span>  <span class="mf">5.5713e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.0645e+00</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1.3818e-01</span><span class="p">,</span>  <span class="mf">5.1562e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.0229e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0529e-01</span><span class="p">,</span>  <span class="mf">2.6562e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.6702e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.0830e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6833e-01</span><span class="p">]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_get_float_status">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_get_float_status</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_get_float_status" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_get_float_status(self) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>计算npu_get_float_status算子函数。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 数据内存地址张量，数据类型为float32。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_get_float_status</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_giou">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_giou</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_giou" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_giou(self, gtboxes, trans=False, is_cross=False, mode=0) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>首先计算两个框的最小封闭面积和IoU，然后计算封闭区域中不属于两个框的封闭面积的比例，最后从IoU中减去这个比例，得到GIoU。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 标注框，shape为(N, 4) 数据类型为float16或float32的2D张量。“N”表示标注框的数量，值“4”表示[x1, y1, x2, y2]或[x, y, w, h]。</p>
<p>gtboxes (Tensor) - 真值框，shape为(M, 4) 数据类型为float16或float32的2D张量。“M”表示真值框的数量，值“4”表示[x1, y1, x2, y2]或[x, y, w, h]。</p>
<p>trans (Bool，默认值为False) - 值为True代表“xywh”，值为False代表“xyxy”。</p>
<p>is_cross (Bool，默认值为False) - 控制输出shape是[M, N]还是[1,N]。如果值为True，则输出shape为[M,N]。如果为False，则输出shape为[1,N]。</p>
<p>mode (Int，默认值为0) - 计算模式，取值为0或1。0表示IoU，1表示IoF。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="mi">4</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="mi">4</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box1</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box2</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_giou</span><span class="p">(</span><span class="n">box1</span><span class="p">,</span> <span class="n">box2</span><span class="p">,</span> <span class="n">trans</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">is_cross</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_grid_assign_positive">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_grid_assign_positive</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_grid_assign_positive" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_grid_assign_positive(self, overlaps, box_responsible_flags, max_overlaps, argmax_overlaps, gt_max_overlaps, gt_argmax_overlaps, num_gts, pos_iou_thr, min_pos_iou, gt_max_assign_all) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>执行position-sensitive的候选区域池化梯度计算。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - float16或float32类型的张量, shape为(n, )。</p>
<p>overlaps (Tensor) - 数据类型与assigned_gt_inds相同，表示gt_bboxes和bboxes之间的IoU，shape为(k,n)。</p>
<p>box_responsible_flags (Tensor) - 支持uint8数据类型。表示框是否responsible的标志。</p>
<p>max_overlaps (Tensor) - 数据类型与assigned_gt_inds. overlaps.max(axis=0)相同。</p>
<p>argmax_overlaps (Tensor) - 支持uint32数据类型，overlaps.argmax(axis=0)。</p>
<p>gt_max_overlaps (Tensor) - 数据类型与assigned_gt_inds. overlaps.max(axis=1)相同。</p>
<p>gt_argmax_overlaps (Tensor) - 支持uint32数据类型， overlaps.argmax(axis=1)。</p>
<p>num_gts (Tensor) - 支持uint32数据类型，real k ，shape为 (1, )。</p>
<p>pos_iou_thr (Float) - 正检测框的IoU阈值。</p>
<p>min_pos_iou (Float) - 检测框被视为正检测框的最小IoU</p>
<p>gt_max_assign_all (Bool) - 是否将与某个gt有相同最高重叠的所有检测框分配给该gt。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">assigned_gt_inds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">overlaps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box_responsible_flags</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">max_overlap</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">argmax_overlap</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gt_max_overlaps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gt_argmax_overlaps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_grid_assign_positive</span><span class="p">(</span><span class="n">assigned_gt_inds</span><span class="p">,</span> <span class="n">overlaps</span><span class="p">,</span> <span class="n">box_responsible_flags</span><span class="p">,</span> <span class="n">max_overlap</span><span class="p">,</span> <span class="n">argmax_overlap</span><span class="p">,</span> <span class="n">gt_max_overlaps</span><span class="p">,</span> <span class="n">gt_argmax_overlaps</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_gru">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_gru</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_gru" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_gru(input, hx, weight_input, weight_hidden, bias_input, bias_hidden, seq_length, has_biases, num_layers, dropout, train, bidirectional, batch_first) -&gt; (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)</p>
<p><strong>功能描述</strong></p>
<p>计算DynamicGRUV2。</p>
<p><strong>参数说明</strong></p>
<p>input (Tensor) - 数据类型：float16；格式：FRACTAL_NZ。</p>
<p>hx (Tensor) - 数据类型：float16, float32；格式：FRACTAL_NZ。</p>
<p>weight_input (Tensor) - 数据类型：float16；格式：FRACTAL_Z。</p>
<p>weight_hidden (Tensor) - 数据类型：float16；格式：FRACTAL_Z。</p>
<p>bias_input (Tensor) - 数据类型：float16, float32；格式：ND。</p>
<p>bias_hidden (Tensor) - 数据类型：float16, float32；格式：ND。</p>
<p>seq_length (Tensor) - 数据类型：int32；格式：ND。</p>
<p>has_biases (Bool，默认值为True)</p>
<p>num_layers (Int)</p>
<p>dropout (Float)</p>
<p>train (Bool，默认值为True) - 标识训练是否在op进行的bool参数。</p>
<p>bidirectional (Bool，默认值为True)</p>
<p>batch_first (Bool，默认值为True)</p>
<p><strong>输出说明</strong></p>
<p>y (Tensor) - 数据类型：float16, float32；格式：FRACTAL_NZ。</p>
<p>output_h (Tensor) - 数据类型：float16, float32；格式：FRACTAL_NZ。</p>
<p>update (Tensor) - 数据类型：float16, float32；格式：FRACTAL_NZ。</p>
<p>reset (Tensor) - 数据类型：float16, float32；格式：FRACTAL_NZ。</p>
<p>new (Tensor) - 数据类型：float16, float32；格式：FRACTAL_NZ。</p>
<p>hidden_new (Tensor) - 数据类型：float16, float32；格式：FRACTAL_NZ。</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_ifmr">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_ifmr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_ifmr" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_ifmr(Tensor data, Tensor data_min, Tensor data_max, Tensor cumsum, float min_percentile, float max_percentile, float search_start, float search_end, float search_step, bool with_offset) -&gt; (Tensor, Tensor)</p>
<p><strong>功能描述</strong></p>
<p>使用“begin,end,strides”数组对ifmr结果进行计数。</p>
<p><strong>参数说明</strong></p>
<p>data (Tensor) - 特征图张量。</p>
<p>data_min (Tensor) - 特征图最小值的张量。</p>
<p>data_max (Tensor) - 特征图最大值的张量。</p>
<p>cumsum (Tensor) - cumsum bin数据张量。</p>
<p>min_percentile (Float) - 最小初始化百分位数。</p>
<p>max_percentile (Float) - 最大初始化百分位数。</p>
<p>search_start (Float) - 搜索起点。</p>
<p>search_end (Float) - 搜索终点。</p>
<p>search_step (Float) - 搜索步长。</p>
<p>with_offset (Bool) - 是否使用offset。</p>
<p><strong>输出说明</strong></p>
<p>scale (Tensor) - 最优尺度。</p>
<p>offset (Tensor) - 最优offset。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[[</span><span class="mf">0.4508</span><span class="p">,</span> <span class="mf">0.6513</span><span class="p">,</span> <span class="mf">0.4734</span><span class="p">,</span> <span class="mf">0.1924</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0402</span><span class="p">,</span> <span class="mf">0.5502</span><span class="p">,</span> <span class="mf">0.0694</span><span class="p">,</span> <span class="mf">0.9032</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.4844</span><span class="p">,</span> <span class="mf">0.5361</span><span class="p">,</span> <span class="mf">0.9369</span><span class="p">,</span> <span class="mf">0.7874</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.5157</span><span class="p">,</span> <span class="mf">0.1863</span><span class="p">,</span> <span class="mf">0.4574</span><span class="p">,</span> <span class="mf">0.8033</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.5986</span><span class="p">,</span> <span class="mf">0.8090</span><span class="p">,</span> <span class="mf">0.7605</span><span class="p">,</span> <span class="mf">0.8252</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.4264</span><span class="p">,</span> <span class="mf">0.8952</span><span class="p">,</span> <span class="mf">0.2279</span><span class="p">,</span> <span class="mf">0.9746</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.0803</span><span class="p">,</span> <span class="mf">0.7114</span><span class="p">,</span> <span class="mf">0.8773</span><span class="p">,</span> <span class="mf">0.2341</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.6497</span><span class="p">,</span> <span class="mf">0.0423</span><span class="p">,</span> <span class="mf">0.8407</span><span class="p">,</span> <span class="mf">0.9515</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.1821</span><span class="p">,</span> <span class="mf">0.5931</span><span class="p">,</span> <span class="mf">0.7160</span><span class="p">,</span> <span class="mf">0.4968</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.7977</span><span class="p">,</span> <span class="mf">0.0899</span><span class="p">,</span> <span class="mf">0.9572</span><span class="p">,</span> <span class="mf">0.0146</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.2804</span><span class="p">,</span> <span class="mf">0.8569</span><span class="p">,</span> <span class="mf">0.2292</span><span class="p">,</span> <span class="mf">0.1118</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.5747</span><span class="p">,</span> <span class="mf">0.4064</span><span class="p">,</span> <span class="mf">0.8370</span><span class="p">,</span> <span class="mf">0.1611</span><span class="p">]]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">min_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">min_value</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0146</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">max_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">max_value</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.9746</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">histc</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bins</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">min</span><span class="o">=</span><span class="n">min_value</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">max</span><span class="o">=</span><span class="n">max_value</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>  <span class="o">&gt;&gt;&gt;</span> <span class="n">cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">hist</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cdf</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">7</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">17</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">25</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">34</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">37</span><span class="p">,</span> <span class="mi">37</span><span class="p">,</span> <span class="mi">37</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">38</span><span class="p">,</span> <span class="mi">39</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">42</span><span class="p">,</span> <span class="mi">42</span><span class="p">,</span> <span class="mi">43</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">46</span><span class="p">,</span> <span class="mi">47</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">47</span><span class="p">,</span> <span class="mi">48</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span><span class="p">,</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_ifmr</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">min_value</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">max_value</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cdf</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">min_percentile</span><span class="o">=</span><span class="mf">0.999999</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">max_percentile</span><span class="o">=</span><span class="mf">0.999999</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">search_start</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">search_end</span><span class="o">=</span><span class="mf">1.3</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">search_step</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">with_offset</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span>  <span class="n">tensor</span><span class="p">(</span><span class="mf">0.0080</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offset</span>  <span class="n">tensor</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_indexing">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_indexing</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_indexing" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_indexing(self, begin, end, strides, begin_mask=0, end_mask=0, ellipsis_mask=0, new_axis_mask=0, shrink_axis_mask=0) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>使用“begin,end,strides”数组对index结果进行计数。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 输入张量。</p>
<p>begin (ListInt) - 待选择的第一个值的index。</p>
<p>end (ListInt) - 待选择的最后一个值的index。</p>
<p>strides (ListInt) - index增量。</p>
<p>begin_mask (Int，默认值为0) - 位掩码(bitmask)，其中位“i”为“1”意味着忽略开始值，尽可能使用最大间隔。</p>
<p>end_mask (Int，默认值为0) - 类似于“begin_mask”。</p>
<p>ellipsis_mask (Int，默认值为0) - 位掩码，其中位“i”为“1”意味着第“i”个位置实际上是省略号。</p>
<p>new_axis_mask (Int，默认值为0) - 位掩码，其中位“i”为“1”意味着在第“i”位创建新的1D shape。</p>
<p>shrink_axis_mask (Int，默认值为0) - 位掩码，其中位“i”意味着第“i”位应缩小维数。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_indexing</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_iou">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_iou</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_iou" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_iou(bboxes, gtboxes, mode=0) -&gt; Tensor</p>
<p>torch_npu.npu_ptiou(bboxes, gtboxes, mode=0) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>根据ground-truth和预测区域计算交并比(IoU)或前景交叉比(IoF)。</p>
<p><strong>参数说明</strong></p>
<p>bboxes (Tensor) - 输入张量。</p>
<p>gtboxes (Tensor) - 输入张量。</p>
<p>mode (Int，默认值为0) - 0为IoU模式，1为IoF模式。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">38</span><span class="p">,</span> <span class="mi">42</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gtboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_iou</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_iou</span><span class="p">(</span><span class="n">bboxes</span><span class="p">,</span> <span class="n">gtboxes</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_iou</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.4985</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.9961</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_layer_norm_eval">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_layer_norm_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_layer_norm_eval" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_layer_norm_eval(input, normalized_shape, weight=None, bias=None, eps=1e-05) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>对层归一化结果进行计数。与torch.nn.functional.layer_norm相同, 优化NPU设备实现。</p>
<p><strong>参数说明</strong></p>
<p>input (Tensor) - 输入张量。</p>
<p>normalized_shape (ListInt) - size为预期输入的输入shape。</p>
<p>weight (Tensor, 可选，默认值为None) - gamma张量。</p>
<p>bias (Tensor, 可选默认值为None) - beta张量。</p>
<p>eps (Float，默认值为1e-5) - 为保证数值稳定性添加到分母中的ε值。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.1863</span><span class="p">,</span> <span class="mf">0.3755</span><span class="p">,</span> <span class="mf">0.1115</span><span class="p">,</span> <span class="mf">0.7308</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.6004</span><span class="p">,</span> <span class="mf">0.6832</span><span class="p">,</span> <span class="mf">0.8951</span><span class="p">,</span> <span class="mf">0.2087</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.8548</span><span class="p">,</span> <span class="mf">0.0176</span><span class="p">,</span> <span class="mf">0.8498</span><span class="p">,</span> <span class="mf">0.3703</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.5609</span><span class="p">,</span> <span class="mf">0.0114</span><span class="p">,</span> <span class="mf">0.5021</span><span class="p">,</span> <span class="mf">0.1242</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.3966</span><span class="p">,</span> <span class="mf">0.3022</span><span class="p">,</span> <span class="mf">0.2323</span><span class="p">,</span> <span class="mf">0.3914</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.1554</span><span class="p">,</span> <span class="mf">0.0149</span><span class="p">,</span> <span class="mf">0.1718</span><span class="p">,</span> <span class="mf">0.4972</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">normalized_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">normalized_shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="o">*</span><span class="n">normalized_shape</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span>        <span class="n">nan</span><span class="p">,</span>  <span class="mf">6.1223e-41</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.3159e-20</span><span class="p">,</span>  <span class="mf">9.1834e-41</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="o">*</span><span class="n">normalized_shape</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mf">5.6033e-39</span><span class="p">,</span> <span class="mf">6.1224e-41</span><span class="p">,</span> <span class="mf">6.1757e-39</span><span class="p">,</span> <span class="mf">6.1224e-41</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_layer_norm_eval</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">normalized_shape</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span>        <span class="n">nan</span><span class="p">,</span>  <span class="mf">6.7474e-41</span><span class="p">,</span>  <span class="mf">8.3182e-20</span><span class="p">,</span>  <span class="mf">2.0687e-40</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span>        <span class="n">nan</span><span class="p">,</span>  <span class="mf">8.2494e-41</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.9784e-20</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.2186e-41</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span>        <span class="n">nan</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6695e-41</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.7173e-20</span><span class="p">,</span>  <span class="mf">2.1353e-41</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span>        <span class="n">nan</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3497e-41</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.1281e-20</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.9827e-42</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span>        <span class="n">nan</span><span class="p">,</span>  <span class="mf">3.5663e-41</span><span class="p">,</span>  <span class="mf">1.2002e-19</span><span class="p">,</span>  <span class="mf">1.4314e-40</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span>        <span class="n">nan</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.2792e-42</span><span class="p">,</span>  <span class="mf">1.7902e-20</span><span class="p">,</span>  <span class="mf">2.1050e-40</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_linear">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_linear</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_linear" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_linear(input, weight, bias=None) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>将矩阵“a”乘以矩阵“b”，生成“a*b”。</p>
<p><strong>参数说明</strong></p>
<p>input (Tensor) - 2D矩阵张量。数据类型：float32、float16、int32、int8。格式：[ND, NHWC, FRACTAL_NZ]。</p>
<p>weight (Tensor) - 2D矩阵张量。数据类型：float32、float16、int32、int8。格式：[ND, NHWC, FRACTAL_NZ]。</p>
<p>bias (Tensor，可选，默认值为None) - 1D张量。数据类型：float32、float16、int32。格式：[ND, NHWC]。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">3.6335</span><span class="p">,</span> <span class="mf">4.3713</span><span class="p">,</span> <span class="mf">2.4440</span><span class="p">,</span> <span class="mf">2.0081</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">5.3273</span><span class="p">,</span> <span class="mf">6.3089</span><span class="p">,</span> <span class="mf">3.9601</span><span class="p">,</span> <span class="mf">3.2410</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_lstm">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_lstm</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_lstm" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_lstm(x, weight, bias, seqMask, h, c, has_biases, num_layers, dropout, train, bidirectional, batch_first, flag_seq, direction)</p>
<p><strong>功能描述</strong></p>
<p>计算DynamicRNN。</p>
<p><strong>参数说明</strong></p>
<p>x (Tensor) - 4D张量。数据类型：float16, float32；格式：FRACTAL_NZ。</p>
<p>weight (Tensor) - 4D张量。数据类型：float16, float32；格式：FRACTAL_ZN_LSTM。</p>
<p>bias (Tensor) - 1D张量。数据类型：float16, float32；格式：ND。</p>
<p>seqMask (Tensor) - 张量。仅支持为FRACTAL_NZ格式的float16和ND格式的int32类型。</p>
<p>h (Tensor) - 4D张量。数据类型：float16, float32；格式：FRACTAL_NZ。</p>
<p>c (Tensor) - 4D张量。数据类型：float16, float32；格式：FRACTAL_NZ。</p>
<p>has_biases (Bool) - 如果值为True,则存在偏差。</p>
<p>num_layers (Int) - 循环层数，目前只支持单层。</p>
<p>dropout (Float) - 如果值为非零，则在除最后一层外的每个LSTM层的输出上引入一个dropout层，丢弃概率等于dropout参数值。目前不支持。</p>
<p>train (Bool，默认值为True) - 标识训练是否在op进行的bool参数。</p>
<p>bidirectional (Bool) - 如果值为True，LSTM为双向。当前不支持。</p>
<p>batch_first (Bool) - 如果值为True，则输入和输出张量将表示为(batch, seq, feature)。当前不支持。</p>
<p>flag_seq (Bool) - 如果值为True，输入为PackSequnce。当前不支持。</p>
<p>direction (Bool) - 如果值为True，则方向为“REDIRECTIONAL”，否则为“UNIDIRECTIONAL”。</p>
<p><strong>输出说明</strong></p>
<p>y (Tensor) - 4D张量。数据类型：float16, float32；格式：FRACTAL_NZ。</p>
<p>output_h (Tensor) - 4D张量。数据类型：float16, float32；格式：FRACTAL_NZ。</p>
<p>output_c (Tensor) - 4D张量。数据类型：float16, float32；格式：FRACTAL_NZ。</p>
<p>i (Tensor) - 4D张量。数据类型：float16, float32；格式：FRACTAL_NZ。</p>
<p>j (Tensor) - 4D张量。数据类型：float16, float32；格式：FRACTAL_NZ。</p>
<p>f (Tensor) - 4D张量。数据类型：float16, float32；格式：FRACTAL_NZ。</p>
<p>o (Tensor) - 4D张量。数据类型：float16, float32；格式：FRACTAL_NZ。</p>
<p>tanhct (Tensor) - 4D张量。数据类型：float16, float32；格式：FRACTAL_NZ。</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_masked_fill_range">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_masked_fill_range</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_masked_fill_range" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_masked_fill_range(self, start, end, value, axis=-1) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>同轴上被range.boxes屏蔽(masked)的填充张量。自定义屏蔽填充范围算子。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - shape为1D (D,)、2D (N,D)或3D (N,D)的float32/float16/int32/int8 ND张量。</p>
<p>start (Tensor) - 屏蔽填充开始位置。shape为(num,N)的int32 3D张量。</p>
<p>end (Tensor) - 屏蔽填充结束位置。shape为(num,N)的int32 3D张量。</p>
<p>value (Tensor) - 屏蔽填充值。shape为(num,)的float32/float16/int32/int8 2D张量。</p>
<p>axis (Int，默认值为-1) - 带有int32屏蔽填充的轴。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.9419</span><span class="p">,</span> <span class="mf">0.4919</span><span class="p">,</span> <span class="mf">0.2874</span><span class="p">,</span> <span class="mf">0.6560</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.6691</span><span class="p">,</span> <span class="mf">0.6668</span><span class="p">,</span> <span class="mf">0.0330</span><span class="p">,</span> <span class="mf">0.1006</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.3888</span><span class="p">,</span> <span class="mf">0.7011</span><span class="p">,</span> <span class="mf">0.7141</span><span class="p">,</span> <span class="mf">0.7878</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0366</span><span class="p">,</span> <span class="mf">0.9738</span><span class="p">,</span> <span class="mf">0.4689</span><span class="p">,</span> <span class="mf">0.0979</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">start</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">end</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_masked_fill_range</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.4919</span><span class="p">,</span> <span class="mf">0.2874</span><span class="p">,</span> <span class="mf">0.6560</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.6691</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.0330</span><span class="p">,</span> <span class="mf">0.1006</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.3888</span><span class="p">,</span> <span class="mf">0.7011</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.7878</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0366</span><span class="p">,</span> <span class="mf">0.9738</span><span class="p">,</span> <span class="mf">0.4689</span><span class="p">,</span> <span class="mf">0.0979</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_max">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_max</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_max" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_max(self, dim, keepdim=False) -&gt; (Tensor, Tensor)</p>
<p><strong>功能描述</strong></p>
<p>使用dim对最大结果进行计数。类似于torch.max, 优化NPU设备实现。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 输入张量。</p>
<p>dim (Int) - 待降低维度。</p>
<p>keepdim (Bool，默认值为False) - 输出张量是否保留dim。</p>
<p><strong>输出说明</strong></p>
<p>values (Tensor) - 输入张量中的最大值。</p>
<p>indices (Tensor) - 输入张量中最大值的index。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[[</span><span class="o">-</span><span class="mf">1.8135</span><span class="p">,</span>  <span class="mf">0.2078</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.6678</span><span class="p">,</span>  <span class="mf">0.7846</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">0.6458</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0923</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.2124</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9112</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="o">-</span><span class="mf">0.5800</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4979</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.2580</span><span class="p">,</span>  <span class="mf">1.1335</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">0.6669</span><span class="p">,</span>  <span class="mf">0.1876</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.1160</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1061</span><span class="p">]]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_max</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.6678</span><span class="p">,</span>  <span class="mf">0.7846</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.6458</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0923</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">0.2580</span><span class="p">,</span>  <span class="mf">1.1335</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.6669</span><span class="p">,</span>  <span class="mf">0.1876</span><span class="p">]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_min">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_min</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_min" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_min(self, dim, keepdim=False) -&gt; (Tensor, Tensor)</p>
<p><strong>功能描述</strong></p>
<p>使用dim对最小结果进行计数。类似于torch.min, 优化NPU设备实现。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 输入张量。</p>
<p>dim (Int) - 待降低维度。</p>
<p>keepdim (Bool) - 输出张量是否保留dim。</p>
<p><strong>输出说明</strong></p>
<p>values (Tensor) - 输入张量中的最小值。</p>
<p>indices (Tensor) - 输入张量中最小值的index。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[[</span><span class="o">-</span><span class="mf">0.9909</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2369</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.9569</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6223</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">0.1157</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3147</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.7761</span><span class="p">,</span>  <span class="mf">0.1344</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span> <span class="mf">1.6292</span><span class="p">,</span>  <span class="mf">0.5953</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.6940</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6367</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">1.2335</span><span class="p">,</span>  <span class="mf">0.2131</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.0748</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7046</span><span class="p">]]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_min</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.9909</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6223</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.7761</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3147</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">0.6940</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6367</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.2335</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7046</span><span class="p">]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_mish">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_mish</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_mish" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>按元素计算self的双曲正切。</p>
<p><strong>参数解释</strong>：</p>
<p>self (Tensor) - 数据类型：float16、float32。</p>
<p>约束条件：</p>
<p>无</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_mish</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_multi_head_attention">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_multi_head_attention</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_multi_head_attention" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_multi_head_attention(Tensor query, Tensor key, Tensor value, Tensor query_weight, Tensor key_weight, Tensor value_weight, Tensor attn_mask, Tensor out_proj_weight, Tensor query_bias, Tensor key_bia, Tensor value_bias, Tensor out_proj_bias, Tensor dropout_mask, int attn_head_num, int attn_dim_per_head, int src_len, int tgt_len, float dropout_prob, bool softmax_use_float) -&gt; (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)</p>
<p><strong>功能描述</strong></p>
<p>实现Transformer模块中的MultiHeadAttention计算逻辑。</p>
<p><strong>参数说明</strong></p>
<p>query: Tensor类型，仅支持float16</p>
<p>key: Tensor类型，仅支持float16</p>
<p>value: Tensor类型，仅支持float16</p>
<p>query_weight: Tensor类型，仅支持float16</p>
<p>key_weight: Tensor类型，仅支持float16</p>
<p>value_weight: Tensor类型，仅支持float16</p>
<p>attn_mask: Tensor类型，仅支持float16</p>
<p>out_proj_weight: Tensor类型，仅支持float16</p>
<p>query_bias: Tensor类型，仅支持float16</p>
<p>key_bias: Tensor类型，仅支持float16</p>
<p>value_bias: Tensor类型，仅支持float16</p>
<p>out_proj _bias: Tensor类型，仅支持float16</p>
<p>dropout_mask_input: Tensor类型，仅支持float16</p>
<p>attn_head_num： Attention Head numbers, Int型</p>
<p>attn_dim_per_head：Attention dim of a Head , Int型</p>
<p>src_len：source length, Int型</p>
<p>tgt_len：target length, Int型</p>
<p>keep_prob：dropout keep probability, Float型</p>
<p>softmax_use_float：SoftMax Use Float32 to keep precision, Bool型</p>
<p><strong>输出说明</strong></p>
<p>y: Tensor类型，仅支持float16</p>
<p>dropout_mask: Tensor类型，仅支持float16</p>
<p>query_res: Tensor类型，仅支持float16</p>
<p>key_res: Tensor类型，仅支持float16</p>
<p>value_res: Tensor类型，仅支持float16</p>
<p>attn_scores: Tensor类型，仅支持float16</p>
<p>attn_res: Tensor类型，仅支持float16</p>
<p>context: Tensor类型，仅支持float16</p>
<p><strong>约束说明</strong></p>
<p>Attr attn_head_num：需16对齐</p>
<p>Attr attn_dim_per_head：需16对齐</p>
<p>Attr src_len：需16对齐</p>
<p>tgt_len：需16对齐</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attn_head_num</span> <span class="o">=</span> <span class="mi">16</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attn_dim_per_head</span> <span class="o">=</span> <span class="mi">64</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src_len</span> <span class="o">=</span> <span class="mi">64</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tgt_len</span> <span class="o">=</span> <span class="mi">64</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dropout_prob</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softmax_use_float</span> <span class="o">=</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight_col</span> <span class="o">=</span> <span class="n">attn_head_num</span> <span class="o">*</span> <span class="n">attn_dim_per_head</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">query</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">batch</span> <span class="o">*</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">weight_col</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">key</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">batch</span> <span class="o">*</span> <span class="n">src_len</span><span class="p">,</span> <span class="n">weight_col</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">batch</span> <span class="o">*</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">weight_col</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">query_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_col</span><span class="p">,</span> <span class="n">weight_col</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">key_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_col</span><span class="p">,</span> <span class="n">weight_col</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_col</span><span class="p">,</span> <span class="n">weight_col</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_proj_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_col</span><span class="p">,</span> <span class="n">weight_col</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attn_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">attn_head_num</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">query_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_col</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">key_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_col</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_col</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_proj_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_col</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dropout_mask_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">weight_col</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npu_result</span><span class="p">,</span> <span class="n">npu_dropout_mask</span><span class="p">,</span> <span class="n">npu_query_res</span><span class="p">,</span> <span class="n">npu_key_res</span><span class="p">,</span> <span class="n">npu_value_res</span><span class="p">,</span> <span class="n">npu_attn_scores</span><span class="p">,</span> <span class="n">npu_attn_res</span><span class="p">,</span> <span class="n">npu_context</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_multi_head_attention</span> <span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">query_weight</span><span class="p">,</span> <span class="n">key_weight</span><span class="p">,</span> <span class="n">value_weight</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">,</span> <span class="n">out_proj_weight</span><span class="p">,</span> <span class="n">query_bias</span><span class="p">,</span> <span class="n">key_bias</span><span class="p">,</span> <span class="n">value_bias</span><span class="p">,</span> <span class="n">out_proj_bias</span><span class="p">,</span>  <span class="n">dropout_mask_input</span><span class="p">,</span> <span class="n">attn_head_num</span><span class="p">,</span> <span class="n">attn_dim_per_head</span><span class="p">,</span> <span class="n">src_len</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">dropout_prob</span><span class="p">,</span> <span class="n">softmax_use_float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">npu_result</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span> <span class="mf">623.5000</span><span class="p">,</span>   <span class="mf">75.5000</span><span class="p">,</span>  <span class="mf">307.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>   <span class="mf">25.3125</span><span class="p">,</span> <span class="o">-</span><span class="mf">418.7500</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">35.9688</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">254.2500</span><span class="p">,</span> <span class="o">-</span><span class="mf">165.6250</span><span class="p">,</span>  <span class="mf">176.2500</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>   <span class="mf">87.3750</span><span class="p">,</span>   <span class="mf">78.0000</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">65.2500</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">233.2500</span><span class="p">,</span>  <span class="mf">207.3750</span><span class="p">,</span>  <span class="mf">324.7500</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>   <span class="mf">38.6250</span><span class="p">,</span> <span class="o">-</span><span class="mf">264.2500</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">153.7500</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">110.2500</span><span class="p">,</span>  <span class="o">-</span><span class="mf">92.5000</span><span class="p">,</span>  <span class="o">-</span><span class="mf">74.0625</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="o">-</span><span class="mf">68.0625</span><span class="p">,</span>  <span class="mf">195.6250</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">157.6250</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">300.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">184.6250</span><span class="p">,</span>   <span class="o">-</span><span class="mf">6.0039</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="o">-</span><span class="mf">15.7969</span><span class="p">,</span> <span class="o">-</span><span class="mf">299.0000</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">93.1875</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span>  <span class="o">-</span><span class="mf">2.5996</span><span class="p">,</span>   <span class="mf">36.8750</span><span class="p">,</span>  <span class="mf">100.0625</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">112.7500</span><span class="p">,</span>  <span class="mf">202.0000</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">166.3750</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_nms_rotated">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_nms_rotated</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_nms_rotated" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_nms_rotated(dets, scores, iou_threshold, scores_threshold=0, max_output_size=-1, mode=0) -&gt; (Tensor, Tensor)</p>
<p><strong>功能描述</strong></p>
<p>按分数降序选择旋转标注框的子集。</p>
<p><strong>参数说明</strong></p>
<p>dets (Tensor) - shape为[num_boxes, 5]的2D浮点张量</p>
<p>scores (Tensor) - shape为[num_boxes]的1D浮点张量，表示每个框(每行框)对应的一个分数。</p>
<p>iou_threshold (Float) - 表示框与IoU重叠上限阈值的标量。</p>
<p>scores_threshold (Float，默认值为0) - 表示决定何时删除框的分数阈值的标量。</p>
<p>max_output_size (Int，默认值为-1) - 标量整数张量，表示非最大抑制下要选择的最大框数。为-1时即不施加任何约束。</p>
<p>mode (Int，默认值为0) - 指定dets布局类型。如果mode设置为0，则dets的输入值为x、y、w、h和角度。如果mode设置为1，则dets的输入值为x1、y1、x2、y2和角度。</p>
<p><strong>输出说明</strong></p>
<p>selected_index (Tensor) - shape为[M]的1D整数张量，表示从dets张量中选定的index，其中M &lt;= max_output_size。</p>
<p>selected_num (Tensor) - 0D整数张量，表示selected_indices中有效元素的数量。</p>
<p><strong>约束说明</strong></p>
<p>目前不支持mode=1的场景。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dets</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dets</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output1</span><span class="p">,</span> <span class="n">output2</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_nms_rotated</span><span class="p">(</span><span class="n">dets</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mi">76</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">65</span><span class="p">,</span> <span class="mi">91</span><span class="p">,</span> <span class="mi">82</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">62</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">59</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">47</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">55</span><span class="p">,</span> <span class="mi">63</span><span class="p">,</span> <span class="mi">72</span><span class="p">,</span> <span class="mi">39</span><span class="p">,</span> <span class="mi">97</span><span class="p">,</span> <span class="mi">81</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">38</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">74</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">79</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">88</span><span class="p">,</span> <span class="mi">83</span><span class="p">,</span> <span class="mi">37</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">64</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">54</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">98</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span> <span class="mi">86</span><span class="p">,</span> <span class="mi">69</span><span class="p">,</span> <span class="mi">57</span><span class="p">,</span> <span class="mi">43</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span> <span class="mi">42</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">71</span><span class="p">,</span> <span class="mi">46</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">78</span><span class="p">,</span> <span class="mi">66</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span> <span class="mi">52</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output2tensor</span><span class="p">([</span><span class="mi">62</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_nms_v4">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_nms_v4</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_nms_v4" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_nms_v4(boxes, scores, max_output_size, iou_threshold, scores_threshold, pad_to_max_output_size=False) -&gt; (Tensor, Tensor)</p>
<p><strong>功能描述</strong></p>
<p>按分数降序选择标注框的子集。</p>
<p><strong>参数说明</strong></p>
<p>boxes (Tensor) - shape为[num_boxes, 4]的2D浮点张量。</p>
<p>scores (Tensor) - shape为[num_boxes]的1D浮点张量，表示每个框(每行框)对应的一个分数。</p>
<p>max_output_size (Scalar) - 表示non-max suppression下要选择的最大框数的标量。</p>
<p>iou_threshold (Tensor) - 0D浮点张量，表示框与IoU重叠上限的阈值。</p>
<p>scores_threshold (Tensor) - 0D浮点张量，表示决定何时删除框的分数阈值。</p>
<p>pad_to_max_output_size (Bool，默认值为False) - 如果为True，则输出的selected_indices将填充为max_output_size长度。</p>
<p><strong>输出说明</strong></p>
<p>selected_indices (Tensor) - shape为[M]的1D整数张量，表示从boxes张量中选定的index，其中M &lt;= max_output_size。</p>
<p>valid_outputs (Tensor) - 0D整数张量，表示selected_indices中有效元素的数量，有效元素首先呈现。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">boxes</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boxes</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">max_output_size</span> <span class="o">=</span> <span class="mi">20</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iou_threshold</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores_threshold</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npu_output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_nms_v4</span><span class="p">(</span><span class="n">boxes</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">max_output_size</span><span class="p">,</span> <span class="n">iou_threshold</span><span class="p">,</span> <span class="n">scores_threshold</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npu_output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">tensor</span><span class="p">([</span><span class="mi">57</span><span class="p">,</span> <span class="mi">65</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">43</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">52</span><span class="p">,</span> <span class="mi">91</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">78</span><span class="p">,</span> <span class="mi">53</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">62</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">67</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span> <span class="mi">94</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mi">54</span><span class="p">,</span> <span class="mi">92</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">tensor</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_nms_with_mask">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_nms_with_mask</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_nms_with_mask" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_nms_with_mask(input, iou_threshold) -&gt; (Tensor, Tensor, Tensor)</p>
<p><strong>功能描述</strong></p>
<p>生成值0或1，用于nms算子确定有效位。</p>
<p><strong>参数说明</strong></p>
<p>input (Tensor) - 输入张量</p>
<p>iou_threshold (Scalar) - 阈值。如果超过此阈值，则值为1，否则值为0。</p>
<p><strong>输出说明</strong></p>
<p>selected_boxes (Tensor) - shape为[N,5]的2D张量，表示filtered box，包括proposal box和相应的置信度分数。</p>
<p>selected_idx (Tensor) - shape为[N]的1D张量，表示输入建议框的index。</p>
<p>selected_mask (Tensor) - shape为[N]的1D张量，判断输出建议框是否有效。</p>
<p><strong>约束说明</strong></p>
<p>输入box_scores的2nd-dim必须等于8。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iou_threshold</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output1</span><span class="p">,</span> <span class="n">output2</span><span class="p">,</span> <span class="n">output3</span><span class="p">,</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_nms_with_mask</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">iou_threshold</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">2.0000</span><span class="p">,</span> <span class="mf">3.0000</span><span class="p">,</span> <span class="mf">0.6001</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">6.0000</span><span class="p">,</span> <span class="mf">7.0000</span><span class="p">,</span> <span class="mf">8.0000</span><span class="p">,</span> <span class="mf">9.0000</span><span class="p">,</span> <span class="mf">0.3999</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span>      <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_normalize_batch">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_normalize_batch</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_normalize_batch" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_normalize_batch(self, seq_len, normalize_type=0) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>执行批量归一化。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 支持float32数据类型，shape为(n, c, d)。</p>
<p>seq_len (Tensor) - 支持Int32数据类型，shape为(n, )， 表示每批次标准化数据量 。</p>
<p>normalize_type (Int，默认值为0) - 支持 &quot;per_feature&quot;或&quot;all_features&quot;。值为0表示 &quot;per_feature&quot;，值为1表示&quot;all_features&quot;。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="p">,(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">seqlen</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_normalize_batch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span> <span class="mf">1.1496</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6685</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4812</span><span class="p">,</span>  <span class="mf">1.7611</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5187</span><span class="p">,</span>  <span class="mf">0.7571</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.1445</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4393</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7051</span><span class="p">,</span>  <span class="mf">1.0474</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2646</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1582</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.1477</span><span class="p">,</span>  <span class="mf">0.9179</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0656</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.8692</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.7437</span><span class="p">,</span>  <span class="mf">2.8621</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">0.6880</span><span class="p">,</span>  <span class="mf">0.1337</span><span class="p">,</span>  <span class="mf">1.3623</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8081</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2291</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9410</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.3070</span><span class="p">,</span>  <span class="mf">0.5489</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4858</span><span class="p">,</span>  <span class="mf">0.6300</span><span class="p">,</span>  <span class="mf">0.6428</span><span class="p">,</span>  <span class="mf">0.0433</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.5387</span><span class="p">,</span>  <span class="mf">0.8204</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1401</span><span class="p">,</span>  <span class="mf">0.8584</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3686</span><span class="p">,</span>  <span class="mf">0.8444</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_one_hot">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_one_hot</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_one_hot" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_one_hot(input, num_classes=-1, depth=1, on_value=1, off_value=0) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>返回一个one-hot张量。input中index表示的位置采用on_value值，而其他所有位置采用off_value的值。</p>
<p><strong>参数说明</strong></p>
<p>input (Tensor) - 任何shape的class值。</p>
<p>num_classes (Int，默认值为-1) - 待填充的轴。</p>
<p>depth (Int，默认值为1) - one_hot维度的深度。</p>
<p>on_value (Scalar，默认值为1) - 当indices[j] == i时输出中的填充值。</p>
<p>off_value (Scalar，默认值为0) - 当indices[j] != i时输出中的填充值。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">IntTensor</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">=</span><span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_one_hot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">btensor</span><span class="p">([[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_pad">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_pad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_pad" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_pad(input, paddings) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>填充张量。</p>
<p><strong>参数说明</strong></p>
<p>input (Tensor) - 输入张量。</p>
<p>paddings (ListInt) - 数据类型：int32、int64。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">paddings</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">20.</span><span class="p">,</span> <span class="mf">20.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_ps_roi_pooling">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_ps_roi_pooling</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_ps_roi_pooling" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_ps_roi_pooling(x, rois, spatial_scale, group_size, output_dim) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>执行Position Sensitive ROI Pooling。</p>
<p><strong>参数说明</strong></p>
<p>x (Tensor) - 描述特征图的NC1HWC0张量。维度C1必须等于(int(output_dim+15)/C0)) group_size。</p>
<p>rois (Tensor) - shape为[batch, 5, rois_num]的张量，用于描述ROI。每个ROI由五个元素组成：“batch_id”、“x1”、“y1”、“x2”和“y2”，其中“batch_id”表示输入特征图的index，“x1”、“y1”、“x2”，和“y2”必须大于或等于“0.0”。</p>
<p>spatial_scale (Float32) - 将输入坐标映射到ROI坐标的缩放系数。</p>
<p>group_size (Int32) - 指定用于编码position-sensitive评分图的组数。该值必须在(0,128)范围内。</p>
<p>output_dim (Int32) - 指定输出通道数。必须大于0。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">roi</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">],</span> <span class="p">[</span><span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">]]],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[[</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span> <span class="mi">3</span><span class="p">]],</span> <span class="p">[[</span> <span class="mi">4</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mi">5</span><span class="p">]],</span> <span class="p">[[</span> <span class="mi">6</span><span class="p">]],</span> <span class="p">[[</span> <span class="mi">7</span><span class="p">]],</span> <span class="p">[[</span> <span class="mi">8</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span> <span class="mi">9</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">10</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">11</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">12</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mi">13</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">14</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">15</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">16</span><span class="p">]]]],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_ps_roi_pooling</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">roi</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outtensor</span><span class="p">([[[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_ptiou">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_ptiou</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_ptiou" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_ptiou(bboxes, gtboxes, mode=0) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>根据ground-truth和预测区域计算交并比(IoU)或前景交叉比(IoF)。</p>
<p><strong>参数说明</strong></p>
<p>bboxes (Tensor) - 输入张量。</p>
<p>gtboxes (Tensor) - 输入张量。</p>
<p>mode (Int，默认值为0) - 0为IoU模式，1为IoF模式。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">38</span><span class="p">,</span> <span class="mi">42</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gtboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_iou</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_iou</span><span class="p">(</span><span class="n">bboxes</span><span class="p">,</span> <span class="n">gtboxes</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_iou</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.4985</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.9961</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_random_choice_with_mask">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_random_choice_with_mask</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_random_choice_with_mask" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_random_choice_with_mask(x, count=256, seed=0, seed2=0) -&gt; (Tensor, Tensor)</p>
<p><strong>功能描述</strong></p>
<p>混洗非零元素的index。</p>
<p><strong>参数说明</strong></p>
<p>x (Tensor) - 输入张量。</p>
<p>count (Int，默认值为256) - 输出计数。如果值为0，则输出所有非零元素。</p>
<p>seed (Int，默认值为0) - 数据类型：int32，int64。</p>
<p>seed2 (Int，默认值为2) - 数据类型：int32，int64。</p>
<p><strong>输出说明</strong></p>
<p>y (Tensor) - 2D张量, 非零元素的index。</p>
<p>mask (Tensor) - 1D张量, 确定对应index是否有效。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_random_choice_with_mask</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">resulttensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">2</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_reshape">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_reshape</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_reshape" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_reshape(self, shape, bool can_refresh=False) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>reshape张量。仅更改张量shape，其数据不变。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 输入张量。</p>
<p>shape (ListInt) - 定义输出张量的shape。</p>
<p>can_refresh (Bool，默认值为False) - 是否就地刷新reshape。</p>
<p><strong>约束说明</strong></p>
<p>该运算符不能被aclopExecute API直接调用。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">=</span><span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_reshape</span><span class="p">(</span><span class="n">a</span><span class="p">,(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.6657</span><span class="p">,</span> <span class="mf">0.9857</span><span class="p">,</span> <span class="mf">0.7614</span><span class="p">,</span> <span class="mf">0.4368</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.3761</span><span class="p">,</span> <span class="mf">0.4397</span><span class="p">,</span> <span class="mf">0.8609</span><span class="p">,</span> <span class="mf">0.5544</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.7002</span><span class="p">,</span> <span class="mf">0.3063</span><span class="p">,</span> <span class="mf">0.9279</span><span class="p">,</span> <span class="mf">0.5085</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.1009</span><span class="p">,</span> <span class="mf">0.7133</span><span class="p">,</span> <span class="mf">0.8118</span><span class="p">,</span> <span class="mf">0.6193</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_rms_norm">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_rms_norm</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_rms_norm" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_rms_norm(Tensor self, Tensor gamma, float epsilon=1e-06) -&gt; (Tensor, Tensor)</p>
<p><strong>功能描述</strong></p>
<p>RmsNorm算子是大模型常用的归一化操作，相比LayerNorm算子，其去掉了减去均值的部分。</p>
<p><strong>参数说明</strong></p>
<p>self：Tensor类型，支持float16、bfloat16、float32，输入shape支持2-8维。</p>
<p>gamma：Tensor类型，数据类型需要和self保持一致，输入shape支持2-8维，通常为self的最后一维。</p>
<p>epsilon：float数据类型，用于防止除0错误。</p>
<p><strong>输出说明</strong></p>
<p>共两个输出，格式为： (Tensor, Tensor)</p>
<p>第1个输出为Tensor，计算公式的最终输出y；</p>
<p>第2个输出为Tensor，rms_norm的reverse rms中间结果，用于反向计算。</p>
<p><strong>约束说明</strong></p>
<p>输入数据类型仅支持float16、bfloat16和float32。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_npu
&gt;&gt;&gt; x = torch.randn(24, 1, 128).bfloat16().npu()
&gt;&gt;&gt; w = torch.randn(128).bfloat16().npu()
&gt;&gt;&gt; ​
&gt;&gt;&gt; out1 = torch.npu_rms_norm(x, w, epsilon=1e-5)[0]
&gt;&gt;&gt; print(out1)
&gt;&gt;&gt; tensor([[[-0.1123,  0.3398,  0.0986,  ..., -2.1250, -0.8477, -0.3418]],
&gt;&gt;&gt; ​
&gt;&gt;&gt; [[-0.0591,  0.3184, -0.5000,  ...,  1.0312, -1.1719, -0.1621]],
&gt;&gt;&gt; ​
&gt;&gt;&gt; [[-0.1445,  0.3828, -0.3438,  ..., -0.9102, -0.5703,  0.0073]],
&gt;&gt;&gt; ​
&gt;&gt;&gt; ...,
&gt;&gt;&gt; ​
&gt;&gt;&gt; [[-0.1631, -0.3477,  0.4297,  ...,  0.9219,  0.1621,  0.3125]],
&gt;&gt;&gt; ​
&gt;&gt;&gt; [[-0.1387,  0.0815,  0.0967,  ...,  1.7109,  0.1455, -0.1406]],
&gt;&gt;&gt; ​
&gt;&gt;&gt; [[ 0.0698,  1.3438, -0.0127,  ..., -2.2656, -0.4473,  0.3281]]],
&gt;&gt;&gt; device=&#39;npu:0&#39;, dtype=torch.bfloat16)
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_roi_align">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_roi_align</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_roi_align" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_roi_align(features, rois, spatial_scale, pooled_height, pooled_width, sample_num, roi_end_mode) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>从特征图中获取ROI特征矩阵。自定义FasterRcnn算子。</p>
<p><strong>参数说明</strong></p>
<p>features (Tensor) - 5HD张量</p>
<p>rois (Tensor) - ROI位置，shape为(N, 5)的2D张量。“N”表示ROI的数量，“5”表示ROI所在图像的index，分别为“x0”、“y0”、“x1”和“y1”。</p>
<p>spatial_scale (Float32) - 指定“features”与原始图像的缩放比率。</p>
<p>pooled_height (Int32) - 指定H维度。</p>
<p>pooled_width (Int32) - 指定W维度。</p>
<p>sample_num (Int32，默认值为2) - 指定每次输出的水平和垂直采样频率。若此属性设置为0，则采样频率等于“rois”的向上取整值(一个浮点数)。</p>
<p>roi_end_mode (Int32，默认值为1)</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([[[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">18</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">19</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">24</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">29</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mi">31</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">34</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">36</span><span class="p">]]]])</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rois</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">]])</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_roi_align</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">rois</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[[</span> <span class="mf">4.5000</span><span class="p">,</span>  <span class="mf">6.5000</span><span class="p">,</span>  <span class="mf">8.5000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">16.5000</span><span class="p">,</span> <span class="mf">18.5000</span><span class="p">,</span> <span class="mf">20.5000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">28.5000</span><span class="p">,</span> <span class="mf">30.5000</span><span class="p">,</span> <span class="mf">32.5000</span><span class="p">]]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_rotary_mul">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_rotary_mul</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_rotary_mul" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_rotary_mul(Tensor x, Tensor r1, Tensor r2): -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>实现RotaryEmbedding旋转位置编码。支持FakeTensor模式。</p>
<p>x1, x2 = torch.chunk(x, 2, -1)</p>
<p>x_new = torch.cat((-x2, x1), dim=-1)</p>
<p>output = r1 * x + r2 * x_new</p>
<p><strong>参数说明</strong></p>
<p>Tensor x：4维张量，shape为(B, N, S, D)。</p>
<p>Tensor r1：4维张量cos角度，shape为(X, X, X, D)，支持除最后一轴外任意轴广播。</p>
<p>Tensor r2：4维张量sin角度，shape为(X, X, X, D)， 支持除最后一轴外任意轴广播。</p>
<p><strong>约束说明</strong></p>
<p>x，r1，r2的最后一维必须是64的倍数。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="n">tensor</span><span class="p">([[[[</span><span class="mf">0.8594</span><span class="p">,</span> <span class="mf">0.4914</span><span class="p">,</span> <span class="mf">0.9075</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.2126</span><span class="p">,</span> <span class="mf">0.6520</span><span class="p">,</span> <span class="mf">0.2206</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.5515</span><span class="p">,</span> <span class="mf">0.3353</span><span class="p">,</span> <span class="mf">0.6568</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.3686</span><span class="p">,</span> <span class="mf">0.1457</span><span class="p">,</span> <span class="mf">0.8528</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0504</span><span class="p">,</span> <span class="mf">0.2687</span><span class="p">,</span> <span class="mf">0.4036</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.3032</span><span class="p">,</span> <span class="mf">0.8262</span><span class="p">,</span> <span class="mf">0.6302</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0537</span><span class="p">,</span> <span class="mf">0.5141</span><span class="p">,</span> <span class="mf">0.7016</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.4948</span><span class="p">,</span> <span class="mf">0.9778</span><span class="p">,</span> <span class="mf">0.8535</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.3602</span><span class="p">,</span> <span class="mf">0.7874</span><span class="p">,</span> <span class="mf">0.9913</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.1474</span><span class="p">,</span> <span class="mf">0.3422</span><span class="p">,</span> <span class="mf">0.6830</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.4641</span><span class="p">,</span> <span class="mf">0.6254</span><span class="p">,</span> <span class="mf">0.7415</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.1834</span><span class="p">,</span> <span class="mf">0.1067</span><span class="p">,</span> <span class="mf">0.7171</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.8084</span><span class="p">,</span> <span class="mf">0.7570</span><span class="p">,</span> <span class="mf">0.4728</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.4603</span><span class="p">,</span> <span class="mf">0.4991</span><span class="p">,</span> <span class="mf">0.1723</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0483</span><span class="p">,</span> <span class="mf">0.6931</span><span class="p">,</span> <span class="mf">0.0935</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.7522</span><span class="p">,</span> <span class="mf">0.0054</span><span class="p">,</span> <span class="mf">0.1736</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.6196</span><span class="p">,</span> <span class="mf">0.1028</span><span class="p">,</span> <span class="mf">0.7076</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.2745</span><span class="p">,</span> <span class="mf">0.9943</span><span class="p">,</span> <span class="mf">0.6971</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.3267</span><span class="p">,</span> <span class="mf">0.3748</span><span class="p">,</span> <span class="mf">0.1232</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0507</span><span class="p">,</span> <span class="mf">0.4302</span><span class="p">,</span> <span class="mf">0.6249</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.2783</span><span class="p">,</span> <span class="mf">0.8262</span><span class="p">,</span> <span class="mf">0.6014</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.8040</span><span class="p">,</span> <span class="mf">0.7986</span><span class="p">,</span> <span class="mf">0.2831</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.6035</span><span class="p">,</span> <span class="mf">0.2955</span><span class="p">,</span> <span class="mf">0.7711</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.7464</span><span class="p">,</span> <span class="mf">0.3739</span><span class="p">,</span> <span class="mf">0.6637</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.6282</span><span class="p">,</span> <span class="mf">0.7243</span><span class="p">,</span> <span class="mf">0.5445</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.3755</span><span class="p">,</span> <span class="mf">0.0533</span><span class="p">,</span> <span class="mf">0.9468</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.5179</span><span class="p">,</span> <span class="mf">0.3967</span><span class="p">,</span> <span class="mf">0.6558</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0267</span><span class="p">,</span> <span class="mf">0.5549</span><span class="p">,</span> <span class="mf">0.9707</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.4388</span><span class="p">,</span> <span class="mf">0.7458</span><span class="p">,</span> <span class="mf">0.2065</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.6080</span><span class="p">,</span> <span class="mf">0.4242</span><span class="p">,</span> <span class="mf">0.8879</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.3428</span><span class="p">,</span> <span class="mf">0.6976</span><span class="p">,</span> <span class="mf">0.0970</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.9552</span><span class="p">,</span> <span class="mf">0.3663</span><span class="p">,</span> <span class="mf">0.2139</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.2019</span><span class="p">,</span> <span class="mf">0.2452</span><span class="p">,</span> <span class="mf">0.1142</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.3651</span><span class="p">,</span> <span class="mf">0.6993</span><span class="p">,</span> <span class="mf">0.5257</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.9636</span><span class="p">,</span> <span class="mf">0.1691</span><span class="p">,</span> <span class="mf">0.4807</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.9137</span><span class="p">,</span> <span class="mf">0.3510</span><span class="p">,</span> <span class="mf">0.0905</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0177</span><span class="p">,</span> <span class="mf">0.9496</span><span class="p">,</span> <span class="mf">0.1560</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.7437</span><span class="p">,</span> <span class="mf">0.9043</span><span class="p">,</span> <span class="mf">0.0131</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.9699</span><span class="p">,</span> <span class="mf">0.5352</span><span class="p">,</span> <span class="mf">0.9763</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.1850</span><span class="p">,</span> <span class="mf">0.2056</span><span class="p">,</span> <span class="mf">0.0368</span><span class="p">]]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="n">r1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[[</span><span class="mf">0.8433</span><span class="p">,</span> <span class="mf">0.5262</span><span class="p">,</span> <span class="mf">0.2608</span><span class="p">,</span> <span class="mf">0.8501</span><span class="p">,</span> <span class="mf">0.7187</span><span class="p">,</span> <span class="mf">0.6944</span><span class="p">,</span> <span class="mf">0.0193</span><span class="p">,</span> <span class="mf">0.1507</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.0450</span><span class="p">,</span> <span class="mf">0.2257</span><span class="p">,</span> <span class="mf">0.4679</span><span class="p">,</span> <span class="mf">0.8309</span><span class="p">,</span> <span class="mf">0.4740</span><span class="p">,</span> <span class="mf">0.8715</span><span class="p">,</span> <span class="mf">0.7443</span><span class="p">,</span> <span class="mf">0.3354</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.5533</span><span class="p">,</span> <span class="mf">0.9151</span><span class="p">,</span> <span class="mf">0.4215</span><span class="p">,</span> <span class="mf">0.4631</span><span class="p">,</span> <span class="mf">0.9076</span><span class="p">,</span> <span class="mf">0.3093</span><span class="p">,</span> <span class="mf">0.0270</span><span class="p">,</span> <span class="mf">0.7681</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.1800</span><span class="p">,</span> <span class="mf">0.0847</span><span class="p">,</span> <span class="mf">0.6965</span><span class="p">,</span> <span class="mf">0.2059</span><span class="p">,</span> <span class="mf">0.8806</span><span class="p">,</span> <span class="mf">0.3987</span><span class="p">,</span> <span class="mf">0.8446</span><span class="p">,</span> <span class="mf">0.6225</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.1375</span><span class="p">,</span> <span class="mf">0.8765</span><span class="p">,</span> <span class="mf">0.5965</span><span class="p">,</span> <span class="mf">0.3092</span><span class="p">,</span> <span class="mf">0.0193</span><span class="p">,</span> <span class="mf">0.9220</span><span class="p">,</span> <span class="mf">0.4997</span><span class="p">,</span> <span class="mf">0.8170</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.8575</span><span class="p">,</span> <span class="mf">0.5525</span><span class="p">,</span> <span class="mf">0.8528</span><span class="p">,</span> <span class="mf">0.7262</span><span class="p">,</span> <span class="mf">0.4026</span><span class="p">,</span> <span class="mf">0.5704</span><span class="p">,</span> <span class="mf">0.0390</span><span class="p">,</span> <span class="mf">0.9240</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.9780</span><span class="p">,</span> <span class="mf">0.3927</span><span class="p">,</span> <span class="mf">0.7343</span><span class="p">,</span> <span class="mf">0.3922</span><span class="p">,</span> <span class="mf">0.5004</span><span class="p">,</span> <span class="mf">0.8561</span><span class="p">,</span> <span class="mf">0.6021</span><span class="p">,</span> <span class="mf">0.6530</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.6565</span><span class="p">,</span> <span class="mf">0.9988</span><span class="p">,</span> <span class="mf">0.4238</span><span class="p">,</span> <span class="mf">0.0092</span><span class="p">,</span> <span class="mf">0.5131</span><span class="p">,</span> <span class="mf">0.5257</span><span class="p">,</span> <span class="mf">0.1649</span><span class="p">,</span> <span class="mf">0.0272</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.9103</span><span class="p">,</span> <span class="mf">0.2476</span><span class="p">,</span> <span class="mf">0.7573</span><span class="p">,</span> <span class="mf">0.8500</span><span class="p">,</span> <span class="mf">0.9348</span><span class="p">,</span> <span class="mf">0.4306</span><span class="p">,</span> <span class="mf">0.3612</span><span class="p">,</span> <span class="mf">0.5378</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.7141</span><span class="p">,</span> <span class="mf">0.3559</span><span class="p">,</span> <span class="mf">0.6620</span><span class="p">,</span> <span class="mf">0.3335</span><span class="p">,</span> <span class="mf">0.4000</span><span class="p">,</span> <span class="mf">0.2479</span><span class="p">,</span> <span class="mf">0.3490</span><span class="p">,</span> <span class="mf">0.7000</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.5321</span><span class="p">,</span> <span class="mf">0.3485</span><span class="p">,</span> <span class="mf">0.9162</span><span class="p">,</span> <span class="mf">0.9207</span><span class="p">,</span> <span class="mf">0.3262</span><span class="p">,</span> <span class="mf">0.7929</span><span class="p">,</span> <span class="mf">0.1258</span><span class="p">,</span> <span class="mf">0.6689</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.1023</span><span class="p">,</span> <span class="mf">0.1938</span><span class="p">,</span> <span class="mf">0.3887</span><span class="p">,</span> <span class="mf">0.6893</span><span class="p">,</span> <span class="mf">0.0849</span><span class="p">,</span> <span class="mf">0.3700</span><span class="p">,</span> <span class="mf">0.5747</span><span class="p">,</span> <span class="mf">0.9674</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.4520</span><span class="p">,</span> <span class="mf">0.5313</span><span class="p">,</span> <span class="mf">0.0377</span><span class="p">,</span> <span class="mf">0.1202</span><span class="p">,</span> <span class="mf">0.9326</span><span class="p">,</span> <span class="mf">0.0442</span><span class="p">,</span> <span class="mf">0.4651</span><span class="p">,</span> <span class="mf">0.7036</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.3994</span><span class="p">,</span> <span class="mf">0.9332</span><span class="p">,</span> <span class="mf">0.5104</span><span class="p">,</span> <span class="mf">0.0930</span><span class="p">,</span> <span class="mf">0.4481</span><span class="p">,</span> <span class="mf">0.8753</span><span class="p">,</span> <span class="mf">0.5597</span><span class="p">,</span> <span class="mf">0.6068</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.9895</span><span class="p">,</span> <span class="mf">0.5833</span><span class="p">,</span> <span class="mf">0.6771</span><span class="p">,</span> <span class="mf">0.4255</span><span class="p">,</span> <span class="mf">0.4513</span><span class="p">,</span> <span class="mf">0.6330</span><span class="p">,</span> <span class="mf">0.9070</span><span class="p">,</span> <span class="mf">0.3103</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.0609</span><span class="p">,</span> <span class="mf">0.8202</span><span class="p">,</span> <span class="mf">0.6031</span><span class="p">,</span> <span class="mf">0.3628</span><span class="p">,</span> <span class="mf">0.1118</span><span class="p">,</span> <span class="mf">0.2747</span><span class="p">,</span> <span class="mf">0.4521</span><span class="p">,</span> <span class="mf">0.8347</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.6759</span><span class="p">,</span> <span class="mf">0.8744</span><span class="p">,</span> <span class="mf">0.3595</span><span class="p">,</span> <span class="mf">0.2361</span><span class="p">,</span> <span class="mf">0.4899</span><span class="p">,</span> <span class="mf">0.3769</span><span class="p">,</span> <span class="mf">0.6809</span><span class="p">,</span> <span class="mf">0.0101</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.0730</span><span class="p">,</span> <span class="mf">0.0576</span><span class="p">,</span> <span class="mf">0.5242</span><span class="p">,</span> <span class="mf">0.5510</span><span class="p">,</span> <span class="mf">0.9780</span><span class="p">,</span> <span class="mf">0.4704</span><span class="p">,</span> <span class="mf">0.9607</span><span class="p">,</span> <span class="mf">0.1699</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.3613</span><span class="p">,</span> <span class="mf">0.6096</span><span class="p">,</span> <span class="mf">0.0246</span><span class="p">,</span> <span class="mf">0.6088</span><span class="p">,</span> <span class="mf">0.4984</span><span class="p">,</span> <span class="mf">0.9788</span><span class="p">,</span> <span class="mf">0.2026</span><span class="p">,</span> <span class="mf">0.1484</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.3086</span><span class="p">,</span> <span class="mf">0.9697</span><span class="p">,</span> <span class="mf">0.8166</span><span class="p">,</span> <span class="mf">0.9566</span><span class="p">,</span> <span class="mf">0.9874</span><span class="p">,</span> <span class="mf">0.4547</span><span class="p">,</span> <span class="mf">0.5250</span><span class="p">,</span> <span class="mf">0.2041</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.7784</span><span class="p">,</span> <span class="mf">0.4269</span><span class="p">,</span> <span class="mf">0.0110</span><span class="p">,</span> <span class="mf">0.6878</span><span class="p">,</span> <span class="mf">0.6575</span><span class="p">,</span> <span class="mf">0.3382</span><span class="p">,</span> <span class="mf">0.1889</span><span class="p">,</span> <span class="mf">0.8344</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.9608</span><span class="p">,</span> <span class="mf">0.6153</span><span class="p">,</span> <span class="mf">0.4812</span><span class="p">,</span> <span class="mf">0.0547</span><span class="p">,</span> <span class="mf">0.2978</span><span class="p">,</span> <span class="mf">0.3610</span><span class="p">,</span> <span class="mf">0.5285</span><span class="p">,</span> <span class="mf">0.6162</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.2123</span><span class="p">,</span> <span class="mf">0.1364</span><span class="p">,</span> <span class="mf">0.6027</span><span class="p">,</span> <span class="mf">0.7450</span><span class="p">,</span> <span class="mf">0.2485</span><span class="p">,</span> <span class="mf">0.2149</span><span class="p">,</span> <span class="mf">0.7849</span><span class="p">,</span> <span class="mf">0.8886</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.0514</span><span class="p">,</span> <span class="mf">0.9511</span><span class="p">,</span> <span class="mf">0.4865</span><span class="p">,</span> <span class="mf">0.8380</span><span class="p">,</span> <span class="mf">0.6947</span><span class="p">,</span> <span class="mf">0.2378</span><span class="p">,</span> <span class="mf">0.5839</span><span class="p">,</span> <span class="mf">0.8434</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.0871</span><span class="p">,</span> <span class="mf">0.4179</span><span class="p">,</span> <span class="mf">0.1669</span><span class="p">,</span> <span class="mf">0.8703</span><span class="p">,</span> <span class="mf">0.1946</span><span class="p">,</span> <span class="mf">0.0302</span><span class="p">,</span> <span class="mf">0.9516</span><span class="p">,</span> <span class="mf">0.1208</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.5780</span><span class="p">,</span> <span class="mf">0.6859</span><span class="p">,</span> <span class="mf">0.2405</span><span class="p">,</span> <span class="mf">0.5083</span><span class="p">,</span> <span class="mf">0.3872</span><span class="p">,</span> <span class="mf">0.7649</span><span class="p">,</span> <span class="mf">0.1329</span><span class="p">,</span> <span class="mf">0.0252</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.2404</span><span class="p">,</span> <span class="mf">0.5456</span><span class="p">,</span> <span class="mf">0.7009</span><span class="p">,</span> <span class="mf">0.6524</span><span class="p">,</span> <span class="mf">0.7623</span><span class="p">,</span> <span class="mf">0.5965</span><span class="p">,</span> <span class="mf">0.0437</span><span class="p">,</span> <span class="mf">0.4080</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.8390</span><span class="p">,</span> <span class="mf">0.4172</span><span class="p">,</span> <span class="mf">0.4781</span><span class="p">,</span> <span class="mf">0.2405</span><span class="p">,</span> <span class="mf">0.1502</span><span class="p">,</span> <span class="mf">0.2020</span><span class="p">,</span> <span class="mf">0.4192</span><span class="p">,</span> <span class="mf">0.8185</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.0899</span><span class="p">,</span> <span class="mf">0.1961</span><span class="p">,</span> <span class="mf">0.7368</span><span class="p">,</span> <span class="mf">0.4798</span><span class="p">,</span> <span class="mf">0.4303</span><span class="p">,</span> <span class="mf">0.9281</span><span class="p">,</span> <span class="mf">0.5410</span><span class="p">,</span> <span class="mf">0.0620</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.8945</span><span class="p">,</span> <span class="mf">0.3589</span><span class="p">,</span> <span class="mf">0.5637</span><span class="p">,</span> <span class="mf">0.4875</span><span class="p">,</span> <span class="mf">0.1523</span><span class="p">,</span> <span class="mf">0.9478</span><span class="p">,</span> <span class="mf">0.9040</span><span class="p">,</span> <span class="mf">0.3410</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.3591</span><span class="p">,</span> <span class="mf">0.2702</span><span class="p">,</span> <span class="mf">0.5949</span><span class="p">,</span> <span class="mf">0.3337</span><span class="p">,</span> <span class="mf">0.3578</span><span class="p">,</span> <span class="mf">0.8890</span><span class="p">,</span> <span class="mf">0.6608</span><span class="p">,</span> <span class="mf">0.6578</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.4953</span><span class="p">,</span> <span class="mf">0.7975</span><span class="p">,</span> <span class="mf">0.2891</span><span class="p">,</span> <span class="mf">0.9552</span><span class="p">,</span> <span class="mf">0.0092</span><span class="p">,</span> <span class="mf">0.1293</span><span class="p">,</span> <span class="mf">0.2362</span><span class="p">,</span> <span class="mf">0.7821</span><span class="p">]]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="n">r2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[[</span><span class="mf">6.4270e-01</span><span class="p">,</span> <span class="mf">1.3050e-01</span><span class="p">,</span> <span class="mf">9.6509e-01</span><span class="p">,</span> <span class="mf">1.4090e-01</span><span class="p">,</span> <span class="mf">1.8660e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.7512e-01</span><span class="p">,</span> <span class="mf">3.8567e-01</span><span class="p">,</span> <span class="mf">4.1776e-01</span><span class="p">,</span> <span class="mf">9.7718e-01</span><span class="p">,</span> <span class="mf">5.6305e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.3091e-01</span><span class="p">,</span> <span class="mf">4.6385e-01</span><span class="p">,</span> <span class="mf">1.8142e-01</span><span class="p">,</span> <span class="mf">3.7779e-01</span><span class="p">,</span> <span class="mf">3.8012e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.1807e-01</span><span class="p">,</span> <span class="mf">3.3292e-01</span><span class="p">,</span> <span class="mf">5.8488e-01</span><span class="p">,</span> <span class="mf">5.8188e-01</span><span class="p">,</span> <span class="mf">5.7776e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.1828e-01</span><span class="p">,</span> <span class="mf">9.6087e-01</span><span class="p">,</span> <span class="mf">7.2219e-01</span><span class="p">,</span> <span class="mf">8.5045e-02</span><span class="p">,</span> <span class="mf">3.6623e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.3758e-01</span><span class="p">,</span> <span class="mf">7.9666e-01</span><span class="p">,</span> <span class="mf">6.9932e-01</span><span class="p">,</span> <span class="mf">9.9202e-01</span><span class="p">,</span> <span class="mf">2.5493e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.3017e-01</span><span class="p">,</span> <span class="mf">7.9396e-01</span><span class="p">,</span> <span class="mf">5.0109e-01</span><span class="p">,</span> <span class="mf">6.5580e-01</span><span class="p">,</span> <span class="mf">3.2200e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.8023e-01</span><span class="p">,</span> <span class="mf">4.4098e-01</span><span class="p">,</span> <span class="mf">1.0576e-01</span><span class="p">,</span> <span class="mf">8.0548e-01</span><span class="p">,</span> <span class="mf">2.2453e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.4705e-01</span><span class="p">,</span> <span class="mf">8.7682e-02</span><span class="p">,</span> <span class="mf">4.7264e-01</span><span class="p">,</span> <span class="mf">8.9034e-02</span><span class="p">,</span> <span class="mf">8.5720e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.7576e-01</span><span class="p">,</span> <span class="mf">2.8438e-01</span><span class="p">,</span> <span class="mf">8.6523e-01</span><span class="p">,</span> <span class="mf">8.1707e-02</span><span class="p">,</span> <span class="mf">3.0075e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.9069e-01</span><span class="p">,</span> <span class="mf">9.7404e-01</span><span class="p">,</span> <span class="mf">9.3865e-01</span><span class="p">,</span> <span class="mf">5.7160e-01</span><span class="p">,</span> <span class="mf">1.6332e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.3868e-01</span><span class="p">,</span> <span class="mf">5.8658e-01</span><span class="p">,</span> <span class="mf">5.3993e-01</span><span class="p">,</span> <span class="mf">3.8271e-02</span><span class="p">,</span> <span class="mf">9.9662e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.2892e-01</span><span class="p">,</span> <span class="mf">7.8558e-01</span><span class="p">,</span> <span class="mf">9.4502e-01</span><span class="p">,</span> <span class="mf">9.7633e-01</span><span class="p">,</span> <span class="mf">1.7877e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.6446e-02</span><span class="p">,</span> <span class="mf">2.3411e-01</span><span class="p">,</span> <span class="mf">6.7531e-01</span><span class="p">,</span> <span class="mf">1.5023e-01</span><span class="p">,</span> <span class="mf">4.4280e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.4457e-01</span><span class="p">,</span> <span class="mf">3.6683e-01</span><span class="p">,</span> <span class="mf">4.3424e-01</span><span class="p">,</span> <span class="mf">7.4145e-01</span><span class="p">,</span> <span class="mf">8.2433e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.8660e-01</span><span class="p">,</span> <span class="mf">6.7477e-01</span><span class="p">,</span> <span class="mf">5.5000e-02</span><span class="p">,</span> <span class="mf">5.1344e-01</span><span class="p">,</span> <span class="mf">9.3115e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.8280e-01</span><span class="p">,</span> <span class="mf">9.2177e-01</span><span class="p">,</span> <span class="mf">4.5470e-01</span><span class="p">,</span> <span class="mf">2.5540e-01</span><span class="p">,</span> <span class="mf">4.6632e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.3960e-01</span><span class="p">,</span> <span class="mf">4.4320e-01</span><span class="p">,</span> <span class="mf">1.0808e-01</span><span class="p">,</span> <span class="mf">7.5544e-01</span><span class="p">,</span> <span class="mf">4.6372e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.4322e-01</span><span class="p">,</span> <span class="mf">1.9141e-01</span><span class="p">,</span> <span class="mf">5.5918e-02</span><span class="p">,</span> <span class="mf">7.0804e-01</span><span class="p">,</span> <span class="mf">1.8789e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.4276e-01</span><span class="p">,</span> <span class="mf">9.1742e-01</span><span class="p">,</span> <span class="mf">9.1980e-01</span><span class="p">,</span> <span class="mf">6.2728e-01</span><span class="p">,</span> <span class="mf">4.1787e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.9545e-01</span><span class="p">,</span> <span class="mf">9.0569e-01</span><span class="p">,</span> <span class="mf">7.9123e-01</span><span class="p">,</span> <span class="mf">9.7596e-01</span><span class="p">,</span> <span class="mf">7.2507e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.3772e-01</span><span class="p">,</span> <span class="mf">8.2560e-01</span><span class="p">,</span> <span class="mf">5.9359e-01</span><span class="p">,</span> <span class="mf">7.1134e-01</span><span class="p">,</span> <span class="mf">5.1029e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.1601e-01</span><span class="p">,</span> <span class="mf">2.9094e-01</span><span class="p">,</span> <span class="mf">3.4174e-01</span><span class="p">,</span> <span class="mf">9.0532e-01</span><span class="p">,</span> <span class="mf">5.0960e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.4441e-01</span><span class="p">,</span> <span class="mf">7.0498e-01</span><span class="p">,</span> <span class="mf">4.2729e-01</span><span class="p">,</span> <span class="mf">7.6714e-01</span><span class="p">,</span> <span class="mf">6.3755e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.4604e-01</span><span class="p">,</span> <span class="mf">5.9109e-01</span><span class="p">,</span> <span class="mf">7.9137e-01</span><span class="p">,</span> <span class="mf">7.5149e-01</span><span class="p">,</span> <span class="mf">2.2092e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.5235e-01</span><span class="p">,</span> <span class="mf">3.6915e-01</span><span class="p">,</span> <span class="mf">6.4961e-01</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">8.7862e-01</span><span class="p">,</span> <span class="mf">1.1325e-01</span><span class="p">,</span> <span class="mf">2.4575e-01</span><span class="p">,</span> <span class="mf">9.7429e-01</span><span class="p">,</span> <span class="mf">1.9362e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.2297e-01</span><span class="p">,</span> <span class="mf">3.5184e-02</span><span class="p">,</span> <span class="mf">5.2755e-01</span><span class="p">,</span> <span class="mf">7.6429e-01</span><span class="p">,</span> <span class="mf">2.4700e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.2860e-01</span><span class="p">,</span> <span class="mf">2.4555e-01</span><span class="p">,</span> <span class="mf">4.4557e-01</span><span class="p">,</span> <span class="mf">7.0955e-03</span><span class="p">,</span> <span class="mf">2.0326e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.6354e-02</span><span class="p">,</span> <span class="mf">3.5959e-01</span><span class="p">,</span> <span class="mf">3.4059e-01</span><span class="p">,</span> <span class="mf">8.6852e-01</span><span class="p">,</span> <span class="mf">1.3858e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.8500e-01</span><span class="p">,</span> <span class="mf">1.3601e-01</span><span class="p">,</span> <span class="mf">7.3152e-01</span><span class="p">,</span> <span class="mf">8.3474e-01</span><span class="p">,</span> <span class="mf">2.7017e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.8078e-01</span><span class="p">,</span> <span class="mf">6.1084e-01</span><span class="p">,</span> <span class="mf">1.6540e-01</span><span class="p">,</span> <span class="mf">4.3081e-01</span><span class="p">,</span> <span class="mf">8.5738e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.1890e-01</span><span class="p">,</span> <span class="mf">6.6872e-01</span><span class="p">,</span> <span class="mf">3.1698e-01</span><span class="p">,</span> <span class="mf">4.2576e-02</span><span class="p">,</span> <span class="mf">1.5236e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.0526e-01</span><span class="p">,</span> <span class="mf">1.9493e-01</span><span class="p">,</span> <span class="mf">6.6122e-03</span><span class="p">,</span> <span class="mf">1.8332e-01</span><span class="p">,</span> <span class="mf">5.6981e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.4090e-01</span><span class="p">,</span> <span class="mf">6.0783e-01</span><span class="p">,</span> <span class="mf">5.8742e-01</span><span class="p">,</span> <span class="mf">9.1761e-04</span><span class="p">,</span> <span class="mf">2.0904e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.6419e-01</span><span class="p">,</span> <span class="mf">9.9559e-01</span><span class="p">,</span> <span class="mf">5.8233e-01</span><span class="p">,</span> <span class="mf">6.8562e-01</span><span class="p">,</span> <span class="mf">8.6456e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.9931e-01</span><span class="p">,</span> <span class="mf">3.5637e-01</span><span class="p">,</span> <span class="mf">2.4642e-01</span><span class="p">,</span> <span class="mf">2.3428e-02</span><span class="p">,</span> <span class="mf">6.9037e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.7560e-01</span><span class="p">,</span> <span class="mf">1.8703e-01</span><span class="p">,</span> <span class="mf">3.5244e-01</span><span class="p">,</span> <span class="mf">6.3031e-01</span><span class="p">,</span> <span class="mf">1.8450e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.2194e-01</span><span class="p">,</span> <span class="mf">9.3016e-02</span><span class="p">,</span> <span class="mf">9.0488e-01</span><span class="p">,</span> <span class="mf">2.4294e-02</span><span class="p">,</span> <span class="mf">5.1122e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.0793e-01</span><span class="p">,</span> <span class="mf">7.9585e-01</span><span class="p">,</span> <span class="mf">7.9594e-02</span><span class="p">,</span> <span class="mf">5.2137e-01</span><span class="p">,</span> <span class="mf">9.8359e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.5022e-01</span><span class="p">,</span> <span class="mf">4.1925e-01</span><span class="p">,</span> <span class="mf">3.3284e-01</span><span class="p">,</span> <span class="mf">4.7939e-01</span><span class="p">,</span> <span class="mf">9.9081e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">3.3931e-01</span><span class="p">,</span> <span class="mf">2.6461e-01</span><span class="p">,</span> <span class="mf">5.3063e-01</span><span class="p">,</span> <span class="mf">1.0328e-01</span><span class="p">,</span> <span class="mf">8.0720e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">9.9480e-01</span><span class="p">,</span> <span class="mf">3.0833e-01</span><span class="p">,</span> <span class="mf">5.6780e-01</span><span class="p">,</span> <span class="mf">3.9551e-01</span><span class="p">,</span> <span class="mf">6.7176e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.8049e-01</span><span class="p">,</span> <span class="mf">1.5653e-01</span><span class="p">,</span> <span class="mf">1.7595e-02</span><span class="p">,</span> <span class="mf">6.6493e-02</span><span class="p">,</span> <span class="mf">5.1989e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.2691e-01</span><span class="p">,</span> <span class="mf">7.3295e-01</span><span class="p">,</span> <span class="mf">5.7169e-01</span><span class="p">,</span> <span class="mf">4.9911e-01</span><span class="p">,</span> <span class="mf">1.0260e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">5.2307e-01</span><span class="p">,</span> <span class="mf">7.4247e-01</span><span class="p">,</span> <span class="mf">1.1682e-01</span><span class="p">,</span> <span class="mf">5.8123e-01</span><span class="p">,</span> <span class="mf">7.3496e-02</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.4274e-02</span><span class="p">,</span> <span class="mf">2.4704e-01</span><span class="p">,</span> <span class="mf">6.0424e-02</span><span class="p">,</span> <span class="mf">2.6161e-01</span><span class="p">,</span> <span class="mf">7.7966e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">7.1244e-01</span><span class="p">,</span> <span class="mf">2.2077e-01</span><span class="p">,</span> <span class="mf">5.0723e-01</span><span class="p">,</span> <span class="mf">9.6665e-01</span><span class="p">,</span> <span class="mf">6.0933e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">8.1332e-01</span><span class="p">,</span> <span class="mf">3.0749e-01</span><span class="p">,</span> <span class="mf">2.1297e-02</span><span class="p">,</span> <span class="mf">3.6734e-01</span><span class="p">,</span> <span class="mf">9.2542e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.3554e-01</span><span class="p">,</span> <span class="mf">9.7240e-01</span><span class="p">,</span> <span class="mf">4.4344e-01</span><span class="p">,</span> <span class="mf">4.2534e-01</span><span class="p">,</span> <span class="mf">4.6205e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">6.1811e-01</span><span class="p">,</span> <span class="mf">5.8800e-01</span><span class="p">,</span> <span class="mf">5.4673e-01</span><span class="p">,</span> <span class="mf">1.2535e-01</span><span class="p">,</span> <span class="mf">2.9959e-01</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">4.4890e-01</span><span class="p">,</span> <span class="mf">2.7185e-01</span><span class="p">,</span> <span class="mf">5.0243e-01</span><span class="p">]]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_rotary_mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">r1</span><span class="p">,</span> <span class="n">r2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[[</span> <span class="mf">0.1142</span><span class="p">,</span>  <span class="mf">0.1891</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4689</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.5704</span><span class="p">,</span>  <span class="mf">0.5375</span><span class="p">,</span>  <span class="mf">0.6079</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.2264</span><span class="p">,</span>  <span class="mf">0.1155</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7678</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.9857</span><span class="p">,</span>  <span class="mf">0.3382</span><span class="p">,</span>  <span class="mf">0.9441</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.1902</span><span class="p">,</span>  <span class="mf">0.1329</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3613</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.9793</span><span class="p">,</span>  <span class="mf">0.5628</span><span class="p">,</span>  <span class="mf">0.8669</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.3349</span><span class="p">,</span>  <span class="mf">0.1532</span><span class="p">,</span>  <span class="mf">0.1124</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.3125</span><span class="p">,</span>  <span class="mf">0.6741</span><span class="p">,</span>  <span class="mf">1.1248</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.0473</span><span class="p">,</span>  <span class="mf">0.2978</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6940</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.2753</span><span class="p">,</span>  <span class="mf">0.2604</span><span class="p">,</span>  <span class="mf">1.0379</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span> <span class="mf">0.0136</span><span class="p">,</span>  <span class="mf">0.4723</span><span class="p">,</span>  <span class="mf">0.1371</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.1081</span><span class="p">,</span>  <span class="mf">0.2462</span><span class="p">,</span>  <span class="mf">0.6316</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0769</span><span class="p">,</span>  <span class="mf">0.6558</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0734</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.2714</span><span class="p">,</span>  <span class="mf">0.2221</span><span class="p">,</span>  <span class="mf">0.2195</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.3755</span><span class="p">,</span>  <span class="mf">0.5364</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1131</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.3105</span><span class="p">,</span>  <span class="mf">0.1225</span><span class="p">,</span>  <span class="mf">0.6166</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.3535</span><span class="p">,</span>  <span class="mf">0.0164</span><span class="p">,</span>  <span class="mf">0.0095</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.1361</span><span class="p">,</span>  <span class="mf">0.2570</span><span class="p">,</span>  <span class="mf">0.5811</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.2992</span><span class="p">,</span>  <span class="mf">0.2981</span><span class="p">,</span>  <span class="mf">0.0242</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.2881</span><span class="p">,</span>  <span class="mf">0.2367</span><span class="p">,</span>  <span class="mf">0.9582</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span> <span class="mf">0.1699</span><span class="p">,</span>  <span class="mf">0.3589</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7443</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.4751</span><span class="p">,</span>  <span class="mf">0.7291</span><span class="p">,</span>  <span class="mf">0.2717</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.3657</span><span class="p">,</span>  <span class="mf">0.0397</span><span class="p">,</span>  <span class="mf">0.1818</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.9113</span><span class="p">,</span>  <span class="mf">0.4130</span><span class="p">,</span>  <span class="mf">0.8279</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.0657</span><span class="p">,</span>  <span class="mf">0.2528</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6658</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.8184</span><span class="p">,</span>  <span class="mf">0.2057</span><span class="p">,</span>  <span class="mf">1.2864</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.1058</span><span class="p">,</span>  <span class="mf">0.1859</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0998</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0662</span><span class="p">,</span>  <span class="mf">0.5590</span><span class="p">,</span>  <span class="mf">1.0525</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.2651</span><span class="p">,</span>  <span class="mf">0.3719</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8170</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.2789</span><span class="p">,</span>  <span class="mf">0.3916</span><span class="p">,</span>  <span class="mf">1.0407</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">0.5998</span><span class="p">,</span>  <span class="mf">0.5740</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0154</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.1746</span><span class="p">,</span>  <span class="mf">0.1982</span><span class="p">,</span>  <span class="mf">0.6338</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0766</span><span class="p">,</span>  <span class="mf">0.1790</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1490</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.4387</span><span class="p">,</span>  <span class="mf">0.2592</span><span class="p">,</span>  <span class="mf">0.4924</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.4765</span><span class="p">,</span>  <span class="mf">0.0485</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0226</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.2219</span><span class="p">,</span>  <span class="mf">0.3445</span><span class="p">,</span>  <span class="mf">0.2265</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.1006</span><span class="p">,</span>  <span class="mf">0.8073</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1540</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.1045</span><span class="p">,</span>  <span class="mf">0.2633</span><span class="p">,</span>  <span class="mf">0.2194</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0157</span><span class="p">,</span>  <span class="mf">0.3997</span><span class="p">,</span>  <span class="mf">0.3131</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0538</span><span class="p">,</span>  <span class="mf">0.0647</span><span class="p">,</span>  <span class="mf">0.4821</span><span class="p">]]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_rotated_box_decode">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_rotated_box_decode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_rotated_box_decode" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_rotated_box_decode(anchor_boxes, deltas, weight) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>旋转标注框编码。</p>
<p><strong>参数说明</strong></p>
<p>anchor_box (Tensor) - shape为(B,5,N)的3D输入张量，表示锚点框。“B”表示批处理大小数量，“N”表示标注框数量，值“5”表示“x0”、“x1”、“y0”、“y1”和“angle”。</p>
<p>deltas (Tensor) - shape为(B,5,N)数据类型为float32 (float16)的3D张量。</p>
<p>weight (Tensor，默认值为[1.0, 1.0, 1.0, 1.0, 1.0]) - “x0”、“x1”、“y0”、“y1”和“angle”的浮点列表。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">anchor_boxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">4.137</span><span class="p">],[</span><span class="mf">33.72</span><span class="p">],[</span><span class="mf">29.4</span><span class="p">],</span> <span class="p">[</span><span class="mf">54.06</span><span class="p">],</span> <span class="p">[</span><span class="mf">41.28</span><span class="p">]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">deltas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">0.0244</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.992</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2109</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.315</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">37.25</span><span class="p">]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_rotated_box_decode</span><span class="p">(</span><span class="n">anchor_boxes</span><span class="p">,</span> <span class="n">deltas</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span>  <span class="mf">1.7861</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">10.5781</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">33.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">17.2969</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">88.4375</span><span class="p">]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_rotated_box_encode">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_rotated_box_encode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_rotated_box_encode" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_rotated_box_encode(anchor_box, gt_bboxes, weight) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>旋转标注框编码。</p>
<p><strong>参数说明</strong></p>
<p>anchor_box (Tensor) - shape为(B,5,N)的3D输入张量，表示锚点框。“B”表示批处理大小数量，“N”表示标注框数量，值“5”表示“x0”、“x1”、“y0”、“y1”和“angle”。</p>
<p>gt_bboxes (Tensor) - shape为(B,5,N)数据类型为float32 (float16)的3D张量。</p>
<p>weight (Tensor，默认值为[1.0, 1.0, 1.0, 1.0, 1.0]) - “x0”、“x1”、“y0”、“y1”和“angle”的浮点列表。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">anchor_boxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">30.69</span><span class="p">],</span> <span class="p">[</span><span class="mf">32.6</span><span class="p">],</span> <span class="p">[</span><span class="mf">45.94</span><span class="p">],</span> <span class="p">[</span><span class="mf">59.88</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">44.53</span><span class="p">]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gt_bboxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">30.44</span><span class="p">],</span> <span class="p">[</span><span class="mf">18.72</span><span class="p">],</span> <span class="p">[</span><span class="mf">33.22</span><span class="p">],</span> <span class="p">[</span><span class="mf">45.56</span><span class="p">],</span> <span class="p">[</span><span class="mf">8.5</span><span class="p">]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_rotated_box_encode</span><span class="p">(</span><span class="n">anchor_boxes</span><span class="p">,</span> <span class="n">gt_bboxes</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.4253</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.5166</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.7021</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">0.0162</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.1328</span><span class="p">]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_rotated_iou">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_rotated_iou</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_rotated_iou" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_rotated_iou(self, query_boxes, trans=False, mode=0, is_cross=True,v_threshold=0.0, e_threshold=0.0) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>计算旋转框的IoU。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 梯度增量数据，shape为(B, 5, N)数据类型为float32的3D张量。</p>
<p>query_boxes (Tensor) - 标注框，shape为(B, 5, K) 数据类型为float32的3D张量。</p>
<p>trans (Bool，默认值为False) - 值为True表示“xyxyt”，值为False表示“xywht”。</p>
<p>is_cross (Bool，默认值为True) - 值为True时表示交叉计算，为False时表示一对一计算。</p>
<p>mode (Int，默认值为0) - 计算模式，取值为0或1。0表示IoU，1表示IoF。</p>
<p>v_threshold (Float，可选，默认值为0.0) - provide condition relaxation for intersection calculation.</p>
<p>e_threshold (Float，可选，默认值为0.0) - provide condition relaxation for intersection calculation.</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box1</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box2</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_rotated_iou</span><span class="p">(</span><span class="n">box1</span><span class="p">,</span> <span class="n">box2</span><span class="p">,</span> <span class="n">trans</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">is_cross</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">3.3325e-01</span><span class="p">,</span> <span class="mf">1.0162e-01</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.0162e-01</span><span class="p">,</span> <span class="mf">1.0000e+00</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">0.0000e+00</span><span class="p">,</span> <span class="mf">0.0000e+00</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000e+00</span><span class="p">,</span> <span class="mf">5.9605e-08</span><span class="p">]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_rotated_overlaps">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_rotated_overlaps</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_rotated_overlaps" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_rotated_overlaps(self, query_boxes, trans=False) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>计算旋转框的重叠面积。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) -梯度增量数据，shape为(B, 5, N)数据类型为float32的3D张量。</p>
<p>query_boxes (Tensor) - 标注框，shape为(B, 5, K) 数据类型为float32的3D张量。</p>
<p>trans (Bool，默认值为False) - 值为True表示“xyxyt”，值为False表示“xywht”。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box1</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box2</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_rotated_overlaps</span><span class="p">(</span><span class="n">box1</span><span class="p">,</span> <span class="n">box2</span><span class="p">,</span> <span class="n">trans</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.1562</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.1562</span><span class="p">,</span> <span class="mf">0.3713</span><span class="p">,</span> <span class="mf">0.0611</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0611</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_scaled_masked_softmax">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_scaled_masked_softmax</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_scaled_masked_softmax" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_scaled_masked_softmax(x, mask, scale=1.0, fixed_triu_mask=False) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>计算输入张量x缩放并按照mask遮蔽后的Softmax结果。</p>
<p><strong>参数说明</strong></p>
<p>x(Tensor)- 输入的logits。支持数据类型：float16、float32、bfloat16。支持格式：[ND，FRACTAL_NZ]。</p>
<p>mask(Tensor)- 输入的掩码。支持数据类型：bool。支持格式：[ND，FRACTAL_NZ]。</p>
<p>scale(float，默认值为1.0)- x的缩放系数。</p>
<p>fixed_triu_mask(bool，默认值为False)- 是否使用自动生成的上三角bool掩码。</p>
<p><strong>约束说明</strong></p>
<p>当前输入x的shape，只支持转为[NCHW]格式后，H和W轴长度大于等于32、小于等于4096、且能被32整除的场景。</p>
<p>输入mask的shape，必须能被broadcast成x的shape。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="mi">2048</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fixed_triu_mask</span> <span class="o">=</span> <span class="kc">False</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_scaled_masked_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">fixed_triu_mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">size</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="mi">2048</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_scatter">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_scatter</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_scatter" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_scatter(self, indices, updates, dim) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>使用dim对scatter结果进行计数。类似于torch.scatter，优化NPU设备实现。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 输入张量。</p>
<p>indices (Tensor) - 待scatter的元素index，可以为空，也可以与src有相同的维数。当为空时，操作返回“self unchanged”。</p>
<p>updates (Tensor) - 待scatter的源元素。</p>
<p>dim (Int) - 要进行index的轴。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>    <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.6279</span><span class="p">,</span> <span class="mf">0.1226</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9041</span><span class="p">,</span> <span class="mf">1.0980</span><span class="p">]])</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.6279</span><span class="p">,</span> <span class="mf">0.1226</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.9041</span><span class="p">,</span> <span class="mf">1.0980</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updates</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.1993</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5247</span><span class="p">])</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updates</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.1993</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5247</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_scatter</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.1993</span><span class="p">,</span>  <span class="mf">0.1226</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.9041</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5247</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_sign_bits_pack">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_sign_bits_pack</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_sign_bits_pack" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_sign_bits_pack(Tensor self, int size) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>将float类型1位Adam打包为uint8。</p>
<p><strong>参数说明</strong></p>
<p>x(Tensor) - 1D float张量。</p>
<p>size(Int) - reshape时输出张量的第一个维度。</p>
<p><strong>约束说明</strong></p>
<p>Size可被float打包的输出整除。如果x的size可被8整除，则输出的size为(size of x)/8；否则，输出的size为(size of x // 8) + 1。将在小端位置添加-1浮点值以填充可整除性。Atlas 训练系列产品支持float32和float16类型输入。Atlas 推理系列产品(Ascend 310P处理器)支持float32和float16类型输入。Atlas 200/300/500 推理产品仅支持float16类型输入。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="n">b</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">sign_bits_pack</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="n">b</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">159</span><span class="p">],[</span><span class="mi">15</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">binary</span> <span class="n">form</span> <span class="n">of</span> <span class="mi">159</span> <span class="ow">is</span> <span class="n">ob10011111</span><span class="p">,</span> <span class="n">corresponds</span> <span class="n">to</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span> <span class="n">respectively</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_sign_bits_unpack">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_sign_bits_unpack</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_sign_bits_unpack" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_sign_bits_unpack(x, dtype, size) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>将uint8类型1位Adam拆包为float。</p>
<p><strong>参数说明</strong></p>
<p>x(Tensor) - 1D uint8张量。</p>
<p>dtype(torch.dtype) - 值为1设置输出类型为float16，值为0设置输出类型为float32。</p>
<p>size(Int) - reshape时输出张量的第一个维度。</p>
<p><strong>约束说明</strong></p>
<p>Size可被uint8s拆包的输出整除。输出大小为(size of x) * 8。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">159</span><span class="p">,</span> <span class="mi">15</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="n">b</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_sign_bits_unpack</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="n">b</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">&gt;&gt;&gt;</span><span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">binary</span> <span class="n">form</span> <span class="n">of</span> <span class="mi">159</span> <span class="ow">is</span> <span class="n">ob00001111</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_silu">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_silu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_silu" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_silu(self) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>计算self的Swish。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 数据类型：float16、float32</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_silu</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.4397</span><span class="p">,</span> <span class="mf">0.7178</span><span class="p">,</span> <span class="mf">0.5190</span><span class="p">,</span> <span class="mf">0.2654</span><span class="p">,</span> <span class="mf">0.2230</span><span class="p">,</span> <span class="mf">0.2674</span><span class="p">,</span> <span class="mf">0.6051</span><span class="p">,</span> <span class="mf">0.3522</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.4679</span><span class="p">,</span> <span class="mf">0.1764</span><span class="p">,</span> <span class="mf">0.6650</span><span class="p">,</span> <span class="mf">0.3175</span><span class="p">,</span> <span class="mf">0.0530</span><span class="p">,</span> <span class="mf">0.4787</span><span class="p">,</span> <span class="mf">0.5621</span><span class="p">,</span> <span class="mf">0.4026</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_slice">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_slice</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_slice" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_slice(self, offsets, size) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>从张量中提取切片。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 输入张量。</p>
<p>offsets (ListInt) - 数据类型：int32，int64。</p>
<p>size (ListInt) - 数据类型：int32，int64。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">10</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offsets</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">&gt;&gt;&gt;</span> <span class="n">size</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_slice</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">6.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_softmax_cross_entropy_with_logits">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_softmax_cross_entropy_with_logits</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_softmax_cross_entropy_with_logits" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_softmax_cross_entropy_with_logits(features, labels) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>计算softmax的交叉熵cost。</p>
<p><strong>参数说明</strong></p>
<p>features (Tensor) - 张量，一个“batch_size * num_classes”矩阵。</p>
<p>labels (Tensor) - 与“features”同类型的张量。一个“batch_size * num_classes”矩阵。</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_sort_v2">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_sort_v2</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_sort_v2" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_sort_v2(self, dim=-1, descending=False, out=None) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>沿给定维度，按无index值对输入张量元素进行升序排序。若dim未设置，则选择输入的最后一个维度。如果descending为True，则元素将按值降序排序。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 输入张量。</p>
<p>dim (Int, 可选,默认值为-1) - 进行排序的维度。</p>
<p>descending (Bool, 可选，默认值为None) - 排序顺序控制(升序或降序)。</p>
<p><strong>约束说明</strong></p>
<p>目前仅支持输入的最后一个维度(dim=-1)。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.0067</span><span class="p">,</span>  <span class="mf">1.7790</span><span class="p">,</span>  <span class="mf">0.5031</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7217</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">1.1685</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0486</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2938</span><span class="p">,</span>  <span class="mf">1.3241</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.1880</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.7447</span><span class="p">,</span>  <span class="mf">1.3976</span><span class="p">,</span>  <span class="mf">0.7380</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sorted_x</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_sort_v2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sorted_x</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.7217</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0067</span><span class="p">,</span>  <span class="mf">0.5029</span><span class="p">,</span>  <span class="mf">1.7793</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">1.0488</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2937</span><span class="p">,</span>  <span class="mf">1.1689</span><span class="p">,</span>  <span class="mf">1.3242</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">-</span><span class="mf">2.7441</span><span class="p">,</span>  <span class="mf">0.1880</span><span class="p">,</span>  <span class="mf">0.7378</span><span class="p">,</span>  <span class="mf">1.3975</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_stride_add">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_stride_add</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_stride_add" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_stride_add(x1, x2, offset1, offset2, c1_len) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>添加两个张量的partial values，格式为NC1HWC0。</p>
<p><strong>参数说明</strong></p>
<p>x1 (Tensor) - 5HD张量。</p>
<p>x2 (Tensor) - 与“x1”类型相同shape相同(C1值除外)的张量。</p>
<p>offset1 (Scalar) - “x1”中C1的offset value。</p>
<p>offset2 (Scalar) - “x2”中C1的offset value。</p>
<p>c1_len (Scalar) - “y”的C1 len。该值必须小于“x1”和“x2”中C1与offset的差值。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[[[</span><span class="mf">1.</span><span class="p">]]]]])</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">=</span><span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_stride_add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">btensor</span><span class="p">([[[[[</span><span class="mf">2.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[[</span><span class="mf">0.</span><span class="p">]]]]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_transpose">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_transpose</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_transpose" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_transpose(self, perm, require_contiguous=True) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>返回原始张量视图，其维度已permute，结果连续。支持FakeTensor模式。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 输入张量。</p>
<p>perm (ListInt) - 对应维度排列。</p>
<p>require_contiguous(Bool，默认值为True) - 用户是否显式指定npu_contiguous算子适配需要对输入Tensor做转连续。默认为False，低性能模式。用户明确知道输入Tensor为连续Tensor或转置Tensor时，才能设置为True使用高性能模式。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">npu_transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_yolo_boxes_encode">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_yolo_boxes_encode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_yolo_boxes_encode" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.npu_transpose(self, perm, require_contiguous=True) -&gt; Tensor</p>
<p><strong>功能描述</strong></p>
<p>返回原始张量视图，其维度已permute，结果连续。支持FakeTensor模式。</p>
<p><strong>参数说明</strong></p>
<p>self (Tensor) - 输入张量。</p>
<p>perm (ListInt) - 对应维度排列。</p>
<p>require_contiguous(Bool，默认值为True) - 用户是否显式指定npu_contiguous算子适配需要对输入Tensor做转连续。默认为False，低性能模式。用户明确知道输入Tensor为连续Tensor或转置Tensor时，才能设置为True使用高性能模式。</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">npu_transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.one_">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">one_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.one_" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p>torch_npu.one_(self) -&gt; Tensor</p>
<p>用1填充self张量。</p>
<p><strong>参数解释</strong>：</p>
<p>self (Tensor) - 输入张量。</p>
<p>约束条件：</p>
<p>无</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xtensor</span><span class="p">([[</span><span class="mf">0.6072</span><span class="p">,</span> <span class="mf">0.9726</span><span class="p">,</span> <span class="mf">0.3475</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">0.3717</span><span class="p">,</span> <span class="mf">0.6135</span><span class="p">,</span> <span class="mf">0.6788</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">one_</span><span class="p">()</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_swiglu">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_swiglu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_swiglu" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p><strong>接口原型</strong>：</p>
<p>torch_npu.npu_swiglu(Tensor self, int dim=-1) -&gt; (Tensor)</p>
<p><strong>功能描述</strong>：</p>
<p>提供swiglu的激活函数。</p>
<p>公式如下：</p>
<p>outputs = swiglu(x, dim = -1) = swish(A) * B = A * sigmoid(A) * B</p>
<p>“x”是输入Tensor。</p>
<p>“dim”是切分维度，默认为-1。</p>
<p>“A”和“B”是x沿dim维度切分的Tensor。</p>
<p><strong>参数说明</strong>：</p>
<p>“x”：Tensor类型，shape支持1-8维，dtype支持FP32、FP16或BF16类型。</p>
<p>“dim”：Int类型，默认为-1。</p>
<p><strong>输出说明</strong>：</p>
<p>输出为Tensor，计算公式的最终输出outputs。</p>
<p>支持的型号:</p>
<p>Atlas A2 训练系列产品</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_swiglu</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_trans_quant_param">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_trans_quant_param</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_trans_quant_param" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p><strong>功能描述</strong>:</p>
<p>完成量化计算参数scale数据类型的转换</p>
<p><strong>接口原型</strong>:</p>
<p>npu_trans_quant_param(Tensor scale, Tensor? offset=None) -&gt; Tensor</p>
<p><strong>参数说明</strong>:</p>
<p>scale(计算输入)：Device侧的Tensor类型，数据类型支持FLOAT32。数据格式支持ND，shape是1维(t，)或者2维(1, n)。其中t=1或n, 其中n与x2的n一致。</p>
<p>offset( 计算输入)：Device侧的Tensor类型，可选参数。数据类型支持FLOAT32，数据格式支持ND，shape是1维(t，)，或者2维(1, n)。其中t=1或n, 其中n与x2的n一致。</p>
<p><strong>输出说明</strong>:</p>
<p>一个Tensor类型的输出，代表npu_trans_quant_param的计算结果。</p>
<p><strong>约束说明</strong>:</p>
<p>1.传入的scale，out不能是空。</p>
<p>2.scale、offset、out的数据类型和数据格式需要在支持的范围之内。</p>
<p>3.scale、offset的shape需要为1维(t,)或者2维(1, n)。其中t = 1或n，其中n与x2的n一致。</p>
<p>4.当scale的shape为两维(1, n)时，scale和offset的shape需要保持一致，且输出shape也为(1, n)。</p>
<p>支持的PyTorch版本:</p>
<p>PyTorch 2.2</p>
<p>PyTorch 2.1</p>
<p>PyTorch 2.0</p>
<p>PyTorch 1.11.0</p>
<p>支持的型号:</p>
<p>Atlas A2 训练系列产品</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; 单算子调用：
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_npu
&gt;&gt;&gt; import logging
&gt;&gt;&gt; import os
&gt;&gt;&gt;
&gt;&gt;&gt; scale = torch.randn(16, dtype=torch.float32)
&gt;&gt;&gt; offset = torch.randn(16, dtype=torch.float32)
&gt;&gt;&gt; npu_out = torch_npu.npu_trans_quant_param(scale.npu(), offset.npu())
&gt;&gt;&gt;
&gt;&gt;&gt; 图模式：
&gt;&gt;&gt; 说明：图模式下，npu_trans_quant_param计算出来的结果tensor为uint64数据类型。由于torch不支持该数据类型，需要搭配其他接口使用，如下面示例代码中的npu_quant_matmul。
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_npu
&gt;&gt;&gt; import torchair as tng
&gt;&gt;&gt; from torchair.ge_concrete_graph import ge_apis as ge
&gt;&gt;&gt; from torchair.configs.compiler_config import CompilerConfig
&gt;&gt;&gt; import logging
&gt;&gt;&gt; from torchair.core.utils import logger
&gt;&gt;&gt; logger.setLevel(logging.DEBUG)
&gt;&gt;&gt; import os
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; os.environ[&quot;ENABLE_ACLNN&quot;] = &quot;true&quot;
&gt;&gt;&gt; config = CompilerConfig()
&gt;&gt;&gt; npu_backend = tng.get_npu_backend(compiler_config=config)
&gt;&gt;&gt;
&gt;&gt;&gt; class MyModel(torch.nn.Module):
&gt;&gt;&gt; def __init__(self):
&gt;&gt;&gt; super().__init__()
&gt;&gt;&gt; def forward(self, x1, x2, scale, offset, bias):
&gt;&gt;&gt; scale_1 = torch_npu.npu_trans_quant_param(scale, offset)
&gt;&gt;&gt; return torch_npu.npu_quant_matmul(x1, x2, scale_1, offset=offset, bias=bias)
&gt;&gt;&gt; cpu_model = MyModel()
&gt;&gt;&gt; model = cpu_model.npu()
&gt;&gt;&gt; cpu_x1 = torch.randint(-1, 1, (15, 1, 512), dtype=torch.int8)
&gt;&gt;&gt; cpu_x2 = torch.randint(-1, 1, (15, 512, 128), dtype=torch.int8)
&gt;&gt;&gt; scale = torch.randn(1, dtype=torch.float32)
&gt;&gt;&gt; offset = torch.randn(1, dtype=torch.float32)
&gt;&gt;&gt; bias = torch.randint(-1,1, (15, 1, 128), dtype=torch.int32)
&gt;&gt;&gt; model = torch.compile(cpu_model, backend=npu_backend, dynamic=True)
&gt;&gt;&gt; npu_out = model(cpu_x1.npu(), cpu_x2.npu(), scale.npu(), offset.npu(), bias.npu())
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_quant_matmul">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_quant_matmul</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_quant_matmul" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p><strong>功能描述</strong>:</p>
<p>完成量化的矩阵乘计算，最小支持输入维度为2维，最大支持输入维度为6维。</p>
<p><strong>接口原型</strong>:</p>
<p>npu_quant_matmul(Tensor x1, Tensor x2, Tensor scale, <a href="#id5"><span class="problematic" id="id6">*</span></a>，Tensor? offset=None, Tensor? pertoken_scale=None, Tensor? bias=None, ScalarType? output_dtype=None) -&gt; Tensor</p>
<p><strong>参数说明</strong>:</p>
<p>x1(计算输入)：Device侧的Tensor类型，数据类型支持INT8。数据格式支持ND，shape最少是2维，最多是6维。</p>
<p>x2(计算输入)：Device侧的Tensor类型，数据类型支持INT8。数据格式支持ND，shape最少是2维，最多是6维。</p>
<p>scale(计算输入)：Device侧的Tensor类型，数据类型支持FLOAT32, INT64, BFLOAT16。数据格式支持ND，shape是1维(t，)，t = 1或n，其中n与x2的n一致。如需传入INT64数据类型的scale,  需要提前调用torch_npu.npu_trans_quant_param接口来获取INT64数据类型的scale。</p>
<p>offset( 计算输入)：Device侧的Tensor类型，可选参数。数据类型支持FLOAT32，数据格式支持ND，shape是1维(t，)，t = 1或n，其中n与x2的n一致。</p>
<p>pertoken_scale(计算输入)：Device侧的Tensor类型，可选参数。数据类型支持FLOAT32，数据格式支持ND，shape是1维(m，)，其中m与x1的m一致。310P当前不支持pertoken_scale。</p>
<p>bias( 计算输入)：Device侧的Tensor类型，可选参数。数据类型支持INT32，BFLOAT16, 数据格式支持ND，shape支持1维(n，)或3维(batch,1,n)，n与x2的n一致。bias 3维(batch,1,n)只出现在out为3维的场景下，同时batch值需要等于x1, x2 boardcast后推导出的batch值。</p>
<p>output_dtype( 计算输入)：Device侧的ScalarType，可选参数。表示输出Tensor的数据类型，支持输入torch.int8，torch.float16, torch.bfloat16。默认值为None，代表输出Tensor数据类型为INT8。310P只支持output_dtype为torch.int8(含None, 下同)和torch.float16。</p>
<p><strong>输出说明</strong>:</p>
<p>一个Tensor类型的输出，代表量化matmul的计算结果。如果output_dtype为torch.float16，输出的数据类型为FLOAT16；如果output_dtype为torch.bfloat16，输出的数据类型为BFLOAT16；如果output_dtype为torch.int8或者None，输出的数据类型为INT8；如果output_dtype非以上数据类型，返回错误码。</p>
<p><strong>约束说明</strong>:</p>
<p>传入的x1、x2、scale不能是空。</p>
<p>x1、x2、bias、scale、offset、pertoken_scale、output_dtype的数据类型和数据格式需要在支持的范围之内。</p>
<p>x1、x2的shape需要在2-6维范围。</p>
<p>scale, offset的shape需要为1维(t，)，t = 1或n，n与x2的n一致。</p>
<p>pertoken_scale的shape需要为1维(m, )，m与x1的m一致，310P当前不支持pertoken_scale。</p>
<p>bias的shape支持1维(n，)或3维(batch,1,n)，n与x2的n一致, batch值需要等于x1, x2 boardcast后推导出的batch值。</p>
<p>bias的shape在out 是2,4,5,6维情况下需要为1维，在out 是3维情况下可以为1维或3维。</p>
<p>output_dtype为torch.bfloat16时，scale需要为BFLOAT16数据类型的Tensor。output_dtype为torch.float16或torch.int8时，scale在pertoken_scale为空时可为FLOAT32或INT64数据类型的Tensor。output_dtype为torch.float16时，scale在pertoken_scale不为空时必须为float32。</p>
<p>bias为BFLOAT16数据类型时，output_dtype需要为torch.bfloat16。</p>
<p>目前输出INT8/FLOAT16且无pertoken_scale情况下，图模式不支持scale直接传入FLOAT32数据类型。</p>
<p>pertoken_scale仅支持float32，目前仅在输出float16和bfloat16场景下可不为空。</p>
<p>offset不为空时，output_dtype仅支持int8。</p>
<p>x1与x2最后一维的shape大小不能超过65535</p>
<p>310P和Atlas A2芯片下，需要调用npu_format_cast完成输入x2(weight)高性能数据排布功能。310P需要将x2转置后调用npu_format_cast，Atlas A2需要将x2非转置后调用npu_format_cast。</p>
<p>支持的PyTorch版本:</p>
<p>PyTorch 2.2</p>
<p>PyTorch 2.1</p>
<p>PyTorch 2.0</p>
<p>PyTorch 1.11.0</p>
<p>支持的型号:</p>
<p>Atlas A2 训练系列产品</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; 1.单算子调用：
&gt;&gt;&gt; 在单算子模式下不支持使能高带宽的x2数据排布，如果想追求极致性能，请使用图模式
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_npu
&gt;&gt;&gt; import logging
&gt;&gt;&gt; import os
&gt;&gt;&gt;
&gt;&gt;&gt; cpu_x1 = torch.randint(-5, 5, (1, 256, 768), dtype=torch.int8)
&gt;&gt;&gt; cpu_x2 = torch.randint(-5, 5, (31, 768, 16), dtype=torch.int8)
&gt;&gt;&gt; scale = torch.randn(16, dtype=torch.float32)
&gt;&gt;&gt; offset = torch.randn(16, dtype=torch.float32)
&gt;&gt;&gt; bias = torch.randint(-5, 5, (31, 1, 16), dtype=torch.int32)
&gt;&gt;&gt; # Method 1: You can directly call npu_quant_matmul
&gt;&gt;&gt; npu_out = torch_npu.npu_quant_matmul(cpu_x1.npu(), cpu_x2.npu(), scale.npu(), offset=offset.npu(), bias=bias.npu())
&gt;&gt;&gt;
&gt;&gt;&gt; # Method 2: You can first call npu_trans_quant_param to convert scale and offset from float32 to int64 when output dtype is torch.int8 or torch.float16
&gt;&gt;&gt; scale_1 = torch_npu.npu_trans_quant_param(scale.npu(), offset.npu())
&gt;&gt;&gt; npu_out = torch_npu.npu_quant_matmul(cpu_x1.npu(), cpu_x2.npu(), scale_1, bias=bias.npu())
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; 2.图模式(输出int8/fp16且无pertoken情况下，必须先调用npu_trans_quant_param):
&gt;&gt;&gt; 2.1 通用
&gt;&gt;&gt; 2.1.1 示例一：输出float16
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_npu
&gt;&gt;&gt; import torchair as tng
&gt;&gt;&gt; from torchair.ge_concrete_graph import ge_apis as ge
&gt;&gt;&gt; from torchair.configs.compiler_config import CompilerConfig
&gt;&gt;&gt; import logging
&gt;&gt;&gt; from torchair.core.utils import logger
&gt;&gt;&gt; logger.setLevel(logging.DEBUG)
&gt;&gt;&gt; import os
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; # &quot;ENABLE_ACLNN&quot;是否使能走aclnn，true: 回调走aclnn，false: 在线编译
&gt;&gt;&gt; os.environ[&quot;ENABLE_ACLNN&quot;] = &quot;true&quot;
&gt;&gt;&gt; config = CompilerConfig()
&gt;&gt;&gt; npu_backend = tng.get_npu_backend(compiler_config=config)
&gt;&gt;&gt;
&gt;&gt;&gt; class MyModel(torch.nn.Module):
&gt;&gt;&gt; def __init__(self):
&gt;&gt;&gt; super().__init__()
&gt;&gt;&gt; def forward(self, x1, x2, scale, offset, bias):
&gt;&gt;&gt; return torch_npu.npu_quant_matmul(x1, x2, scale, offset=offset, bias=bias, output_dtype=torch.float16)
&gt;&gt;&gt; cpu_model = MyModel()
&gt;&gt;&gt; model = cpu_model.npu()
&gt;&gt;&gt; cpu_x1 = torch.randint(-1, 1, (15, 1, 512), dtype=torch.int8)
&gt;&gt;&gt; cpu_x2 = torch.randint(-1, 1, (15, 512, 128), dtype=torch.int8)
&gt;&gt;&gt; scale = torch.randn(1, dtype=torch.float32)
&gt;&gt;&gt; # pertoken_scale为空时，输出fp16必须先调用npu_trans_quant_param, 将scale(offset)从float转为int64.
&gt;&gt;&gt; scale_1 = torch_npu.npu_trans_quant_param(scale.npu(), None)
&gt;&gt;&gt; bias = torch.randint(-1, 1, (15, 1, 128), dtype=torch.int32)
&gt;&gt;&gt; # dynamic=True: 动态图模式，dynamic=False: 静态图模式
&gt;&gt;&gt; model = torch.compile(cpu_model, backend=npu_backend, dynamic=True)
&gt;&gt;&gt; npu_out = model(cpu_x1.npu(), cpu_x2.npu(), scale_1, None, bias.npu())
&gt;&gt;&gt;
&gt;&gt;&gt; 2.1.2 示例2：输出bfloat16
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_npu
&gt;&gt;&gt; import torchair as tng
&gt;&gt;&gt; from torchair.ge_concrete_graph import ge_apis as ge
&gt;&gt;&gt; from torchair.configs.compiler_config import CompilerConfig
&gt;&gt;&gt; import logging
&gt;&gt;&gt; from torchair.core.utils import logger
&gt;&gt;&gt; logger.setLevel(logging.DEBUG)
&gt;&gt;&gt; import os
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; os.environ[&quot;ENABLE_ACLNN&quot;] = &quot;true&quot;
&gt;&gt;&gt; config = CompilerConfig()
&gt;&gt;&gt; npu_backend = tng.get_npu_backend(compiler_config=config)
&gt;&gt;&gt;
&gt;&gt;&gt; class MyModel(torch.nn.Module):
&gt;&gt;&gt; def __init__(self):
&gt;&gt;&gt; super().__init__()
&gt;&gt;&gt; def forward(self, x1, x2, scale, offset, bias, pertoken_scale):
&gt;&gt;&gt; return torch_npu.npu_quant_matmul(x1, x2.t(), scale, offset=offset, bias=bias, pertoken_scale=pertoken_scale, output_dtype=torch.bfloat16)
&gt;&gt;&gt; cpu_model = MyModel()
&gt;&gt;&gt; model = cpu_model.npu()
&gt;&gt;&gt; m = 15
&gt;&gt;&gt; k = 11264
&gt;&gt;&gt; n = 6912
&gt;&gt;&gt; bias_flag = True
&gt;&gt;&gt; cpu_x1 = torch.randint(-1, 1, (m, k), dtype=torch.int8)
&gt;&gt;&gt; cpu_x2 = torch.randint(-1, 1, (n, k), dtype=torch.int8)
&gt;&gt;&gt; scale = torch.randint(-1, 1, (n,), dtype=torch.bfloat16)
&gt;&gt;&gt; pertoken_scale = torch.randint(-1, 1, (m,), dtype=torch.float32)
&gt;&gt;&gt;
&gt;&gt;&gt; bias = torch.randint(-1, 1, (n,), dtype=torch.bfloat16)
&gt;&gt;&gt; model = torch.compile(cpu_model, backend=npu_backend, dynamic=True)
&gt;&gt;&gt; if bias_flag:
&gt;&gt;&gt; npu_out = model(cpu_x1.npu(), cpu_x2.npu(), scale.npu(), None, None, pertoken_scale.npu())
&gt;&gt;&gt; else:
&gt;&gt;&gt; npu_out = model(cpu_x1.npu(), cpu_x2.npu(), scale.npu(), None, bias.npu(), pertoken_scale.npu())
&gt;&gt;&gt;
&gt;&gt;&gt; 2.2.1 310P 将x2转置(batch,n,k)后format
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_npu
&gt;&gt;&gt; import torchair as tng
&gt;&gt;&gt; from torchair.ge_concrete_graph import ge_apis as ge
&gt;&gt;&gt; from torchair.configs.compiler_config import CompilerConfig
&gt;&gt;&gt; import logging
&gt;&gt;&gt; from torchair.core.utils import logger
&gt;&gt;&gt; logger.setLevel(logging.DEBUG)
&gt;&gt;&gt; import os
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; os.environ[&quot;ENABLE_ACLNN&quot;] = &quot;true&quot;
&gt;&gt;&gt; config = CompilerConfig()
&gt;&gt;&gt; npu_backend = tng.get_npu_backend(compiler_config=config)
&gt;&gt;&gt;
&gt;&gt;&gt; class MyModel(torch.nn.Module):
&gt;&gt;&gt; def __init__(self):
&gt;&gt;&gt; super().__init__()
&gt;&gt;&gt; def forward(self, x1, x2, scale, offset, bias):
&gt;&gt;&gt; return torch_npu.npu_quant_matmul(x1, x2.transpose(2,1), scale, offset=offset, bias=bias)
&gt;&gt;&gt; cpu_model = MyModel()
&gt;&gt;&gt; model = cpu_model.npu()
&gt;&gt;&gt; cpu_x1 = torch.randint(-1, 1, (15, 1, 512), dtype=torch.int8).npu()
&gt;&gt;&gt; cpu_x2 = torch.randint(-1, 1, (15, 512, 128), dtype=torch.int8).npu()
&gt;&gt;&gt; # Process x2 into a high-bandwidth format(29) offline to improve performance, please ensure that the input is continuous with (batch,n,k) layout
&gt;&gt;&gt; cpu_x2_t_29 = torch_npu.npu_format_cast(cpu_x2.transpose(2,1).contiguous(), 29)
&gt;&gt;&gt; scale = torch.randn(1, dtype=torch.float32).npu()
&gt;&gt;&gt; offset = torch.randn(1, dtype=torch.float32).npu()
&gt;&gt;&gt; bias = torch.randint(-1,1, (128,), dtype=torch.int32).npu()
&gt;&gt;&gt; # Process scale from float32 to int64 offline to improve performance
&gt;&gt;&gt; scale_1 = torch_npu.npu_trans_quant_param(scale, offset)
&gt;&gt;&gt; model = torch.compile(cpu_model, backend=npu_backend, dynamic=False)
&gt;&gt;&gt; npu_out = model(cpu_x1, cpu_x2_t_29, scale_1, offset, bias)
&gt;&gt;&gt;
&gt;&gt;&gt; 2.2.2 Atlas A2将非转置(batch,k,n)后转format
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_npu
&gt;&gt;&gt; import torchair as tng
&gt;&gt;&gt; from torchair.ge_concrete_graph import ge_apis as ge
&gt;&gt;&gt; from torchair.configs.compiler_config import CompilerConfig
&gt;&gt;&gt; import logging
&gt;&gt;&gt; from torchair.core.utils import logger
&gt;&gt;&gt; logger.setLevel(logging.DEBUG)
&gt;&gt;&gt; import os
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; config = CompilerConfig()
&gt;&gt;&gt; npu_backend = tng.get_npu_backend(compiler_config=config)
&gt;&gt;&gt;
&gt;&gt;&gt; class MyModel(torch.nn.Module):
&gt;&gt;&gt; def __init__(self):
&gt;&gt;&gt; super().__init__()
&gt;&gt;&gt; def forward(self, x1, x2, scale, offset, bias, pertoken_scale):
&gt;&gt;&gt; return torch_npu.npu_quant_matmul(x1, x2, scale, offset=offset, bias=bias, pertoken_scale=pertoken_scale,output_dtype=torch.bfloat16)
&gt;&gt;&gt; cpu_model = MyModel()
&gt;&gt;&gt; model = cpu_model.npu()
&gt;&gt;&gt; m = 15
&gt;&gt;&gt; k = 11264
&gt;&gt;&gt; n = 6912
&gt;&gt;&gt; bias_flag = True
&gt;&gt;&gt; cpu_x1 = torch.randint(-1, 1, (m, k), dtype=torch.int8)
&gt;&gt;&gt; cpu_x2 = torch.randint(-1, 1, (n, k), dtype=torch.int8)
&gt;&gt;&gt; # Process x2 into a high-bandwidth format(29) offline to improve performance, please ensure that the input is continuous with (batch,k,n) layout
&gt;&gt;&gt; x2_notranspose_29 = torch_npu.npu_format_cast(cpu_x2.npu().transpose(1,0).contiguous(), 29)
&gt;&gt;&gt; scale = torch.randint(-1, 1, (n,), dtype=torch.bfloat16)
&gt;&gt;&gt; pertoken_scale = torch.randint(-1, 1, (m,), dtype=torch.float32)
&gt;&gt;&gt;
&gt;&gt;&gt; bias = torch.randint(-1,1, (n,), dtype=torch.bfloat16)
&gt;&gt;&gt; model = torch.compile(cpu_model, backend=npu_backend, dynamic=True)
&gt;&gt;&gt; if bias_flag:
&gt;&gt;&gt; npu_out = model(cpu_x1.npu(), x2_notranspose_29, scale.npu(), None, None, pertoken_scale.npu())
&gt;&gt;&gt; else:
&gt;&gt;&gt; npu_out = model(cpu_x1.npu(), x2_notranspose_29, scale.npu(), None, bias.npu(), pertoken_scale.npu())
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_weight_quant_batchmatmul">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_weight_quant_batchmatmul</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_weight_quant_batchmatmul" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p><strong>功能描述</strong>:</p>
<p>该接口用于实现矩阵乘计算中的weight输入和输出的量化操作，支持pertensor，perchannel，pergroup多场景量化(310P当前仅支持perchannel)。</p>
<p><strong>接口原型</strong>:</p>
<p>npu_weight_quant_batchmatmul(Tensor x, Tensor weight, Tensor antiquant_scale, Tensor? antiquant_offset=None, Tensor? quant_scale=None, Tensor? quant_offset=None, Tensor? bias=None, int antiquant_group_size=0) -&gt; Tensor</p>
<p><strong>参数说明</strong>:</p>
<p>x : Device侧Tensor类型，即矩阵乘中的x。数据格式支持ND，数据类型支持FLOAT16/BFLOAT16， 支持非连续的Tensor，支持输入维度为两维(M,K) ；310P上数据类型仅支持FLOAT16，支持输入维度为2-6维，支持batch轴但不支持broadcast。</p>
<p>weight：Device侧Tensor类型，即矩阵乘中的weight。数据格式支持ND，数据类型支持INT8， 支持非连续的Tensor，支持输入维度为两维(K,N)；310P上数据类型仅支持FLOAT16，支持输入维度为2-6维，支持batch轴但不支持broadcast，维度需与x保持一致。</p>
<p>antiquantscale：Device侧Tensor类型，反量化的scale，用于weight矩阵反量化 。数据格式支持ND，数据类型支持FLOAT16/BFLOAT16，支持非连续的Tensor，支持输入维度为两维(1, N)或 一维(N, )、(1, )；310P上数据类型仅支持FLOAT16。</p>
<p>antiquantoffset：Device侧Tensor类型，反量化的offset，用于weight矩阵反量化 。数据格式支持ND，数据类型支持FLOAT16/BFLOAT16，支持非连续的Tensor，支持输入维度为两维(1, N)或 一维(N, )、(1, )；310P上数据类型仅支持FLOAT16。</p>
<p>quantscale：Device侧Tensor类型，量化的scale，用于输出矩阵的量化 。数据格式支持ND，数据类型支持FLOAT32/INT64，支持输入维度为两维(1, N) 或 一维(N, )、(1, )；310P暂未使用此参数。</p>
<p>quantoffset: Device侧Tensor类型，量化的offset，用于输出矩阵的量化 。数据格式支持ND，数据类型支持FLOAT32，支持输入维度为两维(1, N) 或 一维(N, )、(1, )；310P暂未使用此参数。</p>
<p>bias：Device侧Tensor类型， 即矩阵乘中的bias，数据格式支持ND，数据类型支持FLOAT16/FLOAT32， 支持非连续的Tensor，支持输入维度为两维(1, N) 或 一维(N, )、(1, )。</p>
<p>antiquant_group_size：int类型， 用于控制pergroup场景下的group大小，当前默认为0，预留参数，暂未使用。</p>
<p><strong>输出说明</strong>:</p>
<p>输出为Tensor类型，代表计算结果。当输入存在quantscale时输出数据类型为INT8，当输入不存quant_sclae时输出数据类型和输入x一致。</p>
<p><strong>约束说明</strong>:</p>
<p>x和weight必须为(M,K)和(K,N)格式，M、K、N的范围为[1, 65535]；310P无此约束。</p>
<p>不支持空Tensor输入。</p>
<p>antiquantscale和antiquantoffset的输入shape要保持一致。</p>
<p>quantscale和quantoffset的输入shape要保持一致。</p>
<p>quantoffset不能独立于quantscale存在。</p>
<p>当x输入类型为BFLOAT16类型时候，bias的输入类型为FLOAT32；当x输入类型为FLOAT16类型时候，bias的输入类型为FLOAT16。</p>
<p>如需传入INT64数据类型的quantscale,  需要提前调用torch_npu.npu_trans_quant_param接口将数据类型为FLOAT32的quantscale和quantoffset转换为数据类型为INT64的quantscale输入。</p>
<p>支持的PyTorch版本:</p>
<p>PyTorch 2.0</p>
<p>PyTorch 2.1</p>
<p>PyTorch 1.11.0</p>
<p>支持的芯片型号:</p>
<p>Atlas A2 训练系列产品</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; 单算子模式：
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_npu
&gt;&gt;&gt;
&gt;&gt;&gt; cpu_x = torch.randn((8192, 320),device=&#39;npu&#39;,dtype=torch.bfloat16)
&gt;&gt;&gt; cpu_weight = torch.randn((320, 256),device=&#39;npu&#39;,dtype=torch.int8)
&gt;&gt;&gt; cpu_antiquantscale = torch.randn((1, 256),device=&#39;npu&#39;,dtype=torch.bfloat16)
&gt;&gt;&gt; cpu_antiquantoffset = torch.randn((1, 256),device=&#39;npu&#39;,dtype=torch.bfloat16)
&gt;&gt;&gt; npu_out = torch_npu.npu_weight_quant_batchmatmul(cpu_x.npu(), cpu_weight.npu(), cpu_antiquantscale.npu(), cpu_antiquantoffset.npu())
&gt;&gt;&gt;
&gt;&gt;&gt; 图模式：
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_npu
&gt;&gt;&gt; import  torchair as tng
&gt;&gt;&gt; from torchair.configs.compiler_config import CompilerConfig
&gt;&gt;&gt; config = CompilerConfig()
&gt;&gt;&gt; config.debug.graph_dump.type = &quot;pbtxt&quot;
&gt;&gt;&gt; npu_backend = tng.get_npu_backend(compiler_config=config)
&gt;&gt;&gt;
&gt;&gt;&gt; cpu_x = torch.randn((8192, 320),device=&#39;npu&#39;,dtype=torch.bfloat16)
&gt;&gt;&gt; cpu_weight = torch.randn((320, 256),device=&#39;npu&#39;,dtype=torch.int8)
&gt;&gt;&gt; cpu_antiquantscale = torch.randn((1, 256),device=&#39;npu&#39;,dtype=torch.bfloat16)
&gt;&gt;&gt; cpu_antiquantoffset = torch.randn((1, 256),device=&#39;npu&#39;,dtype=torch.bfloat16)
&gt;&gt;&gt;
&gt;&gt;&gt; class MyModel(torch.nn.Module):
&gt;&gt;&gt; def __init__(self):
&gt;&gt;&gt; super().__init__()
&gt;&gt;&gt;
&gt;&gt;&gt; def forward(self, x, weight, antiquant_scale, antiquant_offset, quant_scale,quant_offset, bias, antiquant_group_size):
&gt;&gt;&gt; return torch_npu.npu_weight_quant_batchmatmul(x, weight, antiquant_scale, antiquant_offset, quant_scale ,quant_offset, bias, antiquant_group_size)
&gt;&gt;&gt;
&gt;&gt;&gt; cpu_model = MyModel()
&gt;&gt;&gt; model = cpu_model.npu()
&gt;&gt;&gt; model = torch.compile(cpu_model, backend=npu_backend, dynamic=True)npu_out = model(cpu_x.npu(), cpu_weight.npu(), cpu_antiquantscale.npu(), cpu_antiquantoffset.npu(), None, None, None, 0)
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_convert_weight_to_int4pack">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_convert_weight_to_int4pack</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_convert_weight_to_int4pack" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p><strong>功能描述</strong>:</p>
<p>该接口将int32的输入tensor打包为int4存放，每8个int4数据通过一个int32数据承载，并进行交叠排放。</p>
<p><strong>接口原型</strong>:</p>
<p>npu_convert_weight_to_int4pack(Tensor weight, int inner_k_tiles=0) -&gt; Tensor</p>
<p><strong>参数说明</strong>:</p>
<p>weight : Device侧Tensor类型，输入的weight。数据格式支持ND，数据类型支持INT32， 不支持非连续的Tensor。维度支持2维，shape支持（k, n）, (n, k)。最后一维度需要8个元素对齐。</p>
<p>inner_k_tiles：int类型，用于制定内部打包格式中，多少个K-tiles被打包在一起，默认值为0. 预留参数，暂未使用。</p>
<p><strong>输出说明</strong>:</p>
<p>输出为Tensor类型，代表int4打包后的输出。数据类型为INT32，shape为（k, n/8）, (n, k/8), 数据格式支持ND。</p>
<p><strong>约束说明</strong>:</p>
<p>输入weight中的元素的值需要在int4的表示范围内，即[-8, 7]。</p>
<p>支持的PyTorch版本:</p>
<p>PyTorch 2.3.1</p>
<p>PyTorch 2.0</p>
<p>PyTorch 2.1</p>
<p>PyTorch 2.2</p>
<p>PyTorch 1.11</p>
<p>支持的芯片型号:</p>
<p>Atlas A2 训练系列产品</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; 单算子模式：
&gt;&gt;&gt;
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_npu
&gt;&gt;&gt;
&gt;&gt;&gt; m = 128
&gt;&gt;&gt; k = 64
&gt;&gt;&gt; n = 32
&gt;&gt;&gt; trans_weight = False
&gt;&gt;&gt;
&gt;&gt;&gt; cpu_x = torch.randn((m, k), dtype=torch.float16)
&gt;&gt;&gt; if trans_weight:
&gt;&gt;&gt; cpu_weight = torch.randint(low=-8, high=8, size=(n, k), dtype=torch.int32)
&gt;&gt;&gt; cpu_antiquantscale = torch.randn((n, 1), dtype=torch.float16)
&gt;&gt;&gt; cpu_antiquantoffset = torch.randn((n, 1), dtype=torch.float16)
&gt;&gt;&gt; else:
&gt;&gt;&gt; cpu_weight = torch.randint(low=-8, high=8, size=(k, n), dtype=torch.int32)
&gt;&gt;&gt; cpu_antiquantscale = torch.randn((1, n), dtype=torch.float16)
&gt;&gt;&gt; cpu_antiquantoffset = torch.randn((1, n), dtype=torch.float16)
&gt;&gt;&gt;
&gt;&gt;&gt; weight_int4 = torch_npu.npu_convert_weight_to_int4pack(cpu_weight.npu())
&gt;&gt;&gt;
&gt;&gt;&gt; if trans_weight:
&gt;&gt;&gt; cpu_weight = cpu_weight.transpose(-1, -2)
&gt;&gt;&gt; weight_int4 = weight_int4.transpose(-1, -2)
&gt;&gt;&gt; cpu_antiquantscale = cpu_antiquantscale.transpose(-1, -2)
&gt;&gt;&gt; cpu_antiquantoffset = cpu_antiquantoffset.transpose(-1, -2)
&gt;&gt;&gt;
&gt;&gt;&gt; npu_out = torch_npu.npu_weight_quant_batchmatmul(cpu_x.npu(), weight_int4.npu(), cpu_antiquantscale.npu(), cpu_antiquantoffset.npu())
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_grouped_matmul">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_grouped_matmul</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_grouped_matmul" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p><strong>功能描述</strong>:</p>
<p>GroupedMatmul算子可以实现分组矩阵乘计算，每组矩阵乘的维度大小可以不同，是一种灵活的支持方式。其主要输入与输出均为TensorList，其中输入数据x与输出结果y均支持切分及不切分的模式，根据参数split_item来确定x与y是否需要切分，在x需要切分的情况下使用参数group_list来描述对x的m轴进行切分的方式。</p>
<p>根据输入x、输入weight与输出y的Tensor数量不同，可以支持如下4种场景：</p>
<p>x、weight、y都为多Tensor，即每组的数据对应的Tensor是独立的。</p>
<p>x为单Tensor，weight/y为多Tensor，此时需要通过可选参数group_list说明x在行上的分组情况，如group_list[0]=10说明x的前10行参与第一组矩阵乘计算。</p>
<p>x、weight为多Tensor，y为单Tensor，此时每组矩阵乘的结果放在同一个Tensor中连续存放。</p>
<p>x、y为单Tensor，weight为多Tensor，属于前两种情况的组合。</p>
<p>计算公式为：</p>
<p>非量化场景：</p>
<p>y_i = x_i * weight_i + bias_i</p>
<p>量化场景：</p>
<p>y_i = (x_i * weight_i + bias_i) * scale_i + offset_i</p>
<p>反量化场景：</p>
<p>y_i = (x_i * weight_i + bias_i) * scale_i</p>
<p>伪量化场景：</p>
<p>y_i = x_i * (weight_i + antiquant_offset_i) * antiquant_scale_i + bias_i</p>
<p><strong>接口原型</strong>:</p>
<p>PyTorch 2.1及更高的版本中：</p>
<p>npu_grouped_matmul(Tensor[] x, Tensor[] weight, <a href="#id7"><span class="problematic" id="id8">*</span></a>, Tensor[]? bias=None, Tensor[]? scale=None, Tensor[]? offset=None, Tensor[]? antiquant_scale=None, Tensor[]? antiquant_offset=None, int[]? group_list=None, int? split_item=0, ScalarType? output_dtype=None) -&gt; Tensor[]</p>
<p>PyTorch 1.11与2.0版本：</p>
<p>npu_grouped_matmul(Tensor[] x, Tensor[] weight, <a href="#id9"><span class="problematic" id="id10">*</span></a>, Tensor[] bias, Tensor[] scale, Tensor[] offset, Tensor[] antiquant_scale, Tensor[] antiquant_offset, int[]? group_list=None, int? split_item=0, ScalarType? output_dtype=None) -&gt; Tensor[]</p>
<p><strong>参数说明</strong>:</p>
<ul class="simple">
<li><p>x：必选参数，Device侧的TensorList，即输入参数中的x，数据类型支持FLOAT16、BFLOAT16、INT8；数据格式支持ND，支持的最大长度为128个，其中每个Tensor在split_item=0的模式下支持输入2至6维，其余模式下支持输入为2维。</p></li>
<li><p>weight：必选参数，Device侧的TensorList，即输入参数中matmul的weight输入，数据类型支持FLOAT16、BFLOAT16、INT8；数据格式支持ND，支持的最大长度为128个，其中每个Tensor支持输入为2维。</p></li>
<li><p>bias：在PyTorch 1.11与2.0版本中是必选参数，在PyTorch 2.1与更高的版本中是可选参数，Device侧的TensorList，即输入参数中matmul的bias输入，数据类型支持FLOAT16、FLOAT32、INT32；数据格式支持ND，支持的最大长度为128个，其中每个Tensor支持输入为1维。</p></li>
<li><p>scale：可选参数，Device侧的TensorList，代表量化参数中的缩放因子，数据类型支持INT64，数据格式支持ND，长度与weight相同。</p></li>
<li><p>offset：可选参数，Device侧的TensorList，代表量化参数中的偏移量，数据类型支持FLOAT32，数据格式支持ND，长度与weight相同。</p></li>
<li><p>antiquant_scale：可选参数，Device侧的TensorList，代表伪量化参数中的缩放因子，数据类型支持FLOAT16、BFLOAT16，数据格式支持ND，长度与weight相同。</p></li>
<li><p>antiquant_offset：可选参数，Device侧的TensorList，代表伪量化参数中的偏移量，数据类型支持FLOAT16、BFLOAT16，数据格式支持ND，长度与weight相同。</p></li>
</ul>
<p><strong>输出说明</strong>:</p>
<p>Device侧的TensorList类型输出，代表GroupedMatmul的计算结果，当split_item取0或1时，其Tensor个数与weight相同，当split_item取2或3时，其Tensor个数为1。</p>
<p><strong>约束说明</strong>:</p>
<ol class="arabic simple">
<li><p>若x为多Tensor，group_list可以为空；当x为单Tensor，group_list的长度与weight的Tensor个数相同。</p></li>
<li><p>若bias不为空，其Tensor数量须与weight保持一致。</p></li>
<li><p>记一个matmul计算涉及的x、weight与y的维度分别为(m×k)、(k×n)和(m×n)，则每一个matmul的输入与输出须满足[m, k]和[k, n]的k维度相等关系。</p></li>
<li><p>非量化场景支持的输入类型为：</p></li>
</ol>
<ul class="simple">
<li><p>x为FLOAT16、weight为FLOAT16、bias为FLOAT16、scale为空、offset为空、antiquant_scale为空、antiquant_offset为空、output_dtype为FLOAT16；</p></li>
<li><p>x为BFLOAT16、weight为BFLOAT16、bias为FLOAT32、scale为空、offset为空、antiquant_scale为空、antiquant_offset为空、output_dtype为BFLOAT16；</p></li>
</ul>
<ol class="arabic simple" start="5">
<li><p>量化场景支持的输入类型为：x为INT8、weight为INT8、bias为INT32、scale为UINT64、offset为空、antiquant_scale为空、antiquant_offset为空、output_dtype为INT8；</p></li>
<li><p>伪量化场景支持的输入类型为：</p></li>
</ol>
<ul class="simple">
<li><p>x为FLOAT16、weight为INT8、bias为FLOAT16、scale为空，offset为空，antiquant_scale为FLOAT16、antiquant_offset为FLOAT16、output_dtype为FLOAT16；</p></li>
<li><p>x为BFLOAT16、weight为INT8、bias为FLOAT32、scale为空，offset为空，antiquant_scale为BFLOAT16、antiquant_offset为BFLOAT16、output_dtype为BFLOAT16；</p></li>
</ul>
<ol class="arabic simple" start="7">
<li><p>对于实际无bias的场景，在PyTorch 1.11与2.0版本中，须手动指定“bias=[]”；在PyTorch 2.1及更高的版本中，可以直接不指定bias参数。scale、offset、antiquantScale、antiquantOffset四个参数在不同PyTorch版本中的约束与bias相同。</p></li>
</ol>
<p>output_dtype的数据类型当前只支持None，或者与输入x的数据类型相同。</p>
<p>支持的PyTorch版本:</p>
<p>PyTorch 2.3</p>
<p>PyTorch 2.2</p>
<p>PyTorch 2.1</p>
<p>PyTorch 2.0</p>
<p>PyTorch 1.11</p>
<p>支持的型号:</p>
<p>Atlas A2 训练系列产品</p>
<p>Atlas A3 训练系列产品</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># 单算子调用模式，Torch1.11、Torch2.0版本</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="p">[</span><span class="n">weight1</span><span class="p">,</span> <span class="n">weight2</span><span class="p">,</span> <span class="n">weight3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias</span> <span class="o">=</span> <span class="p">[</span><span class="n">bias1</span><span class="p">,</span> <span class="n">bias2</span><span class="p">,</span> <span class="n">bias3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">group_list</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">split_item</span> <span class="o">=</span> <span class="mi">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npu_out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_grouped_matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="p">[],</span> <span class="n">offset</span><span class="o">=</span><span class="p">[],</span> <span class="n">antiquant_scale</span><span class="o">=</span><span class="p">[],</span> <span class="n">antiquant_offset</span><span class="o">=</span><span class="p">[],</span> <span class="n">group_list</span><span class="o">=</span><span class="n">group_list</span><span class="p">,</span> <span class="n">split_item</span><span class="o">=</span><span class="n">split_item</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 单算子调用模式，Torch2.1及更高的版本</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="p">[</span><span class="n">weight1</span><span class="p">,</span> <span class="n">weight2</span><span class="p">,</span> <span class="n">weight3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias</span> <span class="o">=</span> <span class="p">[</span><span class="n">bias1</span><span class="p">,</span> <span class="n">bias2</span><span class="p">,</span> <span class="n">bias3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">group_list</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">split_item</span> <span class="o">=</span> <span class="mi">0</span><span class="n">npu_out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_grouped_matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">group_list</span><span class="o">=</span><span class="n">group_list</span><span class="p">,</span> <span class="n">split_item</span><span class="o">=</span><span class="n">split_item</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 图模式调用，Torch2.1及更高的版本</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torchair</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tng</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.configs.compiler_config</span><span class="w"> </span><span class="kn">import</span> <span class="n">CompilerConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">CompilerConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npu_backend</span> <span class="o">=</span> <span class="n">tng</span><span class="o">.</span><span class="n">get_npu_backend</span><span class="p">(</span><span class="n">compiler_config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">GMMModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">return</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_grouped_matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">main</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="p">[</span><span class="n">weight1</span><span class="p">,</span> <span class="n">weight2</span><span class="p">,</span> <span class="n">weight3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">GMMModel</span><span class="p">()</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">npu_backend</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">custom_output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_quant_scatter">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_quant_scatter</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_quant_scatter" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p><strong>功能描述</strong>:</p>
<p>先将updates进行量化，然后将updates中的值按指定的轴axis和索引indices更新self中的值，并将结果保存到输出tensor，self本身的数据不变。</p>
<p><strong>接口原型</strong>:</p>
<p>torch_npu.npu_quant_scatter(Tensor self, Tensor indices, Tensor updates, Tensor quant_scales, Tensor? quant_zero_points=None, int axis=0, int quant_axis=1, str reduce='update') -&gt; Tensor</p>
<p><strong>参数说明</strong>:</p>
<p>self：Device侧的Tensor类型，必选输入，源数据张量，数据类型支持INT8，数据格式支持ND，支持非连续的Tensor。</p>
<p>indices：Device侧的Tensor类型，必选输入，索引张量，数据类型支持INT32，数据格式支持ND，支持非连续的Tensor。</p>
<p>updates：Device侧的Tensor类型，必选输入，更新数据张量，数据类型支持BFLOAT16(仅Atlas A2 训练系列产品支持)，数据格式支持ND，支持非连续的Tensor。</p>
<p>quant_scales：Device侧的Tensor类型，必选输入，量化缩放张量，数据类型支持BFLOAT16(仅Atlas A2 训练系列产品支持)，数据格式支持ND，支持非连续的Tensor。</p>
<p>quant_zero_points：Device侧的Tensor类型，可选输入，量化偏移张量，数据类型支持BFLOAT16(仅Atlas A2 训练系列产品支持)，数据格式支持ND，支持非连续的Tensor。</p>
<p>axis：Host侧的int类型，可选参数，updates上用来更新的轴。</p>
<p>quant_axis：Host侧的int类型，可选参数，updates上用来量化的轴。</p>
<p>reduce：Host侧的str类型，可选参数，表示数据操作方式。</p>
<p><strong>输出说明</strong>:</p>
<p>一个Tensor类型的输出，代表self被更新后的结果。</p>
<p><strong>约束说明</strong>:</p>
<p>self的维数只能是3~8维。</p>
<p>indices的维数只能是1维或者2维；如果是2维，其第2维的大小必须是2；不支持索引越界，索引越界不校验；indices映射的self数据段不能重合，若重合则会因为多核并发原因导致多次执行结果不一样。</p>
<p>updates的维数需要与self的维数一样；其第1维的大小等于indices的第1维的大小，且不大于self的第1维的大小；其axis轴的大小不大于self的axis轴的大小；其余维度的大小要跟self对应维度的大小相等；其最后一维的大小必须32B对齐。</p>
<p>quant_scales的元素个数需要等于updates在quant_axis轴的大小。</p>
<p>quant_zero_points的元素个数需要等于updates在quant_axis轴的大小。</p>
<p>axis不能为updates的第1维或最后1维。</p>
<p>quant_axis只能为updates的最后1维。</p>
<p>reduce当前只支持‘update’，即更新操作。</p>
<p>支持的PyTorch版本:</p>
<p>PyTorch 2.3</p>
<p>PyTorch 2.2</p>
<p>PyTorch 2.1</p>
<p>PyTorch 1.11</p>
<p>支持的型号:</p>
<p>Atlas A2 训练系列产品</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">4096</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_var</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_indices</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_updates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updates</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_updates</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_quant_scales</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quant_scales</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_quant_scales</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_quant_zero_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quant_zero_points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_quant_zero_points</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quant_axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reduce</span> <span class="o">=</span> <span class="s2">&quot;update&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_quant_scatter</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">quant_scales</span><span class="p">,</span> <span class="n">quant_zero_points</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">quant_axis</span><span class="o">=</span><span class="n">quant_axis</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="n">reduce</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_quant_scatter_">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_quant_scatter_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_quant_scatter_" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p><a href="#id11"><span class="problematic" id="id12">**</span></a>功能描述**先将:</p>
<p>updates进行量化，然后将updates中的值按指定的轴axis和索引indices更新self中的值，self中的数据被改变。</p>
<p><strong>接口原型</strong>:</p>
<p>torch_npu.npu_quant_scatter_(Tensor(a!) self, Tensor indices, Tensor updates, Tensor quant_scales, Tensor? quant_zero_points=None, int axis=0, int quant_axis=1, str reduce='update') -&gt; Tensor(a!)</p>
<p><strong>参数说明</strong>:</p>
<p>self：Device侧的Tensor类型，必选输入，源数据张量，数据类型支持INT8，数据格式支持ND，支持非连续的Tensor。</p>
<p>indices：Device侧的Tensor类型，必选输入，索引张量，数据类型支持INT32，数据格式支持ND，支持非连续的Tensor。</p>
<p>updates：Device侧的Tensor类型，必选输入，更新数据张量，数据类型支持BFLOAT16(仅Atlas A2 训练系列产品支持)，数据格式支持ND，支持非连续的Tensor。</p>
<p>quant_scales：Device侧的Tensor类型，必选输入，量化缩放张量，数据类型支持BFLOAT16(仅Atlas A2 训练系列产品支持)，数据格式支持ND，支持非连续的Tensor。</p>
<p>quant_zero_points：Device侧的Tensor类型，可选输入，量化偏移张量，数据类型支持BFLOAT16(仅Atlas A2 训练系列产品支持)，数据格式支持ND，支持非连续的Tensor。</p>
<p>axis：Host侧的int类型，可选参数，updates上用来更新的轴。</p>
<p>quant_axis：Host侧的int类型，可选参数，updates上用来量化的轴。</p>
<p>reduce：Host侧的str类型，可选参数，表示数据操作方式。</p>
<p><strong>输出说明</strong>:</p>
<p>返回被更新后的self。</p>
<p><strong>约束说明</strong>:</p>
<p>self的维数只能是3~8维。</p>
<p>indices的维数只能是1维或者2维；如果是2维，其第2维的大小必须是2；不支持索引越界，索引越界不校验；indices映射的self数据段不能重合，若重合则会因为多核并发原因导致多次执行结果不一样。</p>
<p>updates的维数需要与self的维数一样；其第1维的大小等于indices的第1维的大小，且不大于self的第1维的大小；其axis轴的大小不大于self的axis轴的大小；其余维度的大小要跟self对应维度的大小相等；其最后一维的大小必须32B对齐。</p>
<p>quant_scales的元素个数需要等于updates在quant_axis轴的大小。</p>
<p>quant_zero_points的元素个数需要等于updates在quant_axis轴的大小。</p>
<p>axis不能为updates的第1维或最后1维。</p>
<p>quant_axis只能为updates的最后1维。</p>
<p>reduce当前只支持‘update’，即更新操作。</p>
<p>支持的PyTorch版本:</p>
<p>PyTorch 2.3</p>
<p>PyTorch 2.2</p>
<p>PyTorch 2.1</p>
<p>PyTorch 1.11</p>
<p>支持的型号:</p>
<p>Atlas A2 训练系列产品</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">4096</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_var</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_indices</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_updates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updates</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_updates</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_quant_scales</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quant_scales</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_quant_scales</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_quant_zero_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quant_zero_points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_quant_zero_points</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quant_axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reduce</span> <span class="o">=</span> <span class="s2">&quot;update&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_quant_scatter_</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">quant_scales</span><span class="p">,</span> <span class="n">quant_zero_points</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">quant_axis</span><span class="o">=</span><span class="n">quant_axis</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="n">reduce</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_scatter_nd_update">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_scatter_nd_update</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_scatter_nd_update" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p><strong>功能描述</strong>:</p>
<p>将updates中的值按指定的索引indices更新self中的值，并将结果保存到输出tensor，self本身的数据不变。</p>
<p><strong>接口原型</strong>:</p>
<p>torch_npu.npu_scatter_nd_update(Tensor self, Tensor indices, Tensor updates) -&gt; Tensor</p>
<p><strong>参数说明</strong>:</p>
<p>self：Device侧的Tensor类型，必选输入，源数据张量，数据类型支持FLOAT32、FLOAT16、BOOL、BFLOAT16(仅Atlas A2 训练系列产品支持)、INT64(仅Atlas A2 训练系列产品支持)，数据格式支持ND，支持非连续的Tensor，数据类型需要与updates一致，维数只能是1~8维。</p>
<p>indices：Device侧的Tensor类型，必选输入，索引张量，数据类型支持INT32、INT64，数据格式支持ND，支持非连续的Tensor，indices中的索引数据不支持越界。</p>
<p>updates：Device侧的Tensor类型，必选输入，更新数据张量，数据类型支持FLOAT32、FLOAT16、BOOL、BFLOAT16(仅Atlas A2 训练系列产品支持)、INT64(仅Atlas A2 训练系列产品支持)，数据格式支持ND，支持非连续的Tensor，数据类型需要与self一致。</p>
<p><strong>输出说明</strong>:</p>
<p>一个Tensor类型的输出，代表self被更新后的结果。</p>
<p><a href="#id13"><span class="problematic" id="id14">**</span></a>约束说明**indices至少是2维，其最后1维的大小不能超过self的维度大小。</p>
<p>假设indices最后1维的大小是a，则updates的shape等于indices除最后1维外的shape加上self除前a维外的shape。举例：self的shape是(4, 5, 6)，indices的shape是(3, 2)，则updates的shape必须是(3, 6)。</p>
<p>支持的PyTorch版本:</p>
<p>PyTorch 2.3</p>
<p>PyTorch 2.2</p>
<p>PyTorch 2.1</p>
<p>PyTorch 1.11</p>
<p>支持的型号:</p>
<p>Atlas A2 训练系列产品</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">4096</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_var</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_indices</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_updates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updates</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_updates</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_scatter_nd_update</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_scatter_nd_update_">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_scatter_nd_update_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_scatter_nd_update_" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p><strong>功能描述</strong>:</p>
<p>将updates中的值按指定的索引indices更新self中的值，并将结果保存到输出tensor，self中的数据被改变。</p>
<p><strong>接口原型</strong>:</p>
<p>torch_npu.npu_scatter_nd_update_(Tensor(a!) self, Tensor indices, Tensor updates) -&gt; Tensor(a!)</p>
<p><strong>参数说明</strong>:</p>
<p>self：Device侧的Tensor类型，必选输入，源数据张量，数据类型支持FLOAT32、FLOAT16、BOOL、BFLOAT16(仅Atlas A2 训练系列产品支持)、INT64(仅Atlas A2 训练系列产品支持)，数据格式支持ND，支持非连续的Tensor，数据类型需要与updates一致，维数只能是1~8维。</p>
<p>indices：Device侧的Tensor类型，必选输入，索引张量，数据类型支持INT32、INT64，数据格式支持ND，支持非连续的Tensor，indices中的索引数据不支持越界。</p>
<p>updates：Device侧的Tensor类型，必选输入，更新数据张量，数据类型支持FLOAT32、FLOAT16、BOOL、BFLOAT16(仅Atlas A2 训练系列产品支持)、INT64(仅Atlas A2 训练系列产品支持)，数据格式支持ND，支持非连续的Tensor，数据类型需要与self一致。</p>
<p><strong>输出说明</strong>:</p>
<p>返回被更新后的self。</p>
<p><strong>约束说明</strong>:</p>
<p>indices至少是2维，其最后1维的大小不能超过self的维度大小。</p>
<p>假设indices最后1维的大小是a，则updates的shape等于indices除最后1维外的shape加上self除前a维外的shape。举例：self的shape是(4, 5, 6)，indices的shape是(3, 2)，则updates的shape必须是(3, 6)。</p>
<p>支持的PyTorch版本:</p>
<p>PyTorch 2.3</p>
<p>PyTorch 2.2</p>
<p>PyTorch 2.1</p>
<p>PyTorch 1.11</p>
<p>支持的型号:</p>
<p>Atlas A2 训练系列产品</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">4096</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_var</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_indices</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_updates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updates</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data_updates</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_scatter_nd_update_</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_anti_quant">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_anti_quant</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_anti_quant" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p><strong>功能描述</strong>:</p>
<p>将INT4或者INT8数据反量化为FP16或者BF16，其中输入是INT4类型时，将每8个数据看作是一个INT32数据。</p>
<p>计算公式为：</p>
<p>anti_quant(x)=float16((x+offset)*scale)</p>
<p>anti_quant(x)=bfloat16((x+offset)*scale)</p>
<p><strong>接口原型</strong>:</p>
<p>npu_anti_quant(Tensor x, Tensor scale, <a href="#id15"><span class="problematic" id="id16">*</span></a>, Tensor? offset=None, ScalarType? dst_dtype=None, ScalarType? src_dtype=None) -&gt; Tensor</p>
<p><strong>参数说明</strong>:</p>
<p>x：Tensor类型，即输入参数中的x。数据类型支持INT8、INT32(仅Atlas A2 训练系列产品支持)，其中INT32类型数据的每个值是由8个INT4数值拼成的。数据格式支持ND，支持非连续的Tensor。输入最大支持8维。</p>
<p>scale：Tensor类型，数据类型支持FLOAT32、BFLOAT16(仅Atlas A2 训练系列产品支持)，数据格式支持ND，支持非连续的Tensor，仅支持1维Tensor。</p>
<p>offset：Tensor类型，可选参数，数据类型支持FLOAT32、BFLOAT16(仅Atlas A2 训练系列产品支持)，且数据类型必须与scale的数据类型一致。数据格式支持ND，支持非连续的Tensor，仅支持1维Tensor，且shape必须与scale的shape大小一致。</p>
<p>dst_dtype：ScalarType类型，可选参数，输入值允许为torch.float16、torch.bfloat16(仅Atlas A2 训练系列产品支持)，默认值为torch.float16。</p>
<p>src_dtype：ScalarType类型，可选参数，输入值允许为torch.quint4x2(仅Atlas A2 训练系列产品支持)、torch.int8，默认值为torch.int8。</p>
<p><strong>输出说明</strong>:</p>
<p>一个Tensor类型的输出，代表antiquant的计算结果。</p>
<p><strong>约束说明</strong>:</p>
<p>x、scale这两个输入中不能含有空指针。</p>
<p>如果输入scale的shape值不为1，则输入x的最后一维shape值必须与scale的shape一致。</p>
<p>支持的PyTorch版本:</p>
<p>PyTorch 2.1</p>
<p>PyTorch 2.2</p>
<p>PyTorch 2.3</p>
<p>PyTorch 2.4</p>
<p>支持的型号:</p>
<p>Atlas A2训练系列产品</p>
<p>Atlas 推理系列产品</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#单算子调用模式</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">=</span><span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_anti_quant</span><span class="p">(</span><span class="n">x_tensor</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="n">offset</span><span class="p">,</span> <span class="n">dst_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#torch api入图模式</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torchair</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tng</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.ge_concrete_graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">ge_apis</span> <span class="k">as</span> <span class="n">ge</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.configs.compiler_config</span><span class="w"> </span><span class="kn">import</span> <span class="n">CompilerConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">CompilerConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">debug</span><span class="o">.</span><span class="n">graph_dump</span><span class="o">.</span><span class="n">type</span> <span class="o">=</span> <span class="s1">&#39;pbtxt&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npu_backend</span> <span class="o">=</span> <span class="n">tng</span><span class="o">.</span><span class="n">get_npu_backend</span><span class="p">(</span><span class="n">compiler_config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">scale</span><span class="p">,</span><span class="n">offset</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">return</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_anti_quant</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="n">offset</span><span class="p">,</span> <span class="n">dst_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">cpu_model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">npu_backend</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_tensor</span><span class="p">,</span><span class="n">scale</span><span class="p">,</span><span class="n">offset</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_mm_all_reduce_base">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_mm_all_reduce_base</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_mm_all_reduce_base" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p><strong>功能描述</strong>:</p>
<p>TP切分场景下, 实现mm和all_reduce的融合, 融合算子内部实现计算和通信流水并行。</p>
<p><strong>接口原型</strong>:</p>
<p>npu_mm_all_reduce_base(Tensor x1, Tensor x2, str hcom, <a href="#id17"><span class="problematic" id="id18">*</span></a>, str reduce_op='sum', Tensor? bias=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? x3=None, Tensor? dequant_scale=None, Tensor? pertoken_scale=None, int comm_turn=0, int antiquant_group_size=0) -&gt; Tensor</p>
<p><strong>参数说明</strong>:</p>
<p>x1: Device侧的Tensor类型, 支持float16、bfloat16、int8, 支持ND, 输入shape支持2维或者3维。</p>
<p>x2: Device侧的Tensor类型, 支持float16、bfloat16、int8, 支持ND, 非量化场景, 数据类型需要和x1保持一致, 输入shape维度第0维和x1的最后一维保持一致。</p>
<p>hcom: Host侧的String类型, 通信域handle名, 通过get_hccl_comm_name接口获取。</p>
<p><a href="#id19"><span class="problematic" id="id20">*</span></a>: 代表其之前的变量是位置相关, 按照顺序输入, 必选; 之后的变量是键值对赋值的, 位置无关, 可选(不输入会使用默认值)。</p>
<p>reduce_op: Host侧的String类型, reduce操作类型, 当前版本仅支持'sum', 默认值: 'sum'。</p>
<p>bias: Device侧的Tensor类型, 可选输入, 支持int32、float16、bfloat16, 支持ND格式。bias当前仅支持一维, 且维度大小与output/x2的最后一维大小相同。</p>
<p>antiquant_scale: Device侧的Tensor类型, 可选输入, 伪量化场景对x2进行去量化的系数, 支持float16、bfloat16, 支持ND格式。伪量化场景数据类型需要和x1保持一致。antiquant_scale当前per-tensor场景shape为[1],per-channel场景支持shape为[1,n]或者[n]。其中n为x2最后一维的大小。per-group场景支持shape为[ceil(k, antiquant_group_size), n](具体计算逻辑见**约束说明**)。其中k为x2第一维的大小, n为x2最后一维的大小, antiquant_group_size为伪量化场景对输入x2进行反量化计算的groupSize输入。</p>
<p>antiquant_offset: Device侧的Tensor类型, 可选输入, 伪量化场景对x2进行去量化的系数, 支持float16、bfloat16, 支持ND格式。数据类型需要和antiquant_scale保持一致。shape与antiquant_scale保持一致。</p>
<p>x3: Device侧的Tensor类型, 可选输入, matmul计算后的偏移。支持float16、bfloat16。支持ND格式。数据类型需要和输出output保持一致。shape与output的shape相同。伪量化场景暂不支持处理x3。</p>
<p>dequant_scale: Device侧的Tensor类型, 可选输入, matmul计算后的去量化系数。支持int64、uint64、bfloat16、float32, 支持ND格式。shape在per-tensor场景为[1], per-channel场景为[n]/[1,n], 其中n为x2最后一维的大小。</p>
<p>pertoken_scale: Device侧的Tensor类型, 可选输入, matmul计算后的去量化系数。支持float32, 支持ND格式。x1为[b, s, k]时shape为[b*s]，x1为[m, k]时shape为[m]。</p>
<p>comm_quant_scale_1: Device侧的Tensor类型, 可选输入, alltoall通信前后的量化、去量化系数。支持float16、bfloat16, 支持ND格式。per-channel场景，x2为[k, n]时shape为[1, n]或[n]，用户需保证每张卡上数据保持一致且正确。</p>
<p>comm_quant_scale_2: Device侧的Tensor类型, 可选输入, allgather通信前后的量化、去量化系数。支持float16、bfloat16, 支持ND格式。per-channel场景，x2为[k, n]时shape为[1, n]或[n]，用户需保证每张卡上数据保持一致且正确。</p>
<p>comm_turn: Host侧的int类型, 表示rank间通信切分粒度, 默认值: 0, 表示默认的切分方式。当前版本仅支持输入0。</p>
<p>antiquant_group_size: Host侧的int类型, 表示伪量化pre-group算法模式下, 对输入x2进行反量化计算的groupSize输入, 描述一组反量化参数对应的待反量化数据量在k轴方向的大小。当伪量化算法模式不为pre_group时传入0; 当伪量化算法模式为pre_group时传入值的范围为[32, min(k-1, INT_NAX)]且值要求是32的倍数, 其中k为x2第一维的大小。默认值: 0, 为0则表示非per-group场景。</p>
<p><a href="#id21"><span class="problematic" id="id22">**</span></a>输出说明**Tensor类型, 数据类型非量化场景以及伪量化场景与x1保持一致, 全量化场景为float16或者bfloat16。shape第0维度和x1的0维保持一致, 若x1为2维, shape第1维度和x2的1维保持一致, 若x1为3维, shape第1维度和x1的1维保持一致, shape第2维度和x2的1维保持一致。</p>
<p><strong>约束说明</strong>:</p>
<p>输入x1可为2维或者3维、x2必须是2维, 分别为(s, m, k)/(m, k), (k, n), k轴满足mm算子入参要求, k轴相等。bias当前仅支持一维, 且维度大小与output的最后一维大小相同。x3的shape与output的shape相同。antiquant_scale当前per-tensor场景shape为[1],per-channel场景支持shape为[1,n]或者[n], per-group场景支持shape为(ceil(k, antiquant_group_size), n)。antiquant_offset的shape与antiquant_scale一致。dequant_scale的shape在per-tensor场景为[1], per-channel场景为[n]/[1,n]。</p>
<p>[ceil(k, antiquant_group_size), n]中的ceil(k, antiquant_group_size)计算逻辑为: (k + antiquant_group_size - 1) / antiquant_group_size, 并对计算结果取整数部分。</p>
<p>x1、x2不能为空tensor。</p>
<p>非量化场景, x1、x2、bias、x3、output的数据类型保持一致, 可为float16或者bfloat16, antiquant_scale、antiquant_offset、dequant_scale为None。</p>
<p>伪量化场景, x1、bias、x3、antiquant_scale、antiquant_offset, output的数据类型保持一致, 可为float16或者bfloat16, x2的数据类型为int8, dequant_scale为None。</p>
<p>全量化场景, x1、x2的数据类型为int8, dequant_scale的数据类型为int64、uint64、float32或者bfloat16。dequant_scale类型为int64、uint64、float32(仅pertoken场景)时, output数据类型为float16; dequant_scale类型为bfloat16时, output数据类型为bfloat16; bias数据类型为int32; antiquant_scale、antiquant_offset为None。仅输出为BF16时, 支持传入x3。另外, 若dequant_scale需要以int64类型传入, 在调用torch_npu.npu_mm_all_reduce_base()前, 需通过torch_npu.npu_trans_quant_param()接口对dequant_scale进行处理(处理方法见对应的接口使用说明)。</p>
<p>antiquant_group_size中k值的范围与matmul一致, 为[1,65535], INT_MAX大于(k-1)。</p>
<p>x1不支持输入转置后的tensor, x2转置后输入, 需要满足shape的第一维大小与x1的最后一维相同, 满足matmul的计算条件。</p>
<p>Atlas 300I Duo 推理系列产品只支持2卡。</p>
<p>Atlas A2 训练系列产品支持2、4、8卡。</p>
<p>增量场景不使能MC2, 全量场景使能MC2。</p>
<p>支持的PyTorch版本:</p>
<p>PyTorch 2.1</p>
<p>PyTorch 2.0</p>
<p>PyTorch 1.11.0</p>
<p>支持的型号:</p>
<p>Atlas A2 训练系列产品</p>
<p>Atlas 300I Duo 推理系列产品</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.multiprocessing</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">mp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">run_mm_all_reduce_base</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">master_ip</span><span class="p">,</span> <span class="n">master_port</span><span class="p">,</span> <span class="n">x1_shape</span><span class="p">,</span> <span class="n">x2_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch_npu</span><span class="o">.</span><span class="n">npu</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_method</span> <span class="o">=</span> <span class="s1">&#39;tcp://&#39;</span> <span class="o">+</span> <span class="n">master_ip</span> <span class="o">+</span> <span class="s1">&#39;:&#39;</span> <span class="o">+</span> <span class="n">master_port</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;hccl&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.distributed_c10d</span><span class="w"> </span><span class="kn">import</span> <span class="n">_get_default_group</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">__version__</span> <span class="o">&gt;</span> <span class="s1">&#39;2.0.1&#39;</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hcom_info</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">_get_backend</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">get_hccl_comm_name</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">else</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hcom_info</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">get_hccl_comm_name</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">x1_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">x2_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_mm_all_reduce_base</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">hcom_info</span><span class="p">,</span> <span class="n">reduce_op</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;output: &quot;</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">worksize</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">master_ip</span> <span class="o">=</span> <span class="s1">&#39;127.0.0.1&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">master_port</span> <span class="o">=</span> <span class="s1">&#39;50001&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x1_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">512</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">run_mm_all_reduce_base</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">worksize</span><span class="p">,</span> <span class="n">master_ip</span><span class="p">,</span> <span class="n">master_port</span><span class="p">,</span> <span class="n">x1_shape</span><span class="p">,</span> <span class="n">x2_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">),</span> <span class="n">nprocs</span><span class="o">=</span><span class="n">worksize</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_ffn">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_ffn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_ffn" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p><strong>功能描述</strong>:</p>
<p>算子功能：该FFN算子提供MoeFFN和FFN的计算功能。在没有专家分组（expert_tokens为空）时是FFN，有专家分组时是MoeFFN。</p>
<p>计算公式为：</p>
<p>out = activation(xW1+b1)W2+b2</p>
<p>说明：激活层为geglu/swiglu/reglu时，性能使能需要满足门槛要求，即整网中FFN结构所对应的小算子中vector耗时30us且占比10%以上的用例方可尝试FFN融合算子；或在不知道小算子性能的情况下，尝试使能FFN，若性能劣化则不使能FFN。</p>
<p><strong>接口原型</strong>:</p>
<p>npu_ffn(Tensor x, Tensor weight1, Tensor weight2, str activation, <a href="#id23"><span class="problematic" id="id24">*</span></a>, int[]? expert_tokens=None, int[]? expert_tokens_index=None, Tensor? bias1=None, Tensor? bias2=None, Tensor? scale=None, Tensor? offset=None, Tensor? deq_scale1=None, Tensor? deq_scale2=None, Tensor? antiquant_scale1=None, Tensor? antiquant_scale2=None, Tensor? antiquant_offset1=None, Tensor? antiquant_offset2=None, int? inner_precise=None, ScalarType? output_dtype=None) -&gt; Tensor</p>
<p><strong>参数说明</strong>:</p>
<p>x：Tensor类型，即输入参数中的x。公式中的输入x，数据类型支持FLOAT16、BFLOAT16、INT8，数据格式支持ND，支持输入的维度最少是2维[M, K1]，最多是8维。</p>
<p>weight1：Tensor类型，专家的权重数据，公式中的W1，数据类型支持FLOAT16、BFLOAT16、INT8，数据格式支持ND，输入在有/无专家时分别为[E, K1, N1]/[K1, N1]。</p>
<p>weight2：Tensor类型，专家的权重数据，公式中的W2，数据类型支持FLOAT16、BFLOAT16、INT8，数据格式支持ND，输入在有/无专家时分别为[E, K2, N2]/[K2, N2]。</p>
<p>说明： M表示token个数，对应transform中的BS(B（Batch）表示输入样本批量大小、S（Seq-Length）表示输入样本序列长度)；K1表示第一组matmul的输入通道数，对应transform中的H(Head-Size）表示隐藏层的大小)；N1表示第一组matmul的输出通道数；K2表示第二组matmul的输入通道数；N2表示第二组matmul的输出通道数，对应transform中的H；E表示有专家场景的专家数。</p>
<p>expert_tokens：List类型，可选参数。代表各专家的token数，数据类型支持INT，数据格式支持ND，若不为空时可支持的最大长度为256个。</p>
<p>expert_tokens_index：List类型，可选参数。代表各专家计算token的索引值，数据类型支持INT，数据格式支持ND，若不为空时可支持的最大长度为256个。</p>
<p>bias1：Tensor类型，可选参数。权重数据修正值，公式中的b1，数据类型支持FLOAT16、FLOAT32、INT32，数据格式支持ND，输入在有/无专家时分别为[E, N1]/[N1]。</p>
<p>bias2：Tensor类型，可选参数。权重数据修正值，公式中的b2，数据类型支持FLOAT16、FLOAT32、INT32，数据格式支持ND，输入在有/无专家时分别为[E, N2]/[N2]。</p>
<p>activation：string类型，代表使用的激活函数，即输入参数中的activation。当前仅支持fastgelu/gelu/relu/silu/geglu/swiglu/reglu。</p>
<p>scale：Tensor类型，可选参数，量化参数，量化缩放系数，数据类型支持FLOAT32，数据格式支持ND，per-tensor下输入在有/无专家时均为一维向量，输入元素个数在有/无专家时分别为[E]/[1]；per-channel下输入在有/无专家时为二维向量/一维向量，输入元素个数在有/无专家时分别为[E, N1]/[N1]。</p>
<p>offset：Tensor类型，可选参数，量化参数，量化偏移量，数据类型支持FLOAT32，数据格式支持ND，一维向量，输入元素个数在有/无专家时分别为[E]/[1]。</p>
<p>deq_scale1：Tensor类型，可选参数，量化参数，第一组matmul的反量化缩放系数，数据类型支持INT64、FLOAT32、BFLOAT16，数据格式支持ND，输入在有/无专家时分别为[E, N1]/[N1]。</p>
<p>deq_scale2：Tensor类型，可选参数，量化参数，第二组matmul的反量化缩放系数，数据类型支持INT64、FLOAT32、BFLOAT16，数据格式支持ND，输入在有/无专家时分别为[E, N2]/[N2]。</p>
<p>antiquant_scale1：Tensor类型，可选参数，伪量化参数，第一组matmul的缩放系数，数据类型支持FLOAT16、BFLOAT16，数据格式支持ND，per-channel下输入在有/无专家时分别为[E, N1]/[N1]。</p>
<p>antiquant_scale2：Tensor类型，可选参数，伪量化参数，第二组matmul的缩放系数，数据类型支持FLOAT16、BFLOAT16，数据格式支持ND，per-channel下输入在有/无专家时分别为[E, N2]/[N2]。</p>
<p>antiquant_offset1：Tensor类型，可选参数，伪量化参数，第一组matmul的偏移量，数据类型支持FLOAT16、BFLOAT16，数据格式支持ND，per-channel下输入在有/无专家时分别为[E, N1]/[N1]。</p>
<p>antiquant_offset2：Tensor类型，可选参数，伪量化参数，第二组matmul的偏移量，数据类型支持FLOAT16、BFLOAT16，数据格式支持ND，per-channel下输入在有/无专家时分别为[E, N2]/[N2]。</p>
<p>inner_precise：int类型，可选参数，表示高精度或者高性能选择。数据类型支持：INT64。该参数仅对FLOAT16生效，BFLOAT16和INT8不区分高精度和高性能。</p>
<p>innerPrecise为0时，代表开启高精度模式，算子内部采用FLOAT32数据类型计算。</p>
<p>innerPrecise为1时，代表高性能模式。</p>
<p>output_dtype： ScalarType类型，可选参数，该参数只在量化场景生效，其他场景不生效。表示输出Tensor的数据类型，支持输入float16, bfloat16。默认值为None，代表输出Tensor数据类型为float16。</p>
<p><strong>输出说明</strong>:</p>
<p>一个Tensor类型的输出，公式中的输出y，数据类型支持FLOAT16、BFLOAT16，数据格式支持ND，输出维度与x一致。</p>
<p><strong>约束说明</strong>:</p>
<p>有专家时，专家数据的总数需要与x的M保持一致。</p>
<p>激活层为geglu/swiglu/reglu时，仅支持无专家分组时的FLOAT16高性能场景（FLOAT16场景指类型为Tensor的必选参数数据类型都为FLOAT16的场景），且N1=2*K2。</p>
<p>激活层为gelu/fastgelu/relu/silu时，支持有专家或无专家分组的FLOAT16高精度及高性能场景，BFLOAT16场景，量化场景及伪量化场景，且N1=K2。</p>
<p>非量化场景不能输入量化参数和伪量化参数，量化场景不能输入伪量化参数，伪量化场景不能输入量化参数。</p>
<p>量化场景参数类型：x为INT8、weight为INT8、bias为INT32、scale为FLOAT32、offset为FLOAT32，其余参数类型根据y不同分两种情况：</p>
<p>y为FLOAT16，deqScale支持数据类型：UINT64、INT64、FLOAT32。</p>
<p>y为BFLOAT16，deqScale支持数据类型：BFLOAT16。</p>
<p>要求deqScale1与deqScale2的数据类型保持一致。</p>
<p>量化场景支持scale的per-channel模式参数类型：x为INT8、weight为INT8、bias为INT32、scale为FLOAT32、offset为FLOAT32，其余参数类型根据y不同分两种情况：</p>
<p>y为FLOAT16，deqScale支持数据类型：UINT64、INT64。</p>
<p>y为BFLOAT16，deqScale支持数据类型：BFLOAT16。</p>
<p>要求deqScale1与deqScale2的数据类型保持一致。</p>
<p>伪量化场景支持两种不同参数类型：</p>
<p>y为FLOAT16、x为FLOAT16、bias为FLOAT16，antiquant_scale为FLOAT16、antiquant_offset为FLOAT16，weight支持数据类型INT8。</p>
<p>y为BFLOAT16、x为BFLOAT16、bias为FLOAT32，antiquant_scale为BFLOAT16、antiquant_offset为BFLOAT16，weight支持数据类型INT8。innerPrecise参数在BFLOAT16非量化场景，只能配置为0；FLOAT16非量化场景，可以配置为0或者1；量化或者伪量化场景，0和1都可配置，但是配置后不生效。</p>
<p>expert_tokens和expert_tokens_index不可以同时传。</p>
<p>支持的PyTorch版本:</p>
<p>PyTorch 2.1</p>
<p>PyTorch 2.0</p>
<p>PyTorch 1.11.0</p>
<p>支持的型号:</p>
<p>Atlas A2训练系列产品/Atlas 800I A2推理产品中的推理产品</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#单算子调用模式</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1280</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu_weight1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1280</span><span class="p">,</span> <span class="mi">10240</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu_weight2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10240</span><span class="p">,</span> <span class="mi">1280</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">activation</span> <span class="o">=</span> <span class="s2">&quot;fastgelu&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npu_out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_ffn</span><span class="p">(</span><span class="n">cpu_x</span><span class="o">.</span><span class="n">npu</span><span class="p">(),</span> <span class="n">cpu_weight1</span><span class="o">.</span><span class="n">npu</span><span class="p">(),</span> <span class="n">cpu_weight2</span><span class="o">.</span><span class="n">npu</span><span class="p">(),</span> <span class="n">activation</span><span class="p">,</span> <span class="n">inner_precise</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#torch api 入图模式</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torchair</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tng</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.ge_concrete_graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">ge_apis</span> <span class="k">as</span> <span class="n">ge</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.configs.compiler_config</span><span class="w"> </span><span class="kn">import</span> <span class="n">CompilerConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.core.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">logger</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;ENABLE_ACLNN&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;true&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">CompilerConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">debug</span><span class="o">.</span><span class="n">graph_dump</span><span class="o">.</span><span class="n">type</span> <span class="o">=</span> <span class="s2">&quot;pbtxt&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npu_backend</span> <span class="o">=</span> <span class="n">tng</span><span class="o">.</span><span class="n">get_npu_backend</span><span class="p">(</span><span class="n">compiler_config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">MyModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight1</span><span class="p">,</span> <span class="n">weight2</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">expert</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">return</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_ffn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight1</span><span class="p">,</span> <span class="n">weight2</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span>  <span class="n">expert_tokens</span><span class="o">=</span><span class="n">expert</span><span class="p">,</span> <span class="n">inner_precise</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu_model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1954</span><span class="p">,</span> <span class="mi">2560</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu_weight1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">16</span><span class="p">,</span> <span class="mi">2560</span><span class="p">,</span> <span class="mi">5120</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu_weight2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5120</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu&#39;</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">activation</span> <span class="o">=</span> <span class="s2">&quot;fastgelu&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expert</span> <span class="o">=</span> <span class="p">[</span><span class="mi">227</span><span class="p">,</span> <span class="mi">62</span><span class="p">,</span> <span class="mi">78</span><span class="p">,</span> <span class="mi">126</span><span class="p">,</span> <span class="mi">178</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">122</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">182</span><span class="p">,</span> <span class="mi">166</span><span class="p">,</span> <span class="mi">118</span><span class="p">,</span> <span class="mi">66</span><span class="p">,</span> <span class="mi">217</span><span class="p">,</span> <span class="mi">122</span><span class="p">,</span> <span class="mi">243</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">cpu_model</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">cpu_model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">npu_backend</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="n">npu_out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">cpu_x</span><span class="o">.</span><span class="n">npu</span><span class="p">(),</span> <span class="n">cpu_weight1</span><span class="o">.</span><span class="n">npu</span><span class="p">(),</span> <span class="n">cpu_weight2</span><span class="o">.</span><span class="n">npu</span><span class="p">(),</span> <span class="n">activation</span><span class="p">,</span> <span class="n">expert</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_incre_flash_attention">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_incre_flash_attention</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_incre_flash_attention" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p><strong>功能描述</strong>:</p>
<p>增量FA实现, 实现对应公式:</p>
<p>atten_out = softmax(scale*(query*key)+atten_mask)*value</p>
<p><strong>接口原型</strong>:</p>
<p>torch_npu.npu_incre_flash_attention(Tensor query, Tensor key, Tensor value, <a href="#id25"><span class="problematic" id="id26">*</span></a>, Tensor? padding_mask=None, Tensor? atten_mask=None, symint[]? actual_seq_lengths=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? block_table=None, Tensor? dequant_scale1=None, Tensor? quant_scale1=None, Tensor? dequant_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, int num_heads=1, float scale_value=1.0, str input_layout=&quot;BSH&quot;, int num_key_value_heads=0, int block_size=0 , int inner_precise=1) -&gt; Tensor</p>
<p><strong>参数说明</strong>:</p>
<p>query: 三维或者四维Device侧的Input Tensor; 三维: shape是(B,1,H), 对应的input_layout是BSH; 四维: shape是(B,N,1,D), 对应的input_layout是BNSD, 其中N*D=H, 数据类型支持FLOAT16、BFLOAT16, 数据格式支持ND。</p>
<p>key: 三维或者四维Device侧的Input Tensor; 三维: shape是(B,S,H), 对应的input_layout是BSH; 四维: shape是(B,N,S,D), 对应的input_layout是BNSD, 其中N*D=H, 数据类型支持FLOAT16、BFLOAT16, 数据格式支持ND。</p>
<p>value: 三维或者四维Device侧的Input Tensor; 三维: shape是(B,S,H), 对应的input_layout是BSH; 四维: shape是(B,N,S,D), 对应的input_layout是BNSD, 其中N*D=H, 数据类型支持FLOAT16、BFLOAT16, 数据格式支持ND。</p>
<p><a href="#id27"><span class="problematic" id="id28">*</span></a>: 代表其之前的变量是位置相关, 需要按照顺序输入, 必选; 之后的变量是键值对赋值的, 位置无关, 可选(不输入会使用默认值)。</p>
<p>padding_mask: 预留参数, 暂未使用, 默认值为None。</p>
<p>atten_mask: 四维Device侧的Input Tensor, shape是(B,1,1,S); 取值为1代表该位不参与计算(不生效), 为0代表该位参与计算, 默认值为None, 即全部参与计算; 数据类型支持BOOL、INT8、UINT8，数据格式支持ND。</p>
<p>actual_seq_lengths: 二维Host侧的Input数组, 其shape为(B,1), 形如[1, 2, 3], 代表key、value中有效的S序列长度, 默认值为None, 即全部有效, 类型为List int; 数据类型为INT64, 数据格式支持ND。</p>
<p>antiquantScale: Device侧的Input Tensor, 数据类型支持: FLOAT16、BFLOAT16。数据格式支持ND, 表示量化因子, 支持per-channel(list)。 如不使用该功能时可不传或传入None。</p>
<p>antiquantOffset: Device侧的Input Tensor, 数据类型支持: FLOAT16、BFLOAT16。数据格式支持ND, 表示量化偏移, 支持per-channel(list), 由shape决定。 如不使用该功能时可不传或传入None。</p>
<p>blocktable: Device侧的Input Tensor, 数据类型支持: INT32。数据格式支持ND, 表示PageAttention中KV存储使用的block映射表。 如不使用该功能时可不传或传入None。</p>
<p>dequantScale1: Device侧的Input Tensor, 数据类型支持: FLOAT32。数据格式支持ND, 表示BMM1后面反量化的量化因子, 支持per-tensor(scalar)。 如不使用该功能时可不传或传入None。</p>
<p>quantScale1: Device侧的Input Tensor, 数据类型支持: FLOAT32。数据格式支持ND, 表示BMM2前面量化的量化因子, 支持per-tensor(scalar)。 如不使用该功能时可不传或传入None。</p>
<p>dequantScale2: Device侧的Input Tensor, 数据类型支持: FLOAT32。数据格式支持ND, 表示BMM2后面反量化的量化因子, 支持per-tensor(scalar)。 如不使用该功能时可不传或传入None。</p>
<p>quantScale2: Device侧的Input Tensor, 数据类型支持: FLOAT32、BFLOAT16。数据格式支持ND, 表示输出量化的量化因子, 支持per-tensor(scalar)。 如不使用该功能时可不传或传入None。</p>
<p>quantOffset2: Device侧的Input Tensor, 数据类型支持: FLOAT32、BFLOAT16。数据格式支持ND, 表示输出量化的量化偏移, 支持per-tensor(scalar)。 如不使用该功能时可不传或传入None。</p>
<p>kvPaddingSize: Device侧的aclTensor, 数据类型支持: INT64。数据格式支持ND, 表示kv左padding场景使能时, 最后一个有效token到S的距离。 如不使用该功能时可传入nullptr。</p>
<p>num_heads: Host侧的attribute, 代表query的头数, 即query的N, 其乘D为H, 默认值为1; 数据类型为INT。</p>
<p>scale_value: Host侧的attribute, 代表缩放系数, 用来约束梯度, 其默认值为1.0, 典型值为; 数据类型为FLOAT32。</p>
<p>input_layout: Host侧的attribute, 代表query、key、value的布局, 根据输入的query、key、value的shape确定, 三维Tensor是BSH, 四维Tensor是BNSD, 默认值为BSH, 不支持其他值; 数据类型为string。</p>
<p>num_key_value_heads: Host侧的attribute, 代表key、value的头数, 默认值为0, 表示与query的头数相同, 否则表示key、value的头数, 需要能被query的头数(num_heads)整除; 数据类型为INT64。</p>
<p>blockSize (int64_t, 计算输入): Host侧的int64_t, PageAttention中KV存储每个block中最大的token个数, 默认为0, 数据类型支持INT64。</p>
<p>innerPrecise (int64_t, 计算输入): Host侧的int64_t, 代表高精度/高性能选择, 默认值为1(高性能),  数据类型支持INT64。</p>
<p><strong>输出说明</strong>:</p>
<p>共一个输出, 为计算的最终结果atten_out, 类型为Tensor, shape与query保持一致。</p>
<p>非量化场景下, 输出数据类型与query的数据类型保持一致。</p>
<p>量化场景下, 若传入quantScale2, 则输出数据类型为int8; 若不传入quantScale2, 且query、key、value类型为int8, 则输出数据类型为FLOAT16。</p>
<p><strong>约束说明</strong>:</p>
<p>query、key、value的维度必须保持一致, key、value的shape必须保持一致。</p>
<p>num_heads的值要等于query的N。</p>
<p>input_layout的值与query的shape相关, 三维是BSH, 四维是BNSD。</p>
<p>num_key_value_heads的值要等于key、value的N, 需要能被query的头数(num_heads)整除。</p>
<p>D一般取值128、256等典型值, D的限制为16k, 大于16k会报错拦截。</p>
<p>page attention的使能必要条件是blocktable存在且有效, 同时key、value是按照blocktable中的索引在一片连续内存中排布, 支持key、value dtype为FLOAT16/BFLOAT16。</p>
<p>page attention的使能场景下, blockSize是用户自定义的参数, 该参数的取值会影响page attention的性能, 推荐使用128, 或者满足32byte对齐。通常情况下, page attention可以提高吞吐量, 但会带来性能上的下降。</p>
<p>blockTable当前支持的maxBlockNumPerSeq最大为16k, 超过16k会被拦截报错; 如果遇到S超大导致maxBlockNumPerSeq超过16k, 可以调大blockSize解决。</p>
<p>dequantScale1、quantScale1、dequantScale2为一组参数, 需要同时传入, 且传入该组参数后会按照量化场景处理, 需要query、key、value的数据类型为int8, 否则会报错。</p>
<p>quantScale2、quantOffset2为一组参数, 其中quantOffset2可选, 传入该组参数后算子输出数据类型会推导为int8, 若不期望int8输出, 请勿传入该组参数。</p>
<p>kv左padding场景kvCache的搬运起点计算公式为: Smax - kvPaddingSize - actualSeqLengths。kvCache的搬运终点计算公式为: Smax - kvPaddingSize。其中kvCache的搬运起点或终点小于0时, 返回数据结果为全0。</p>
<p>kv左padding场景kvPaddingSize小于0时将被置为0。</p>
<p>kv左padding场景使能需要同时存在actualSeqLengths参数, 否则默认为kv右padding场景。</p>
<p>支持的PyTorch版本:</p>
<p>PyTorch 2.1</p>
<p>支持的型号:</p>
<p>Atlas A2 训练系列产品</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># 单算子调用方式</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 生成随机数据, 并发送到npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">40</span> <span class="o">*</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">40</span> <span class="o">*</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">40</span> <span class="o">*</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">128.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 调用IFA算子</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_incre_flash_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">input_layout</span><span class="o">=</span><span class="s2">&quot;BSH&quot;</span><span class="p">,</span> <span class="n">scale_value</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 执行上述代码的输出类似如下</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.3091</span><span class="p">,</span>  <span class="mf">0.0651</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3525</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8252</span><span class="p">,</span>  <span class="mf">0.4084</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2754</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 入图方式</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torchair</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tng</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.ge_concrete_graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">ge_apis</span> <span class="k">as</span> <span class="n">ge</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.configs.compiler_config</span><span class="w"> </span><span class="kn">import</span> <span class="n">CompilerConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch._dynamo</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">TORCHDYNAMO_VERBOSE</span><span class="o">=</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">TORCH_LOGS</span><span class="o">=</span><span class="s2">&quot;+dynamo&quot;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 支持入图的打印宏</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.core.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">logger</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">CompilerConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">aoe_config</span><span class="o">.</span><span class="n">aoe_mode</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">debug</span><span class="o">.</span><span class="n">graph_dump</span><span class="o">.</span><span class="n">type</span> <span class="o">=</span> <span class="s2">&quot;pbtxt&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npu_backend</span> <span class="o">=</span> <span class="n">tng</span><span class="o">.</span><span class="n">get_npu_backend</span><span class="p">(</span><span class="n">compiler_config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.library</span><span class="w"> </span><span class="kn">import</span> <span class="n">Library</span><span class="p">,</span> <span class="n">impl</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 数据生成</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">40</span> <span class="o">*</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="mi">40</span> <span class="o">*</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="mi">40</span> <span class="o">*</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">atten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale_value</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">128.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">return</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_incre_flash_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">input_layout</span><span class="o">=</span><span class="s2">&quot;BSH&quot;</span><span class="p">,</span> <span class="n">scale_value</span><span class="o">=</span><span class="n">scale_value</span><span class="p">,</span> <span class="n">atten_mask</span><span class="o">=</span><span class="n">atten</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">MetaInfershape</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">npu_backend</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">graph_output</span> <span class="o">=</span> <span class="n">model</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">single_op</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_incre_flash_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">input_layout</span><span class="o">=</span><span class="s2">&quot;BSH&quot;</span><span class="p">,</span> <span class="n">scale_value</span><span class="o">=</span><span class="n">scale_value</span><span class="p">,</span> <span class="n">atten_mask</span><span class="o">=</span><span class="n">atten</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;single op output with mask:&quot;</span><span class="p">,</span> <span class="n">single_op</span><span class="p">,</span> <span class="n">single_op</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;graph output with mask:&quot;</span><span class="p">,</span> <span class="n">graph_output</span><span class="p">,</span> <span class="n">graph_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">MetaInfershape</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 执行上述代码的输出类似如下</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">single</span> <span class="n">op</span> <span class="n">output</span> <span class="k">with</span> <span class="n">mask</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[[</span> <span class="mf">0.2488</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6572</span><span class="p">,</span>  <span class="mf">1.0928</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.1694</span><span class="p">,</span>  <span class="mf">0.1142</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.2266</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">0.9595</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9609</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6602</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.7959</span><span class="p">,</span>  <span class="mf">1.7920</span><span class="p">,</span>  <span class="mf">0.0783</span><span class="p">]]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5120</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">graph</span> <span class="n">output</span> <span class="k">with</span> <span class="n">mask</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[[</span> <span class="mf">0.2488</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6572</span><span class="p">,</span>  <span class="mf">1.0928</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.1694</span><span class="p">,</span>  <span class="mf">0.1142</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.2266</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="o">-</span><span class="mf">0.9595</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9609</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6602</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.7959</span><span class="p">,</span>  <span class="mf">1.7920</span><span class="p">,</span>  <span class="mf">0.0783</span><span class="p">]]],</span>       <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5120</span><span class="p">])</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_prompt_flash_attention">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_prompt_flash_attention</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_prompt_flash_attention" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p><strong>功能描述</strong>:</p>
<p>全量FA实现, 实现对应公式:</p>
<p>atten_out = softmax(scale*(query*key)+atten_mask)*value</p>
<p><strong>接口原型</strong>:</p>
<p>torch_npu.npu_prompt_flash_attention(Tensor query, Tensor key, Tensor value, <a href="#id29"><span class="problematic" id="id30">*</span></a>, Tensor? pse_shiftpadding_mask=None, Tensor? atten_mask=None, int[]? actual_seq_lengths=None, Tensor? deq_scale1=None, Tensor? quant_scale1=None, Tensor? deq_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, int num_heads=1, float scale_value=1.0, int pre_tokens=2147473647, int next_tokens=0, str input_layout=&quot;BSH&quot;, int num_key_value_heads=0, int[]? actual_seq_lengths_kv=None, int sparse_mode=0) -&gt; Tensor</p>
<p><strong>参数说明</strong>:</p>
<p>Query: 三维或者四维Device侧的Input Tensor; 三维: shape是(B,S,H), 对应的input_layout是BSH; 四维: shape是(B,N,S,D), 其中N*D=H, 数据类型支持FLOAT16、BFLOAT16, 数据格式支持ND。</p>
<p>Key: 三维或者四维Device侧的Input Tensor; 三维: shape是(B,S,H), 对应的input_layout是BSH; 四维: shape是(B,N,S,D), 其中N*D=H, 数据类型支持FLOAT16、BFLOAT16, 数据格式支持ND。</p>
<p>Value: 三维或者四维Device侧的Input Tensor; 三维: shape是(B,S,H), 对应的input_layout是BSH; 四维: shape是(B,N,S,D), 其中N*D=H, 数据类型支持FLOAT16、BFLOAT16, 数据格式支持ND。</p>
<p><a href="#id31"><span class="problematic" id="id32">*</span></a>: 代表其之前的变量是位置相关, 需要按照顺序输入, 必选; 之后的变量是键值对赋值的, 位置无关, 可选(不输入会使用默认值)。</p>
<p>pse_shift: 四维Device侧的Input Tensor, shape是(B,N,Query S, Key S)或者(1,N,Query S, Key S), 默认值为None。数据类型支持FLOAT16, 数据格式支持ND。</p>
<p>padding_mask: 预留参数, 暂未使用, 默认值为None。</p>
<p>atten_mask: 四维Device侧的Input Tensor, shape是(B,1,Query S, Key S), 取值为1代表该位不参与计算(不生效), 为0代表该位参与计算, 默认值为None, 即全部参与计算; 数据类型支持FLOAT16、BOOL, 数据格式支持ND。</p>
<p>actual_seq_lengths: 二维Host侧的Input数组, 其shape为(B,1), 形如[1, 2, 3], 代表每个batch中 query的有效序列长度, 默认值为默认值为None, 即全部有效</p>
<p>deqScale1: Device侧的Input Tensor, 其shape为(1), 数据类型支持: UINT64、FLOAT32。数据格式支持ND, 表示第1次Matmul计算后反量化的量化因子, 支持pre-tensor(scalar)。 如不使用该功能时可传入nullptr。</p>
<p>quantScale1: Device侧的Input Tensor, 其shape为(1), 数据类型支持: FLOAT。数据格式支持ND, 表示第2次Matmul计算前量化的量化因子, 支持pre-tensor(scalar)。 如不使用该功能时可传入nullptr。</p>
<p>deqScale2: Device侧的Input Tensor, 其shape为(1), 数据类型支持: UINT64、FLOAT32。数据格式支持ND, 表示第2次Matmul计算后量化的量化因子, 支持pre-tensor(scalar)。 如不使用该功能时可传入nullptr。</p>
<p>quantScale2: Device侧的Input Tensor, 其shape为(1), 数据类型支持: FLOAT。数据格式支持ND, 表示输出量化的量化因子, 支持pre-tensor(scalar)。 如不使用该功能时可传入nullptr。</p>
<p>quantOffset2: Device侧的Input Tensor, 其shape为(1), 数据类型支持: FLOAT。数据格式支持ND, 表示输出量化的量化偏移, 支持pre-tensor(scalar)。 如不使用该功能时可传入nullptr。</p>
<p>num_heads: Host侧的attribute, query的头数, 即BNSD中的N, 其乘以D为H, 默认值为1; 数据类型为INT。</p>
<p>scale_value: Host侧的attribute, 缩放系数, 用来约束梯度, 其默认值为1.0, 典型值为1/sqrt(D); 数据类型为FLOAT32。</p>
<p>pre_tokens: Host侧的attribute, 用于指定参与计算的有效数据块, 其默认值为2147473647</p>
<p>next_tokens: Host侧的attribute, 用于指定参与计算的有效数据块, 其默认值为0</p>
<p>input_layout: Host侧的attribute, 代表query、key、value的布局, 根据输入的Query、Key、Value的shape确定, 三维张量是BSH, 四维张量是BNSD, 默认值为BSH; 数据类型为string。</p>
<p>num_key_value_heads: Host侧的attribute, kv的头数, 默认值为0, 表示与q的头数相同; 否则表示kv的头数, 需要能被q的头数(num_heads)整除; 数据类型为INT64。</p>
<p>actual_seq_lengths_kv: Host侧的attribute, 每个batch中key和value的 S的有效长度, 其shape为(B,1), 代表kv中有效的序列长度, 默认值为默认值为None, 即全部有效; 数据类型为INT64。</p>
<p>sparse_mode: Host侧的attribute, 针对noMask、leftUpCasual、rightDownCasual、band四类sparse场景, 新增可选属性sparse_mode(UINT64, 枚举), 对应枚举值分别为0、2、3、4。</p>
<p><a href="#id33"><span class="problematic" id="id34">**</span></a>输出说明**共一个输出, 为计算的最终结果atten_out, 类型为Tensor, shape与query保持一致。·</p>
<p><strong>约束说明</strong>:</p>
<p>query、key、value的维度必须保持一致, key、value的shape必须保持一致。</p>
<p>pse_shift使能时, 目前只支持query为FLOAT16类型, 且pse_shift也为FLOAT16类型。</p>
<p>num_heads的值要等于query的N。</p>
<p>input_layout的值与query的shape相关, 三维是“BSH”, 四维是“BNSD”。</p>
<p>num_key_value_heads的值要等于key、value的N, 需要能被query的头数(num_heads)整除。</p>
<p>D一般取值128、256等典型值, D的限制为16k, 大于16k会报错拦截。</p>
<p>int8量化相关入参数量与输入、输出数据格式的综合限制:</p>
<p>输入为INT8, 输出为INT8的场景: 入参deqScale1、quantScale1、deqScale2、quantScale2、quantOffset2需要同时存在。</p>
<p>输入为INT8, 输出为FLOAT16的场景: 入参deqScale1、quantScale1、deqScale2需要同时存在, 若存在入参quantOffset2 或quantScale2(即不为nullptr), 则报错并返回。</p>
<p>输入为FLOAT16, 输出为INT8的场景: 入参quantOffset2 或 quantScale2需要同时存在, 若存在入参deqScale1 或 quantScale1 或 deqScale2(即不为nullptr), 则报错并返回。</p>
<p>支持的PyTorch版本:</p>
<p>PyTorch 2.1</p>
<p>支持的芯片型号:</p>
<p>Atlas A2 训练系列产品</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># 单算子调用方式</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 生成随机数据, 并发送到npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">164</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">128.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 调用PFA算子</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_prompt_flash_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">input_layout</span> <span class="o">=</span> <span class="s2">&quot;BNSD&quot;</span><span class="p">,</span> <span class="n">scale_value</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">pre_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">,</span> <span class="n">next_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 执行上述代码的输出类似如下</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.0219</span><span class="p">,</span>  <span class="mf">0.0201</span><span class="p">,</span>  <span class="mf">0.0049</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0118</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0011</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0140</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0294</span><span class="p">,</span>  <span class="mf">0.0256</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0081</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0267</span><span class="p">,</span>  <span class="mf">0.0067</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0117</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0285</span><span class="p">,</span>  <span class="mf">0.0296</span><span class="p">,</span>  <span class="mf">0.0011</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0150</span><span class="p">,</span>  <span class="mf">0.0056</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0062</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0177</span><span class="p">,</span>  <span class="mf">0.0194</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0060</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0226</span><span class="p">,</span>  <span class="mf">0.0029</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0039</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0180</span><span class="p">,</span>  <span class="mf">0.0186</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0067</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0204</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0045</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0164</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0176</span><span class="p">,</span>  <span class="mf">0.0288</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0091</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0304</span><span class="p">,</span>  <span class="mf">0.0033</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0173</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 入图方式</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torchair</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tng</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.ge_concrete_graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">ge_apis</span> <span class="k">as</span> <span class="n">ge</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.configs.compiler_config</span><span class="w"> </span><span class="kn">import</span> <span class="n">CompilerConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch._dynamo</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">TORCHDYNAMO_VERBOSE</span><span class="o">=</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">TORCH_LOGS</span><span class="o">=</span><span class="s2">&quot;+dynamo&quot;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 支持入图的打印宏</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.core.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">logger</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">CompilerConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">aoe_config</span><span class="o">.</span><span class="n">aoe_mode</span> <span class="o">=</span> <span class="s2">&quot;2&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">debug</span><span class="o">.</span><span class="n">graph_dump</span><span class="o">.</span><span class="n">type</span> <span class="o">=</span> <span class="s2">&quot;pbtxt&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npu_backend</span> <span class="o">=</span> <span class="n">tng</span><span class="o">.</span><span class="n">get_npu_backend</span><span class="p">(</span><span class="n">compiler_config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.library</span><span class="w"> </span><span class="kn">import</span> <span class="n">Library</span><span class="p">,</span> <span class="n">impl</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 数据生成</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">164</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">128.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">return</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_prompt_flash_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">input_layout</span> <span class="o">=</span> <span class="s2">&quot;BNSD&quot;</span><span class="p">,</span> <span class="n">scale_value</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">pre_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">,</span> <span class="n">next_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">MetaInfershape</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">npu_backend</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">graph_output</span> <span class="o">=</span> <span class="n">model</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">single_op</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_prompt_flash_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">input_layout</span> <span class="o">=</span> <span class="s2">&quot;BNSD&quot;</span><span class="p">,</span> <span class="n">scale_value</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">pre_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">,</span> <span class="n">next_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;single op output with mask:&quot;</span><span class="p">,</span> <span class="n">single_op</span><span class="p">,</span> <span class="n">single_op</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;graph output with mask:&quot;</span><span class="p">,</span> <span class="n">graph_output</span><span class="p">,</span> <span class="n">graph_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">MetaInfershape</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 执行上述代码的输出类似如下</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">single</span> <span class="n">op</span> <span class="n">output</span> <span class="k">with</span> <span class="n">mask</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.0219</span><span class="p">,</span>  <span class="mf">0.0201</span><span class="p">,</span>  <span class="mf">0.0049</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0118</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0011</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0140</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0294</span><span class="p">,</span>  <span class="mf">0.0256</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0081</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0267</span><span class="p">,</span>  <span class="mf">0.0067</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0117</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0285</span><span class="p">,</span>  <span class="mf">0.0296</span><span class="p">,</span>  <span class="mf">0.0011</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0150</span><span class="p">,</span>  <span class="mf">0.0056</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0062</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0177</span><span class="p">,</span>  <span class="mf">0.0194</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0060</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0226</span><span class="p">,</span>  <span class="mf">0.0029</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0039</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0180</span><span class="p">,</span>  <span class="mf">0.0186</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0067</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0204</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0045</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0164</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0176</span><span class="p">,</span>  <span class="mf">0.0288</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0091</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0304</span><span class="p">,</span>  <span class="mf">0.0033</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0173</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">graph</span> <span class="n">output</span> <span class="k">with</span> <span class="n">mask</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.0219</span><span class="p">,</span>  <span class="mf">0.0201</span><span class="p">,</span>  <span class="mf">0.0049</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0118</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0011</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0140</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0294</span><span class="p">,</span>  <span class="mf">0.0256</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0081</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0267</span><span class="p">,</span>  <span class="mf">0.0067</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0117</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0285</span><span class="p">,</span>  <span class="mf">0.0296</span><span class="p">,</span>  <span class="mf">0.0011</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0150</span><span class="p">,</span>  <span class="mf">0.0056</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0062</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0177</span><span class="p">,</span>  <span class="mf">0.0194</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0060</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0226</span><span class="p">,</span>  <span class="mf">0.0029</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0039</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0180</span><span class="p">,</span>  <span class="mf">0.0186</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0067</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0204</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0045</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0164</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0176</span><span class="p">,</span>  <span class="mf">0.0288</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0091</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0304</span><span class="p">,</span>  <span class="mf">0.0033</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0173</span><span class="p">]],</span>        <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch_npu.npu_fused_infer_attention_score">
<span class="sig-prename descclassname"><span class="pre">torch_npu.</span></span><span class="sig-name descname"><span class="pre">npu_fused_infer_attention_score</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch_npu.npu_fused_infer_attention_score" title="Link to this definition"></a></dt>
<dd></dd></dl>

<p><strong>功能描述</strong>:</p>
<p>算子功能：适配增量&amp;全量推理场景的FlashAttention算子，既可以支持全量计算场景（PromptFlashAttention），也可支持增量计算场景（IncreFlashAttention）。当Query矩阵的S为1，进入IncreFlashAttention分支，其余场景进入PromptFlashAttention分支。</p>
<p>计算公式：atten_out = softmax(scale*(query*key)+atten_mask)*value</p>
<p><strong>接口原型</strong>:</p>
<p>torch_npu.npu_fused_infer_attention_score(Tensor query, Tensor key, Tensor value, <a href="#id35"><span class="problematic" id="id36">*</span></a>, Tensor? pse_shift=None, Tensor? atten_mask=None, SymInt[]? actual_seq_lengths=None, SymInt[]? actual_seq_lengths_kv=None, Tensor? dequant_scale1=None, Tensor? quant_scale1=None, Tensor? dequant_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? key_antiquant_scale=None, Tensor? key_antiquant_offset=None, Tensor? value_antiquant_scale=None, Tensor? value_antiquant_offset=None, Tensor? block_table=None, Tensor? query_padding_size=None, Tensor? kv_padding_size=None, Tensor? key_shared_prefix=None, Tensor? value_shared_prefix=None, SymInt[]? actual_shared_prefix_len=None, int num_heads=1, float scale=1.0, int pre_tokens=2147483647, int next_tokens=2147483647, str input_layout=&quot;BSH&quot;, int num_key_value_heads=0, int sparse_mode=0, int inner_precise=0, int block_size=0, int antiquant_mode=0, int key_antiquant_mode=0, int value_antiquant_mode=0, bool softmax_lse_flag=False) -&gt; (Tensor, Tensor)</p>
<p><strong>参数说明</strong>:</p>
<p>Query: 三维或者四维Device侧的Input Tensor; 三维: shape是(B,S,H), 对应的input_layout是BSH; 四维: shape是(B,N,S,D), 其中N*D=H, 数据类型支持FLOAT16、BFLOAT16, 数据格式支持ND。</p>
<p>Key: 三维或者四维Device侧的Input Tensor; 三维: shape是(B,S,H), 对应的input_layout是BSH; 四维: shape是(B,N,S,D), 其中N*D=H, 数据类型支持FLOAT16、BFLOAT16, 数据格式支持ND。</p>
<p>Value: 三维或者四维Device侧的Input Tensor; 三维: shape是(B,S,H), 对应的input_layout是BSH; 四维: shape是(B,N,S,D), 其中N*D=H, 数据类型支持FLOAT16、BFLOAT16, 数据格式支持ND。</p>
<p><a href="#id37"><span class="problematic" id="id38">*</span></a>: 代表其之前的变量是位置相关, 需要按照顺序输入, 必选; 之后的变量是键值对赋值的, 位置无关, 可选(不输入会使用默认值)。</p>
<p>pse_shift: 四维Device侧的Input Tensor, shape是(B,N,Query S, Key S)或者(1,N,Query S, Key S), 默认值为None。数据类型支持FLOAT16, 数据格式支持ND。</p>
<p>atten_mask: 四维Device侧的Input Tensor, shape是(B,1,Query S, Key S), 取值为1代表该位不参与计算(不生效), 为0代表该位参与计算, 默认值为None, 即全部参与计算; 数据类型支持FLOAT16、BOOL, 数据格式支持ND。</p>
<p>actual_seq_lengths: 二维Host侧的Input数组, 其shape为(B,1), 形如[1, 2, 3], 代表每个batch中 query的有效序列长度, 默认值为默认值为None, 即全部有效。</p>
<p>actual_seq_lengths_kv: Host侧的attribute, 每个batch中key和value的 S的有效长度, 其shape为(B,1), 代表kv中有效的序列长度, 默认值为默认值为None, 即全部有效; 数据类型为INT64。</p>
<p>antiquantScale：Device侧的aclTensor，数据类型支持：FLOAT32、FLOAT16、BFLOAT16。数据格式支持ND（参考），表示反量化因子，支持per-tensor，per-channel，Q_S为1时只支持per-channel，综合约束请见约束与限制。</p>
<p>antiquantOffset：Device侧的aclTensor，数据类型支持：FLOAT32、FLOAT16、BFLOAT16。数据格式支持ND（参考），表示反量化偏移，支持per-tensor，per-channel，Q_S为1时只支持per-channel，综合约束请见约束与限制。</p>
<p>dequant_scale1: Device侧的Input Tensor, 其shape为(1), 数据类型支持: UINT64、FLOAT32。数据格式支持ND, 表示第1次Matmul计算后反量化的量化因子, 支持pre-tensor(scalar)。 如不使用该功能时可传入nullptr。</p>
<p>quantScale1: Device侧的Input Tensor, 其shape为(1), 数据类型支持: FLOAT。数据格式支持ND, 表示第2次Matmul计算前量化的量化因子, 支持pre-tensor(scalar)。 如不使用该功能时可传入nullptr。</p>
<p>dequant_scale2: Device侧的Input Tensor, 其shape为(1), 数据类型支持: UINT64、FLOAT32。数据格式支持ND, 表示第2次Matmul计算后量化的量化因子, 支持pre-tensor(scalar)。 如不使用该功能时可传入nullptr。</p>
<p>quantScale2: Device侧的Input Tensor, 其shape为(1), 数据类型支持: FLOAT。数据格式支持ND, 表示输出量化的量化因子, 支持pre-tensor(scalar)。 如不使用该功能时可传入nullptr。</p>
<p>quantOffset2: Device侧的Input Tensor, 其shape为(1), 数据类型支持: FLOAT。数据格式支持ND, 表示输出量化的量化偏移, 支持pre-tensor(scalar)。 如不使用该功能时可传入nullptr。</p>
<p>blocktable：Device侧的aclTensor，数据类型支持：INT32。数据格式支持ND（参考）。表示PageAttention中KV存储使用的block映射表，如不使用该功能可传入nullptr；Q_S大于等于2时该参数无效。</p>
<p>queryPaddingSize：Query中每个batch的数据是否右对齐，且右对齐的个数是多少。仅支持Q_S等于1；Q_S大于等于2时该参数无效。用户不特意指定时可传入默认值nullptr。</p>
<p>kvPaddingSize：key/value中每个batch的数据是否右对齐，且右对齐的个数是多少。仅支持Q_S等于1；Q_S大于等于2时该参数无效。用户不特意指定时可传入默认值nullptr。</p>
<p>num_heads: Host侧的attribute, query的头数, 即BNSD中的N, 其乘以D为H, 默认值为1; 数据类型为INT。</p>
<p>scale: Host侧的attribute, 缩放系数, 用来约束梯度, 其默认值为1.0, 典型值为1/sqrt(D); 数据类型为FLOAT32。</p>
<p>pre_tokens: Host侧的attribute, 用于指定参与计算的有效数据块, 其默认值为2147473647</p>
<p>next_tokens: Host侧的attribute, 用于指定参与计算的有效数据块, 其默认值为0</p>
<p>input_layout: Host侧的attribute, 代表query、key、value的布局, 根据输入的Query、Key、Value的shape确定, 三维张量是BSH, 四维张量是BNSD, 默认值为BSH; 数据类型为string。</p>
<p>num_key_value_heads: Host侧的attribute, kv的头数, 默认值为0, 表示与q的头数相同; 否则表示kv的头数, 需要能被q的头数(num_heads)整除; 数据类型为INT64。</p>
<p>sparse_mode: Host侧的attribute, 针对noMask、leftUpCasual、rightDownCasual、band四类sparse场景, 新增可选属性sparse_mode(UINT64, 枚举), 对应枚举值分别为0、2、3、4。</p>
<p>innerPrecise：Host侧的int，一共4种模式：0、1、2、3。一共两位bit位，第0位（bit0）表示高精度或者高性能选择，第1位（bit1）表示是否做行无效修正。数据类型支持：INT64。Q_S为1时该参数无效。综合约束请见约束与限制。</p>
<p>innerPrecise为0时，代表开启高精度模式，且不做行无效修正。</p>
<p>innerPrecise为1时，代表高性能模式，且不做行无效修正。</p>
<p>innerPrecise为2时，代表开启高精度模式，且做行无效修正。</p>
<p>innerPrecise为3时，代表高性能模式，且做行无效修正。</p>
<p>softmaxLseFlag：是否输出softmax_lse，支持S轴外切（增加输出）。预留参数，暂不支持。用户不特意指定时可传入默认值false。</p>
<p>blockSize：Host侧的int64_t，PageAttention中KV存储每个block中最大的token个数，默认为0，数据类型支持INT64，Q_S大于等于2时该参数无效。</p>
<p>antiquantMode：伪量化的方式，分为perchannel（perchannel包含pertensor）和pertoken。仅支持Q_S等于1；Q_S大于等于2时该参数无效。用户不特意指定时可传入默认值nullptr。</p>
<p>attentionOut（aclTensor*，计算输出）：Device侧的aclTensor，公式中的输出，数据类型支持FLOAT16、BFLOAT16、INT8。数据格式支持ND。限制：该入参的shape需要与入参query的shape保持一致。</p>
<p>softmaxLse（aclTensor*，计算输出）：flashdecode算法对query乘key的结果先取exp再取sum，最后取log得到的结果。预留参数，暂不支持。用户不特意指定时可传入默认值nullptr。</p>
<p><a href="#id39"><span class="problematic" id="id40">**</span></a>输出说明**共两个输出, atten_out为计算的最终结果, 类型为Tensor, shape与query保持一致。softmaxLse当前预留，暂不支持。</p>
<p><strong>约束说明</strong>:</p>
<p>当Q_S等于1时：请参考Incre_Flash_Attention限制</p>
<p>当Q_S大于1时：请参考Prompt_Flash_Attention限制</p>
<p>支持的PyTorch版本:</p>
<p>PyTorch 2.1</p>
<p>支持的芯片型号:</p>
<p>Atlas A2 训练系列产品</p>
<p><strong>示例</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># 单算子调用方式</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 生成随机数据, 并发送到npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">164</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">128.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 调用FIA算子</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_fused_infer_attention_score</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">input_layout</span> <span class="o">=</span> <span class="s2">&quot;BNSD&quot;</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="p">,</span> <span class="n">pre_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">,</span> <span class="n">next_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 执行上述代码的输出类似如下</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.0219</span><span class="p">,</span>  <span class="mf">0.0201</span><span class="p">,</span>  <span class="mf">0.0049</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0118</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0011</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0140</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0294</span><span class="p">,</span>  <span class="mf">0.0256</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0081</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0267</span><span class="p">,</span>  <span class="mf">0.0067</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0117</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0285</span><span class="p">,</span>  <span class="mf">0.0296</span><span class="p">,</span>  <span class="mf">0.0011</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0150</span><span class="p">,</span>  <span class="mf">0.0056</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0062</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0177</span><span class="p">,</span>  <span class="mf">0.0194</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0060</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0226</span><span class="p">,</span>  <span class="mf">0.0029</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0039</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0180</span><span class="p">,</span>  <span class="mf">0.0186</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0067</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0204</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0045</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0164</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0176</span><span class="p">,</span>  <span class="mf">0.0288</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0091</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0304</span><span class="p">,</span>  <span class="mf">0.0033</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0173</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 入图方式</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch_npu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torchair</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tng</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.ge_concrete_graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">ge_apis</span> <span class="k">as</span> <span class="n">ge</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.configs.compiler_config</span><span class="w"> </span><span class="kn">import</span> <span class="n">CompilerConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch._dynamo</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">TORCHDYNAMO_VERBOSE</span><span class="o">=</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">TORCH_LOGS</span><span class="o">=</span><span class="s2">&quot;+dynamo&quot;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 支持入图的打印宏</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchair.core.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">logger</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">CompilerConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">aoe_config</span><span class="o">.</span><span class="n">aoe_mode</span> <span class="o">=</span> <span class="s2">&quot;2&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">debug</span><span class="o">.</span><span class="n">graph_dump</span><span class="o">.</span><span class="n">type</span> <span class="o">=</span> <span class="s2">&quot;pbtxt&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npu_backend</span> <span class="o">=</span> <span class="n">tng</span><span class="o">.</span><span class="n">get_npu_backend</span><span class="p">(</span><span class="n">compiler_config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.library</span><span class="w"> </span><span class="kn">import</span> <span class="n">Library</span><span class="p">,</span> <span class="n">impl</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 数据生成</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">164</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">128.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">return</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_fused_infer_attention_score</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">input_layout</span> <span class="o">=</span> <span class="s2">&quot;BNSD&quot;</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="p">,</span> <span class="n">pre_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">,</span> <span class="n">next_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">MetaInfershape</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">npu_backend</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">graph_output</span> <span class="o">=</span> <span class="n">model</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">single_op</span> <span class="o">=</span> <span class="n">torch_npu</span><span class="o">.</span><span class="n">npu_fused_infer_attention_score</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">input_layout</span> <span class="o">=</span> <span class="s2">&quot;BNSD&quot;</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span><span class="p">,</span> <span class="n">pre_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">,</span> <span class="n">next_tokens</span><span class="o">=</span><span class="mi">65535</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;single op output with mask:&quot;</span><span class="p">,</span> <span class="n">single_op</span><span class="p">,</span> <span class="n">single_op</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;graph output with mask:&quot;</span><span class="p">,</span> <span class="n">graph_output</span><span class="p">,</span> <span class="n">graph_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">MetaInfershape</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 执行上述代码的输出类似如下</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">single</span> <span class="n">op</span> <span class="n">output</span> <span class="k">with</span> <span class="n">mask</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.0219</span><span class="p">,</span>  <span class="mf">0.0201</span><span class="p">,</span>  <span class="mf">0.0049</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0118</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0011</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0140</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0294</span><span class="p">,</span>  <span class="mf">0.0256</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0081</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0267</span><span class="p">,</span>  <span class="mf">0.0067</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0117</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0285</span><span class="p">,</span>  <span class="mf">0.0296</span><span class="p">,</span>  <span class="mf">0.0011</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0150</span><span class="p">,</span>  <span class="mf">0.0056</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0062</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0177</span><span class="p">,</span>  <span class="mf">0.0194</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0060</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0226</span><span class="p">,</span>  <span class="mf">0.0029</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0039</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0180</span><span class="p">,</span>  <span class="mf">0.0186</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0067</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0204</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0045</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0164</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0176</span><span class="p">,</span>  <span class="mf">0.0288</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0091</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0304</span><span class="p">,</span>  <span class="mf">0.0033</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0173</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">graph</span> <span class="n">output</span> <span class="k">with</span> <span class="n">mask</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.0219</span><span class="p">,</span>  <span class="mf">0.0201</span><span class="p">,</span>  <span class="mf">0.0049</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0118</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0011</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0140</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0294</span><span class="p">,</span>  <span class="mf">0.0256</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0081</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0267</span><span class="p">,</span>  <span class="mf">0.0067</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0117</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0285</span><span class="p">,</span>  <span class="mf">0.0296</span><span class="p">,</span>  <span class="mf">0.0011</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0150</span><span class="p">,</span>  <span class="mf">0.0056</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0062</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0177</span><span class="p">,</span>  <span class="mf">0.0194</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0060</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0226</span><span class="p">,</span>  <span class="mf">0.0029</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0039</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0180</span><span class="p">,</span>  <span class="mf">0.0186</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0067</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0204</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0045</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0164</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span> <span class="mf">0.0176</span><span class="p">,</span>  <span class="mf">0.0288</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0091</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0304</span><span class="p">,</span>  <span class="mf">0.0033</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0173</span><span class="p">]],</span>        <span class="n">device</span><span class="o">=</span><span class="s1">&#39;npu:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="页脚">
        <a href="examples.html" class="btn btn-neutral float-left" title="功能样例" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="faq.html" class="btn btn-neutral float-right" title="FAQ" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2024, Ascend。</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用的 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a> 开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>